"""
MediaMarkt ë¬´í•œ í¬ë¡¤ëŸ¬ V2 (íƒ€ì„ì¡´ ë¶„ë¦¬ ë²„ì „)
ì›ë³¸ mediamarkt.py ê¸°ë°˜ - DB/íƒ€ì„ì¡´/íŒŒì¼ì„œë²„ ì„¤ì •ë§Œ V2ë¡œ ë³€ê²½
- í˜„ì§€ì‹œê°„(ë„¤ëœë€ë“œ)ê³¼ í•œêµ­ì‹œê°„ ë¶„ë¦¬ ì €ì¥
- ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© (DB_CONFIG_V2)
- í•µì‹¬ ë¡œì§ì€ ì›ë³¸ê³¼ ë™ì¼
"""
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import pymysql
from sqlalchemy import create_engine
import paramiko
import time
import random
import re
from datetime import datetime, timedelta
import pytz
import logging
import os
import traceback

# Import configuration V2
from config import DB_CONFIG_V2 as DB_CONFIG, FILE_SERVER_CONFIG

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(message)s',
    handlers=[
        logging.FileHandler('mediamarkt_infinite.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MediaMarktInfiniteScraper:
    def __init__(self):
        self.driver = None
        self.db_engine = None
        self.sftp_client = None
        self.is_logged_in = False
        self.crawl_count = 0
        # V2: íƒ€ì„ì¡´ ë¶„ë¦¬ (í˜„ì§€ì‹œê°„ + í•œêµ­ì‹œê°„)
        self.korea_tz = pytz.timezone('Asia/Seoul')
        self.local_tz = pytz.timezone('Europe/Amsterdam')  # MediaMarkt ë„¤ëœë€ë“œ í˜„ì§€ ì‹œê°„
        self.start_time = datetime.now(self.korea_tz)
        self.saved_cookies = []

        # DB ì—°ê²° ì„¤ì •
        self.setup_db_connection()

        # DBì—ì„œ XPath ë¡œë“œ
        self.load_xpaths_from_db()
        
    def setup_db_connection(self):
        """DB ì—°ê²° ì„¤ì •"""
        try:
            connection_string = (
                f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
                f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
            )
            self.db_engine = create_engine(connection_string)
            logger.info("âœ… DB ì—°ê²° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.db_engine = None
    
    def load_xpaths_from_db(self):
        """DBì—ì„œ MediaMarktìš© ì„ íƒì ë¡œë“œ"""
        try:
            # ë¨¼ì € MediaMarktìš© ì„ íƒìê°€ ìˆëŠ”ì§€ í™•ì¸
            check_query = """
            SELECT COUNT(*) as count
            FROM mall_selectors
            WHERE mall_name = 'mediamarkt' 
              AND country_code = 'de'
              AND is_active = TRUE
            """
            
            check_df = pd.read_sql(check_query, self.db_engine)
            
            if check_df['count'][0] > 0:
                # MediaMarkt ì„ íƒìê°€ ìˆìœ¼ë©´ ë¡œë“œ
                query = """
                SELECT element_type, selector_value, priority
                FROM mall_selectors
                WHERE mall_name = 'mediamarkt' 
                  AND country_code = 'de'
                  AND is_active = TRUE
                ORDER BY element_type, priority DESC
                """
                
                df = pd.read_sql(query, self.db_engine)
                
                # element_typeë³„ë¡œ ê·¸ë£¹í™”
                self.XPATHS = {}
                for element_type in df['element_type'].unique():
                    type_selectors = df[df['element_type'] == element_type]['selector_value'].tolist()
                    self.XPATHS[element_type] = type_selectors
                
                logger.info(f"âœ… DBì—ì„œ MediaMarkt ì„ íƒì ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")
                
            else:
                # MediaMarkt ì„ íƒìê°€ ì—†ìœ¼ë©´ Currys ì„ íƒìë¥¼ ì°¸ê³ ìš©ìœ¼ë¡œ ë¡œë“œ
                logger.warning("âš ï¸ DBì— MediaMarkt ì„ íƒìê°€ ì—†ìŠµë‹ˆë‹¤. Currys ì„ íƒìë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
                
                currys_query = """
                SELECT element_type, selector_value, priority
                FROM mall_selectors
                WHERE mall_name = 'currys' 
                  AND country_code = 'uk'
                  AND is_active = TRUE
                ORDER BY element_type, priority DESC
                """
                
                currys_df = pd.read_sql(currys_query, self.db_engine)
                if not currys_df.empty:
                    logger.info(f"ì°¸ê³ : Currys ì„ íƒì {len(currys_df)}ê°œ ë°œê²¬")
                    logger.info("MediaMarktìš© ì„ íƒìë¥¼ DBì— ì¶”ê°€í•´ì£¼ì„¸ìš”:")
                    logger.info("INSERT INTO mall_selectors (mall_name, country_code, element_type, selector_value, priority, is_active)")
                    logger.info("VALUES ('mediamarkt', 'de', 'price', 'span[data-test=\"branded-price-whole\"]', 1, TRUE);")
                
                # ê¸°ë³¸ê°’ ì„¤ì •
                self.XPATHS = {
                    'price': [
                        "span[data-test='branded-price-whole']",
                        "div.price__large",
                        "span.price",
                        "[itemprop='price']",
                        ".product-price__price",
                        "span[data-testid='price-now']",
                        "div[data-testid='price-box'] span"
                    ],
                    'title': [
                        "h1",
                        "h1[data-test='product-title']",
                        ".product-title",
                        "h1.sc-f0860893-0",
                        "[data-testid='product-name']"
                    ],
                    'imageurl': [
                        "img.product-image",
                        "img[data-test='product-image']",
                        ".product-gallery img",
                        "picture img",
                        "[data-testid='product-image'] img"
                    ],
                    'availability': [
                        "[data-test='delivery-availability']",
                        ".availability-indicator",
                        "[data-testid='availability']"
                    ]
                }
                logger.warning("âš ï¸ ê¸°ë³¸ MediaMarkt ì„ íƒì ì‚¬ìš©")
                
        except Exception as e:
            logger.error(f"ì„ íƒì ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’
            self.XPATHS = {
                'price': ["span[data-test='branded-price-whole']", "div.price__large", "span.price"],
                'title': ["h1", "h1[data-test='product-title']"],
                'imageurl': ["img.product-image", ".product-gallery img"],
                'availability': ["[data-test='delivery-availability']"]
            }
    
    def get_crawl_targets(self, limit=None):
        """DBì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ URL ëª©ë¡ ì¡°íšŒ"""
        try:
            query = """
            SELECT *
            FROM samsung_price_tracking_list
            WHERE country = 'de' 
              AND mall_name = 'mediamarkt'
              AND is_active = TRUE
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql(query, self.db_engine)
            logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ {len(df)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return df.to_dict('records')
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        logger.info("ğŸ”§ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
        
        try:
            options = uc.ChromeOptions()
            
            # ê¸°ë³¸ ì˜µì…˜
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--window-size=1920,1080')
            
            # ë…ì¼ ì„¤ì •
            options.add_argument('--lang=de-DE')
            options.add_experimental_option('prefs', {
                "intl.accept_languages": "de-DE,de"
            })
            
            self.driver = uc.Chrome(options=options)
            self.driver.maximize_window()
            self.driver.set_page_load_timeout(30)
            
            logger.info("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def initial_manual_login(self):
        """ì´ˆê¸° ìˆ˜ë™ ë¡œê·¸ì¸ - Cloudflare í†µê³¼"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ” === ì´ˆê¸° ìˆ˜ë™ ë¡œê·¸ì¸ ===")
        logger.info("="*60)
        
        try:
            # MediaMarkt ë©”ì¸ í˜ì´ì§€ ì ‘ì†
            logger.info("MediaMarkt ì ‘ì† ì¤‘...")
            self.driver.get("https://www.mediamarkt.de")
            
            logger.info("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:")
            logger.info("1. Cloudflare ì±Œë¦°ì§€ê°€ ë‚˜íƒ€ë‚˜ë©´ í•´ê²°í•˜ì„¸ìš”")
            logger.info("2. ì¿ í‚¤ ë™ì˜ íŒì—…ì´ ë‚˜íƒ€ë‚˜ë©´ ìˆ˜ë½í•˜ì„¸ìš”")
            logger.info("3. ì‚¬ì´íŠ¸ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ê¸°ë‹¤ë¦¬ì„¸ìš”")
            logger.info("4. (ì„ íƒ) ë¡œê·¸ì¸ì´ í•„ìš”í•˜ë‹¤ë©´ ë¡œê·¸ì¸í•˜ì„¸ìš”")
            
            input("\nâœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
            
            # í˜„ì¬ ìƒíƒœ í™•ì¸
            current_url = self.driver.current_url
            if "mediamarkt.de" in current_url and not self.check_cloudflare_challenge():
                self.is_logged_in = True
                logger.info("âœ… ë¡œê·¸ì¸ ì„±ê³µ! ë¬´í•œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
                
                # ì¿ í‚¤ ì €ì¥
                try:
                    self.saved_cookies = self.driver.get_cookies()
                    logger.info(f"ğŸ’¾ ì¿ í‚¤ {len(self.saved_cookies)}ê°œ ì €ì¥")
                    
                    # íŒŒì¼ë¡œë„ ì €ì¥
                    import json
                    with open('mediamarkt_cookies.json', 'w') as f:
                        json.dump(self.saved_cookies, f)
                except Exception as e:
                    logger.error(f"ì¿ í‚¤ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                return True
            else:
                logger.error("âŒ ë¡œê·¸ì¸ ì‹¤íŒ¨ - Cloudflareë¥¼ í†µê³¼í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            logger.error(f"ì´ˆê¸° ë¡œê·¸ì¸ ì˜¤ë¥˜: {e}")
            return False
    
    def check_cloudflare_challenge(self):
        """Cloudflare ì±Œë¦°ì§€ í˜ì´ì§€ì¸ì§€ í™•ì¸"""
        try:
            indicators = [
                "Verifying you are human",
                "ì‚¬ëŒì¸ì§€ í™•ì¸",
                "cf-challenge",
                "Just a moment",
                "einen Moment"
            ]
            
            page_source = self.driver.page_source.lower()
            page_title = self.driver.title.lower()
            
            for indicator in indicators:
                if indicator.lower() in page_source or indicator.lower() in page_title:
                    return True
                    
            return False
            
        except Exception:
            return False
    
    def restart_browser(self):
        """ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ë° ì¬ë¡œê·¸ì¸"""
        try:
            logger.info("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
            
            # ê¸°ì¡´ ë“œë¼ì´ë²„ ì¢…ë£Œ
            try:
                self.driver.quit()
            except:
                pass
            
            time.sleep(5)
            
            # ë“œë¼ì´ë²„ ì¬ì„¤ì •
            if not self.setup_driver():
                return False
            
            # ìë™ ì¬ë¡œê·¸ì¸ ì‹œë„ (ì €ì¥ëœ ì¿ í‚¤ ì‚¬ìš©)
            if hasattr(self, 'saved_cookies') and self.saved_cookies:
                try:
                    self.driver.get("https://www.mediamarkt.de")
                    time.sleep(3)
                    
                    # ì¿ í‚¤ ë³µì›
                    for cookie in self.saved_cookies:
                        try:
                            self.driver.add_cookie(cookie)
                        except:
                            pass
                    
                    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                    self.driver.refresh()
                    time.sleep(5)
                    
                    # Cloudflare ì²´í¬
                    if not self.check_cloudflare_challenge():
                        logger.info("âœ… ì¿ í‚¤ë¡œ ìë™ ì¬ë¡œê·¸ì¸ ì„±ê³µ")
                        self.is_logged_in = True
                        return True
                except:
                    pass
            
            # ì¿ í‚¤ ë³µì› ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ ë¡œê·¸ì¸ í•„ìš”
            logger.warning("âš ï¸ ìë™ ì¬ë¡œê·¸ì¸ ì‹¤íŒ¨. ìˆ˜ë™ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
            # ìˆ˜ë™ ì¬ë¡œê·¸ì¸ í”„ë¡¬í”„íŠ¸
            return self.initial_manual_login()
            
        except Exception as e:
            logger.error(f"ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def check_browser_health(self):
        """ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ JavaScript ì‹¤í–‰ìœ¼ë¡œ ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸
            result = self.driver.execute_script("return document.readyState")
            return result == "complete"
        except:
            return False
    
    def keep_session_alive(self):
        """ì„¸ì…˜ ìœ ì§€ë¥¼ ìœ„í•œ í™œë™ (ë” ì•ˆì „í•˜ê²Œ)"""
        try:
            # ë¸Œë¼ìš°ì € ìƒíƒœë§Œ í™•ì¸
            try:
                current_url = self.driver.current_url
                logger.debug(f"í˜„ì¬ URL: {current_url}")
            except:
                logger.warning("âš ï¸ ë¸Œë¼ìš°ì € ì‘ë‹µ ì—†ìŒ")
                self.is_logged_in = False
                return
            
            # ë„ˆë¬´ ìì£¼ ì´ë™í•˜ì§€ ì•ŠìŒ (ì„¸ì…˜ ìœ ì§€ì—ëŠ” í˜„ì¬ í˜ì´ì§€ ìœ ì§€ê°€ ë” ë‚˜ì„ ìˆ˜ë„)
            if "mediamarkt.de" in current_url:
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì‘ì€ ì•¡ì…˜ë§Œ
                try:
                    # ì‘ì€ ìŠ¤í¬ë¡¤
                    self.driver.execute_script("window.scrollBy(0, 100)")
                    time.sleep(1)
                    self.driver.execute_script("window.scrollBy(0, -100)")
                    
                    logger.info("ğŸ’“ ì„¸ì…˜ keep-alive ì™„ë£Œ (ê°€ë²¼ìš´ ì•¡ì…˜)")
                except:
                    pass
            else:
                # MediaMarkt í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ í™ˆìœ¼ë¡œ
                try:
                    self.driver.get("https://www.mediamarkt.de")
                    time.sleep(3)
                    
                    # Cloudflare ì²´í¬
                    if self.check_cloudflare_challenge():
                        logger.error("âŒ Keep-alive ì¤‘ Cloudflare ê°ì§€!")
                        self.is_logged_in = False
                        return
                    
                    logger.info("ğŸ’“ ì„¸ì…˜ keep-alive ì™„ë£Œ (í™ˆ ë°©ë¬¸)")
                except Exception as e:
                    logger.error(f"Keep-alive ì˜¤ë¥˜: {e}")
                    
        except Exception as e:
            logger.error(f"Keep-alive ì˜¤ë¥˜: {e}")
            # ì—ëŸ¬ê°€ ë‚˜ë„ ì¦‰ì‹œ ì„¸ì…˜ì„ ì¢…ë£Œí•˜ì§€ ì•ŠìŒ
    
    def extract_product_info(self, url, row_data):
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ” í˜ì´ì§€ ì ‘ì†: {url}")
            self.driver.get(url)
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            time.sleep(random.uniform(3, 5))
            
            # Cloudflare ì²´í¬
            if self.check_cloudflare_challenge():
                logger.error("âŒ Cloudflare ì±Œë¦°ì§€ ê°ì§€! ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                self.is_logged_in = False
                return None
            
            # V2: íƒ€ì„ì¡´ ë¶„ë¦¬
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)


            # ISO 8601 í˜•ì‹

            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")

            tz_offset = local_time.strftime("%z")

            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"

            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"


            # ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°
            result = {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'de'),
                'ships_from': 'DE',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'MediaMarkt',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': 'o'
            }
            
            # ê°€ê²© ì¶”ì¶œ
            try:
                price_found = False
                
                # DBì—ì„œ ê°€ì ¸ì˜¨ ì„ íƒìë¡œ ì‹œë„
                for selector in self.XPATHS.get('price', []):
                    try:
                        if selector.startswith('//'):
                            # XPathì¸ ê²½ìš°
                            price_elements = self.driver.find_elements(By.XPATH, selector)
                        else:
                            # CSS ì„ íƒìì¸ ê²½ìš°
                            price_elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                        
                        for price_element in price_elements:
                            price_text = price_element.text.strip()
                            
                            if price_text:
                                # ìœ ë¡œ ê°€ê²© ì¶”ì¶œ (ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›)
                                # ì˜ˆ: "89,99 â‚¬", "â‚¬ 89.99", "89.99", "89,99"
                                price_text = price_text.replace('â‚¬', '').strip()
                                price_match = re.search(r'(\d+)[,.]?(\d*)', price_text)
                                if price_match:
                                    price = price_match.group(1)
                                    if price_match.group(2):
                                        price += '.' + price_match.group(2)
                                    result['retailprice'] = float(price)
                                    logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {result['retailprice']}â‚¬ (ì„ íƒì: {selector})")
                                    price_found = True
                                    break
                        
                        if price_found:
                            break
                            
                    except Exception as e:
                        logger.debug(f"ì„ íƒì {selector} ì‹¤íŒ¨: {e}")
                        continue
                
                if not price_found:
                    logger.warning("âŒ DB ì„ íƒìë¡œ ê°€ê²©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    # ì¶”ê°€ ì„ íƒì ì‹œë„ (í˜ì´ì§€ êµ¬ì¡°ê°€ ë³€ê²½ëœ ê²½ìš°)
                    additional_selectors = [
                        "meta[property='product:price:amount']",
                        "[data-price]",
                        ".price-now"
                    ]
                    
                    for selector in additional_selectors:
                        try:
                            if selector.startswith('meta'):
                                elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                                price_text = elem.get_attribute('content')
                            else:
                                elem = self.driver.find_element(By.CSS_SELECTOR, selector)
                                price_text = elem.get_attribute('data-price') or elem.text
                            
                            if price_text:
                                price_match = re.search(r'(\d+)[,.]?(\d*)', price_text)
                                if price_match:
                                    price = price_match.group(1)
                                    if price_match.group(2):
                                        price += '.' + price_match.group(2)
                                    result['retailprice'] = float(price)
                                    logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ (ì¶”ê°€ ì„ íƒì): {result['retailprice']}â‚¬")
                                    
                                    # ìƒˆë¡œìš´ ì„ íƒìë¥¼ DBì— ì¶”ê°€í•˜ë¼ê³  ì œì•ˆ
                                    logger.info(f"ğŸ’¡ ìƒˆë¡œìš´ ì„ íƒì ë°œê²¬: {selector}")
                                    logger.info(f"DBì— ì¶”ê°€í•˜ì„¸ìš”: INSERT INTO mall_selectors (mall_name, country_code, element_type, selector_value, priority, is_active) VALUES ('mediamarkt', 'de', 'price', '{selector}', 2, TRUE);")
                                    break
                        except:
                            continue
                    
            except Exception as e:
                logger.warning(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì œëª© ì¶”ì¶œ
            try:
                for selector in self.XPATHS.get('title', []):
                    try:
                        if selector.startswith('//'):
                            title_element = self.driver.find_element(By.XPATH, selector)
                        else:
                            title_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        
                        result['title'] = title_element.text.strip()
                        logger.info(f"ì œëª©: {result['title']}")
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            try:
                for selector in self.XPATHS.get('imageurl', []):
                    try:
                        if selector.startswith('//'):
                            image_element = self.driver.find_element(By.XPATH, selector)
                        else:
                            image_element = self.driver.find_element(By.CSS_SELECTOR, selector)
                        
                        result['imageurl'] = image_element.get_attribute('src')
                        logger.info(f"ì´ë¯¸ì§€ URL: {result['imageurl']}")
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return None
    
    def save_to_db(self, df):
        """DBì— ê²°ê³¼ ì €ì¥"""
        if self.db_engine is None:
            logger.warning("âš ï¸ DB ì—°ê²°ì´ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
        
        try:
            # mediamarkt_price_crawl_tbl_de_v2 í…Œì´ë¸”ì— ì €ì¥
            df.to_sql('mediamarkt_price_crawl_tbl_de_v2', self.db_engine, if_exists='append', index=False)
            logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
            
            # í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥
            log_records = []
            for _, row in df.iterrows():
                log_records.append({
                    'country_code': 'de',
                    'url': row['producturl'],
                    'status': 'success' if row['retailprice'] is not None else 'failed',
                    'error_message': None if row['retailprice'] is not None else 'Price not found',
                    'execution_time': random.uniform(3, 10),
                    'retailprice': row['retailprice'],
                    'crawl_datetime': row['crawl_datetime']
                })
            
            if log_records:
                log_df = pd.DataFrame(log_records)
                log_df.to_sql('amazon_crawl_logs', self.db_engine, if_exists='append', index=False)
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def upload_to_file_server(self, local_file_path, date_folder):
        """íŒŒì¼ì„œë²„ì— ì—…ë¡œë“œ"""
        try:
            transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
            transport.connect(
                username=FILE_SERVER_CONFIG['username'],
                password=FILE_SERVER_CONFIG['password']
            )
            sftp = paramiko.SFTPClient.from_transport(transport)

            # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            date_dir = f"{FILE_SERVER_CONFIG['upload_path']}/{date_folder}"

            # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(date_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ ë‚ ì§œ ë””ë ‰í† ë¦¬ ìƒì„±: {date_dir}")
                sftp.mkdir(date_dir)

            # ì—…ë¡œë“œ ê²½ë¡œ
            remote_filename = os.path.basename(local_file_path)
            remote_path = f"{date_dir}/{remote_filename}"

            # íŒŒì¼ ì—…ë¡œë“œ
            sftp.put(local_file_path, remote_path)
            logger.info(f"âœ… íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì™„ë£Œ: {remote_path}")

            sftp.close()
            transport.close()

            return True
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    def save_results(self, df):
        """ê²°ê³¼ë¥¼ DBì™€ íŒŒì¼ì„œë²„ì— ì €ì¥"""
        now = datetime.now()
        date_str = now.strftime("%Y%m%d")
        time_str = now.strftime("%H%M%S")
        base_filename = f"{date_str}_{time_str}_de_mediamarkt"

        results = {'db_saved': False, 'server_uploaded': False}

        # DB ì €ì¥
        results['db_saved'] = self.save_to_db(df)

        # íŒŒì¼ì„œë²„ ì—…ë¡œë“œ
        try:
            # 1. CSV íŒŒì¼ ìƒì„±
            csv_filename = f'{base_filename}.csv'
            # Headerë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜
            df.columns = df.columns.str.upper()
            df.to_csv(csv_filename, index=False, encoding='utf-8', lineterminator='\r\n')

            # 2. CSVë¥¼ ZIPìœ¼ë¡œ ì••ì¶•
            zip_filename = f'{base_filename}.zip'
            with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                zipf.write(csv_filename, arcname=csv_filename)

            # 3. MD5 ê³„ì‚°
            def calculate_md5(filename):
                md5 = hashlib.md5()
                with open(filename, 'rb') as f:
                    for chunk in iter(lambda: f.read(4096), b''):
                        md5.update(chunk)
                return md5.hexdigest()

            csv_md5 = calculate_md5(csv_filename)
            zip_md5 = calculate_md5(zip_filename)

            # 4. TXT íŒŒì¼ ìƒì„± (MD5 ì €ì¥)
            txt_filename = f'{base_filename}.txt'
            with open(txt_filename, 'w', encoding='utf-8') as f:
                f.write(f"csv_md5: {csv_md5}\n")
                f.write(f"zip_md5: {zip_md5}\n")

            # 5. ZIPê³¼ TXTë¥¼ ë‚ ì§œ í´ë”ì— ì—…ë¡œë“œ
            if self.upload_to_file_server(zip_filename, date_str):
                if self.upload_to_file_server(txt_filename, date_str):
                    results['server_uploaded'] = True

            # 6. ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
            for temp_file in [csv_filename, zip_filename, txt_filename]:
                if os.path.exists(temp_file):
                    os.remove(temp_file)

            logger.info("ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        return results
    
    def crawl_once(self):
        """1íšŒ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info(f"\n{'='*60}")
        logger.info(f"ğŸš€ í¬ë¡¤ë§ ë¼ìš´ë“œ {self.crawl_count + 1} ì‹œì‘")
        logger.info(f"ì‹œì‘ ì‹œê°„: {datetime.now()}")
        logger.info(f"{'='*60}")
        
        # DBì—ì„œ URL ëª©ë¡ ì¡°íšŒ
        urls_data = self.get_crawl_targets()
        
        if not urls_data:
            logger.warning("í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        logger.info(f"ğŸ“Š ì´ {len(urls_data)}ê°œ ì œí’ˆ ì²˜ë¦¬ ì˜ˆì •")
        
        results = []
        success_count = 0
        
        for idx, row in enumerate(urls_data):
            # ì„¸ì…˜ í™•ì¸
            if not self.is_logged_in:
                logger.error("âŒ ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                break
            
            logger.info(f"\nì§„í–‰ë¥ : {idx + 1}/{len(urls_data)} ({(idx + 1)/len(urls_data)*100:.1f}%)")
            
            # URL ì¶”ì¶œ
            url = row.get('url')
            
            # ì œí’ˆ ì •ë³´ ì¶”ì¶œ
            result = self.extract_product_info(url, row)
            
            if result:
                results.append(result)
                if result['retailprice'] is not None:
                    success_count += 1
            else:
                # resultê°€ Noneì¸ ê²½ìš° (ì„¸ì…˜ ë§Œë£Œ)
                logger.warning(f"âš ï¸ ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {url}")
                # ì„¸ì…˜ ì¬í™•ì¸
                if self.check_cloudflare_challenge():
                    logger.error("Cloudflare ê°ì§€ë¨")
                    self.is_logged_in = False
                    break
            
            # 5ê°œë§ˆë‹¤ keep-alive (ë” ìì£¼)
            if (idx + 1) % 5 == 0:
                # ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸
                if not self.check_browser_health():
                    logger.warning("âš ï¸ ë¸Œë¼ìš°ì € ìƒíƒœ ì´ìƒ ê°ì§€")
                    if self.restart_browser():
                        logger.info("âœ… ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì™„ë£Œ")
                    else:
                        logger.error("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì‹¤íŒ¨. í¬ë¡¤ë§ ì¤‘ë‹¨")
                        self.is_logged_in = False
                        break
                else:
                    self.keep_session_alive()
                
                # ì¤‘ê°„ ì €ì¥
                if results:
                    interim_df = pd.DataFrame(results[-5:])
                    if self.db_engine:
                        try:
                            interim_df.to_sql('mediamarkt_price_crawl_tbl_de_v2', self.db_engine, 
                                            if_exists='append', index=False)
                            logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: 5ê°œ ë ˆì½”ë“œ")
                        except:
                            pass
            
            # ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°
            if idx < len(urls_data) - 1:
                wait_time = random.uniform(5, 10)
                time.sleep(wait_time)
                
                # 25ê°œë§ˆë‹¤ ê¸´ íœ´ì‹
                if (idx + 1) % 25 == 0:
                    logger.info("â˜• 25ê°œ ì²˜ë¦¬ ì™„ë£Œ, 30ì´ˆ íœ´ì‹...")
                    time.sleep(30)
        
        # ê²°ê³¼ ì €ì¥
        if results:
            df = pd.DataFrame(results)
            save_results = self.save_results(df)
            
            # í†µê³„
            logger.info(f"\nğŸ“Š === í¬ë¡¤ë§ ë¼ìš´ë“œ {self.crawl_count + 1} ì™„ë£Œ ===")
            logger.info(f"ì „ì²´ ì œí’ˆ: {len(results)}ê°œ")
            logger.info(f"ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {success_count}ê°œ")
            logger.info(f"ì„±ê³µë¥ : {success_count/len(results)*100:.1f}%")
            logger.info(f"DB ì €ì¥: {'âœ…' if save_results['db_saved'] else 'âŒ'}")
            logger.info(f"íŒŒì¼ì„œë²„ ì—…ë¡œë“œ: {'âœ…' if save_results['server_uploaded'] else 'âŒ'}")
        
        self.crawl_count += 1
    
    def run_infinite_crawling(self):
        """ë¬´í•œ í¬ë¡¤ë§ ì‹¤í–‰"""
        logger.info("\nğŸ”„ === ë¬´í•œ í¬ë¡¤ë§ ëª¨ë“œ ì‹œì‘ ===")
        logger.info("â° 1ì‹œê°„ë§ˆë‹¤ ìë™ í¬ë¡¤ë§ ì‹¤í–‰")
        logger.info("ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
        
        # ì²« ì‹¤í–‰
        self.crawl_once()
        
        # ë¬´í•œ ë£¨í”„
        while self.is_logged_in:
            try:
                # 1ì‹œê°„ ëŒ€ê¸°
                logger.info(f"\nâ³ ë‹¤ìŒ í¬ë¡¤ë§ê¹Œì§€ 1ì‹œê°„ ëŒ€ê¸° ì¤‘...")
                logger.info(f"ë‹¤ìŒ ì‹¤í–‰ ì˜ˆì •: {(datetime.now() + timedelta(hours=1)).strftime('%Y-%m-%d %H:%M:%S')}")
                
                # 1ì‹œê°„ = 60ë¶„, 1ë¶„ë§ˆë‹¤ ì²´í¬
                for i in range(60):
                    if not self.is_logged_in:
                        break
                        
                    time.sleep(60)  # 1ë¶„ ëŒ€ê¸°
                    
                    # 15ë¶„ë§ˆë‹¤ keep-alive (ë” ìì£¼)
                    if i > 0 and i % 15 == 0:
                        logger.info(f"ğŸ’“ Keep-alive ì‹¤í–‰ ({i}/60ë¶„ ê²½ê³¼)")
                        # ë¸Œë¼ìš°ì € ìƒíƒœ í™•ì¸ í›„ keep-alive
                        if self.check_browser_health():
                            self.keep_session_alive()
                        else:
                            logger.warning("âš ï¸ ë¸Œë¼ìš°ì € ì‘ë‹µ ì—†ìŒ. ì¬ì‹œì‘ ì‹œë„...")
                            if not self.restart_browser():
                                logger.error("âŒ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì‹¤íŒ¨")
                                self.is_logged_in = False
                                break
                    
                    # 30ë¶„ë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
                    if i == 30:
                        uptime = datetime.now() - self.start_time
                        logger.info(f"â±ï¸ ê°€ë™ ì‹œê°„: {uptime} | í¬ë¡¤ë§ íšŸìˆ˜: {self.crawl_count}")
                
                # 1ì‹œê°„ í›„ í¬ë¡¤ë§ ì‹¤í–‰
                if self.is_logged_in:
                    self.crawl_once()
                
            except KeyboardInterrupt:
                logger.info("\nğŸ‘‹ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
                break
            except Exception as e:
                logger.error(f"ë¬´í•œ ë£¨í”„ ì˜¤ë¥˜: {e}")
                logger.error(traceback.format_exc())
                time.sleep(300)  # 5ë¶„ ëŒ€ê¸° í›„ ê³„ì†
        
        logger.info("ë¬´í•œ í¬ë¡¤ë§ ì¢…ë£Œ")
    
    def start(self):
        """ë©”ì¸ ì‹œì‘ í•¨ìˆ˜"""
        logger.info("\nğŸš€ MediaMarkt ë¬´í•œ í¬ë¡¤ëŸ¬ ì‹œì‘")
        logger.info("="*60)
        
        # ë“œë¼ì´ë²„ ì„¤ì •
        if not self.setup_driver():
            logger.error("ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        try:
            # ì´ˆê¸° ìˆ˜ë™ ë¡œê·¸ì¸
            if not self.initial_manual_login():
                logger.error("ì´ˆê¸° ë¡œê·¸ì¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return
            
            # ë¬´í•œ í¬ë¡¤ë§ ì‹œì‘
            self.run_infinite_crawling()
            
        except Exception as e:
            logger.error(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("ğŸ”§ ë“œë¼ì´ë²„ ì¢…ë£Œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\nğŸš€ MediaMarkt ë¬´í•œ í¬ë¡¤ëŸ¬")
    print("="*60)
    print("ì´ˆê¸°ì— ìˆ˜ë™ìœ¼ë¡œ Cloudflareë¥¼ í†µê³¼í•œ í›„")
    print("ìë™ìœ¼ë¡œ ë¬´í•œ í¬ë¡¤ë§ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
    print("="*60)
    
    # ìŠ¤í¬ë˜í¼ ìƒì„± ë° ì‹¤í–‰
    scraper = MediaMarktInfiniteScraper()
    
    if scraper.db_engine is None:
        logger.error("DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ì‹œì‘
    scraper.start()

if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = [
        'undetected-chromedriver',
        'selenium',
        'pandas',
        'pymysql',
        'sqlalchemy',
        'paramiko',
        'openpyxl'
    ]
    
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("pip install " + " ".join(required_packages))
    print()
    
    main()