# -*- coding: utf-8 -*-
"""
Amazon ì˜êµ­ ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ V2 (íƒ€ì„ì¡´ ë¶„ë¦¬ ë²„ì „)
ì›ë³¸ uk.py ê¸°ë°˜ - DB/íƒ€ì„ì¡´/íŒŒì¼ì„œë²„ ì„¤ì •ë§Œ V2ë¡œ ë³€ê²½
- í˜„ì§€ì‹œê°„(ì˜êµ­)ê³¼ í•œêµ­ì‹œê°„ ë¶„ë¦¬ ì €ì¥
- ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© (DB_CONFIG_V2)
- í•µì‹¬ ë¡œì§ì€ ì›ë³¸ê³¼ ë™ì¼
"""
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pandas as pd
import pymysql
from sqlalchemy import create_engine
import paramiko
import time
import random
import re
from datetime import datetime
import pytz
import logging
import os
from io import StringIO
import json
import zipfile
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import database configuration V2
from config import DB_CONFIG_V2 as DB_CONFIG

from config import FILE_SERVER_CONFIG

class AmazonUKScraper:
    def __init__(self):
        self.driver = None
        self.db_engine = None
        self.country_code = 'gb'
        self.wait = None
        # V2: íƒ€ì„ì¡´ ë¶„ë¦¬ (í˜„ì§€ì‹œê°„ + í•œêµ­ì‹œê°„)
        self.korea_tz = pytz.timezone('Asia/Seoul')
        self.local_tz = pytz.timezone('Europe/London')  # ì˜êµ­ í˜„ì§€ ì‹œê°„
        
        self.setup_db_connection()
        self.setup_uk_selectors()
        self.load_selectors_from_db()
        
    def setup_db_connection(self):
        """DB ì—°ê²° ì„¤ì •"""
        try:
            connection_string = (
                f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
                f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
            )
            self.db_engine = create_engine(connection_string)
            logger.info("DB ì—°ê²° ì„¤ì • ì™„ë£Œ")
        except Exception as e:
            logger.error(f"DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.db_engine = None
    
    def setup_uk_selectors(self):
        """ì˜êµ­ ì „ìš© ì„ íƒì ì„¤ì • (ë…ì¼ ì„ íƒì ì¶”ê°€)"""
        self.selectors = {
            'price': [
                # ë…ì¼ ì „ìš© xpath ì¶”ê°€ (ìµœìš°ì„ )
                "//*[@id='corePrice_feature_div']/div/div/span[1]/span[1]",
                "/html/body/div[2]/div/div/div[4]/div[1]/div[3]/div/div[1]/div/div/div/form/div/div/div/div/div[3]/div/div[1]/div/div/span[1]/span[1]",
                
                # ìš”ì²­ëœ ì¶”ê°€ ì„ íƒì
                "//*[@id='corePriceDisplay_desktop_feature_div']/div[1]/span[1]",
                "//*[@id='usedBuySection']/div[1]/div/span[2]",
                
                # ë©”ì¸ ê°€ê²© ì˜ì—­
                "//*[@id='corePrice_feature_div']//span[@class='a-offscreen']",
                "//*[@id='corePriceDisplay_desktop_feature_div']//span[@class='a-offscreen']",
                "//*[@id='apex_desktop']//span[@class='a-price']//span[@class='a-offscreen']",
                
                # ì²« ë²ˆì§¸ ê°€ê²©ë§Œ
                "(//span[@class='a-price']//span[@class='a-offscreen'])[1]",
                "(//span[@class='a-price-whole'])[1]",
                
                # ê¸°ë³¸ ê°€ê²© ìš”ì†Œë“¤
                "//*[@id='priceblock_ourprice']",
                "//*[@id='priceblock_dealprice']",
                "//*[@id='listPrice']",
                
                # Whole ê°€ê²©
                "//*[@id='corePrice_feature_div']//span[@class='a-price-whole']",
                "//*[@id='corePriceDisplay_desktop_feature_div']//span[@class='a-price-whole']",
                "//*[@id='apex_desktop']//span[@class='a-price-whole']"
            ],
            'price_fraction': [
                "//*[@id='corePrice_feature_div']//span[@class='a-price-fraction']",
                "//*[@id='corePriceDisplay_desktop_feature_div']//span[@class='a-price-fraction']",
                "//*[@id='apex_desktop']//span[@class='a-price-fraction']",
                "//span[@class='a-price-fraction']"
            ],
            'title': [
                # ë…ì¼ ì „ìš© xpath ì¶”ê°€ (ìµœìš°ì„ )
                "//*[@id='productTitle']",
                "/html/body/div[2]/div/div/div[4]/div[4]/div[1]/div/h1/span",
                
                "#productTitle",
                "//span[@id='productTitle']",
                "//h1/span[@id='productTitle']"
            ],
            'ships_from': [
                # ë…ì¼ ì „ìš© xpath ì¶”ê°€ (ìµœìš°ì„ )
                "//*[@id='fulfillerInfoFeature_feature_div']/div[2]/div[1]/span",
                
                "//*[@id='SSOFpopoverLink_ubb']",
                "//a[@id='SSOFpopoverLink_ubb']",
                "//div[@id='fulfillerInfoFeature_feature_div']//span"
            ],
            'sold_by': [
                # ë…ì¼ ì „ìš© xpath ì¶”ê°€ (ìµœìš°ì„ )
                "//*[@id='merchantInfoFeature_feature_div']/div[2]/div[1]/span",
                
                "//a[@id='sellerProfileTriggerId']",
                "//*[@id='sellerProfileTriggerId']",
                "//div[@id='merchantInfoFeature_feature_div']//a",
                "//div[@id='merchantInfoFeature_feature_div']//span"
            ],
            'imageurl': [
                # ë…ì¼ ì „ìš© xpath ì¶”ê°€ (ìµœìš°ì„ )
                "//*[@id='landingImage']",
                "/html/body/div[2]/div/div/div[4]/div[3]/div[1]/div[1]/div/div/div[2]/div[1]/div[1]/ul/li[1]/span/span/div/img",
                
                "//div[@id='imageBlock']//img[@id='landingImage']",
                "//div[@id='main-image-container']//img",
                "//img[@class='a-dynamic-image']"
            ],
            'availability': [
                "//div[@id='availability']//span",
                "//div[@id='availability_feature_div']//span"
            # ],
            # 'vat_text_list': [
            #     # ë…ì¼ VAT í…ìŠ¤íŠ¸ ì¶”ê°€
            #     "inkl. MwSt.",
            #     "inklusive MwSt.", 
            #     "Steuer inbegriffen",
            #     # ì˜êµ­ VAT í…ìŠ¤íŠ¸
            #     "Tax included", 
            #     "include VAT.",
            #     "VAT included"
            ],
            'excluded_price_areas': [
                'product-comparison',
                'comparison-desktop',
                'non-deal-price',
                'strikethrough',
                'list-price',
                'rrp-price',
                'was-price',
                'usedBuySection',
                'capacity-selection',
                'gebraucht'
            ]
        }
    
    def load_selectors_from_db(self):
        """DBì—ì„œ ì„ íƒì ë¡œë“œ"""
        if not self.db_engine:
            logger.warning("DB ì—°ê²°ì´ ì—†ì–´ ì„ íƒì ë¡œë“œ ë¶ˆê°€")
            return
        
        try:
            query = """
            SELECT element_type, selector_value, priority
            FROM amazon_selectors
            WHERE country_code = 'gb' 
              AND is_active = TRUE
            ORDER BY element_type, priority ASC
            """
            
            df = pd.read_sql(query, self.db_engine)
            logger.info(f"DBì—ì„œ UK ì„ íƒì ë¡œë“œ: {len(df)}ê°œ")
            
            if len(df) > 0:
                for element_type in df['element_type'].unique():
                    selectors = df[df['element_type'] == element_type]['selector_value'].tolist()
                    if element_type in self.selectors:
                        existing = self.selectors[element_type]
                        self.selectors[element_type] = existing + selectors
                    else:
                        self.selectors[element_type] = selectors
                
                logger.info("DB ì„ íƒì ë³‘í•© ì™„ë£Œ")
        except Exception as e:
            logger.error(f"DB ì„ íƒì ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        logger.info("Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
        
        try:
            options = uc.ChromeOptions()
            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-setuid-sandbox')
            options.add_argument('--disable-gpu')
            options.add_argument('--disable-software-rasterizer')
            
            user_agents = [
                'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            ]
            options.add_argument(f'--user-agent={random.choice(user_agents)}')
            options.add_experimental_option('prefs', {'intl.accept_languages': 'en-GB,en,de-DE,de'})
            
            self.driver = uc.Chrome(options=options)
            self.driver.maximize_window()
            self.wait = WebDriverWait(self.driver, 20)
            
            logger.info("ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def handle_captcha_or_block_page(self):
        """ì°¨ë‹¨ í˜ì´ì§€ë‚˜ ìº¡ì°¨ ì²˜ë¦¬"""
        try:
            logger.info("ì°¨ë‹¨/ìº¡ì°¨ í˜ì´ì§€ í™•ì¸ ì¤‘...")
            
            continue_texts = ['Continue shopping', 'Continue', 'Weiter einkaufen', 'Weiter']
            
            all_selectors = []
            for text in continue_texts:
                all_selectors.extend([
                    f"//button[contains(text(), '{text}')]",
                    f"//input[@value='{text}']",
                    f"//a[contains(text(), '{text}')]",
                    f"//span[contains(text(), '{text}')]/ancestor::button"
                ])
            
            all_selectors.extend([
                "//button[contains(@class, 'a-button-primary')]",
                "button.a-button-primary",
                "button[type='submit']"
            ])
            
            for selector in all_selectors:
                try:
                    if selector.startswith('//'):
                        button = self.driver.find_element(By.XPATH, selector)
                    else:
                        button = self.driver.find_element(By.CSS_SELECTOR, selector)
                    
                    if button and button.is_displayed():
                        try:
                            button.click()
                            logger.info("Continue ë²„íŠ¼ í´ë¦­ ì„±ê³µ")
                        except:
                            self.driver.execute_script("arguments[0].click();", button)
                            logger.info("JavaScript í´ë¦­ ì„±ê³µ")
                        
                        time.sleep(3)
                        return True
                except Exception:
                    continue
            
            return False
        except Exception as e:
            logger.error(f"ì°¨ë‹¨ í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def is_excluded_price_element(self, element):
        """ê°€ê²© ìš”ì†Œê°€ ì œì™¸ ëŒ€ìƒì¸ì§€ í™•ì¸"""
        try:
            element_html = self.driver.execute_script("return arguments[0].outerHTML;", element)
            excluded_areas = self.selectors.get('excluded_price_areas', [])
            
            for pattern in excluded_areas:
                if pattern in element_html.lower():
                    return True
            
            return False
        except Exception:
            return False
    
    def detect_currency_and_parse_price(self, price_text, url):
        """URLì„ ê¸°ë°˜ìœ¼ë¡œ í†µí™”ë¥¼ ê°ì§€í•˜ê³  ê°€ê²© íŒŒì‹±"""
        try:
            price_text = price_text.strip()
            
            # URL ê¸°ë°˜ìœ¼ë¡œ êµ­ê°€ ê°ì§€
            is_german = '.de/' in url or 'amazon.de' in url
            
            # ë¬´íš¨í•œ íŒ¨í„´ í™•ì¸
            invalid_patterns = [
                r'^[a-zA-Z\s]+$',
                r'^\d+\s*[a-zA-Z]',
            ]
            
            if is_german:
                invalid_patterns.extend([
                    r'war\s*[â‚¬]',
                    r'uvp\s*[â‚¬]',
                    r'gebraucht'
                ])
            else:
                invalid_patterns.extend([
                    r'was\s*[Â£]',
                    r'list\s*price',
                    r'buy\s*used'
                ])
            
            for pattern in invalid_patterns:
                if re.search(pattern, price_text, re.IGNORECASE):
                    return None
            
            if is_german:
                # ë…ì¼: ìœ ë¡œ ì²˜ë¦¬
                cleaned = re.sub(r'[â‚¬\s]', '', price_text)
                
                # ë…ì¼ í˜•ì‹: 1.234,99
                if re.match(r'^\d{1,3}(?:\.\d{3})*(?:,\d{1,2})?$', cleaned):
                    try:
                        if ',' in cleaned:
                            parts = cleaned.split(',')
                            if len(parts) == 2:
                                whole_part = parts[0].replace('.', '')
                                decimal_part = parts[1]
                                price_value = float(f"{whole_part}.{decimal_part}")
                            else:
                                return None
                        else:
                            price_value = float(cleaned.replace('.', ''))
                        
                        if 5 <= price_value <= 50000:
                            return str(price_value)
                    except:
                        pass
            else:
                # ì˜êµ­: íŒŒìš´ë“œ ì²˜ë¦¬
                cleaned = re.sub(r'[Â£\s]', '', price_text)
                
                # ì˜êµ­ í˜•ì‹: 1,234.99
                if re.match(r'^\d{1,4}(?:,\d{3})*(?:\.\d{1,2})?$', cleaned):
                    try:
                        price_value = float(cleaned.replace(',', ''))
                        if 5 <= price_value <= 50000:
                            return cleaned.replace(',', '')
                    except:
                        pass
            
            return None
        except Exception:
            return None
    
    def extract_price(self, url):
        """ê°€ê²© ì¶”ì¶œ"""
        logger.info("ê°€ê²© ì¶”ì¶œ ì‹œì‘")
        
        price_selectors = self.selectors.get('price', [])
        
        for idx, selector in enumerate(price_selectors, 1):
            try:
                logger.info(f"[{idx}/{len(price_selectors)}] ì‹œë„: {selector}")
                
                if selector.startswith('//') or selector.startswith('('):
                    elements = self.driver.find_elements(By.XPATH, selector)
                else:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                
                for element in elements:
                    if element.is_displayed():
                        if self.is_excluded_price_element(element):
                            continue
                        
                        text = self.extract_clean_text_from_element(element)
                        if text:
                            price = self.detect_currency_and_parse_price(text, url)
                            if price:
                                logger.info(f"ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {price}")
                                return price
            except Exception:
                continue
        
        # whole + fraction ì¡°í•© ì‹œë„
        try:
            whole_elem = self.driver.find_element(By.XPATH, "//*[@id='corePrice_feature_div']//span[@class='a-price-whole']")
            fraction_elem = self.driver.find_element(By.XPATH, "//*[@id='corePrice_feature_div']//span[@class='a-price-fraction']")
            
            if whole_elem and fraction_elem and whole_elem.is_displayed() and fraction_elem.is_displayed():
                whole_text = whole_elem.text.strip()
                fraction_text = fraction_elem.text.strip()
                
                if whole_text and fraction_text:
                    fraction_clean = re.sub(r'[^\d]', '', fraction_text)
                    if fraction_clean:
                        # URLì— ë”°ë¼ í˜•ì‹ ê²°ì •
                        if '.de/' in url or 'amazon.de' in url:
                            combined_price = f"{whole_text},{fraction_clean}"
                        else:
                            combined_price = f"{whole_text}.{fraction_clean}"
                        
                        price = self.detect_currency_and_parse_price(combined_price, url)
                        if price:
                            logger.info(f"ì¡°í•© ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {price}")
                            return price
        except Exception:
            pass
        
        logger.warning("ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨")
        return None
    
    def extract_clean_text_from_element(self, element):
        """ìš”ì†Œì—ì„œ ê¹¨ë—í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            text_methods = [
                element.get_attribute('textContent'),
                element.get_attribute('innerText'),
                element.text
            ]
            
            for text in text_methods:
                if text and text.strip():
                    return text.strip()
            
            return None
        except Exception:
            return None
    
    def extract_element_text(self, selectors, element_name="ìš”ì†Œ"):
        """ì„ íƒì ëª©ë¡ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        for selector in selectors:
            try:
                if selector.startswith('//') or selector.startswith('('):
                    elements = self.driver.find_elements(By.XPATH, selector)
                else:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                
                if elements:
                    for element in elements:
                        try:
                            if element.is_displayed():
                                text = self.extract_clean_text_from_element(element)
                                if text:
                                    return text
                        except Exception:
                            continue
            except Exception:
                continue
        
        return None
    
    def extract_ships_from(self, selectors):
        """Ships From ì •ë³´ ì¶”ì¶œ (Fulfilled by Amazon ì²˜ë¦¬)"""
        for selector in selectors:
            try:
                if selector.startswith('//') or selector.startswith('('):
                    elements = self.driver.find_elements(By.XPATH, selector)
                else:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                
                if elements:
                    for element in elements:
                        try:
                            if element.is_displayed():
                                text = self.extract_clean_text_from_element(element)
                                if text:
                                    # Fulfilled by Amazonì¸ ê²½ìš° Amazonë§Œ ë°˜í™˜
                                    if "Fulfilled by Amazon" in text:
                                        return "Amazon"
                                    return text
                        except Exception:
                            continue
            except Exception:
                continue
        
        return None
    
    def check_stock_availability(self, url):
        """ì¬ê³  ìƒíƒœ í™•ì¸"""
        try:
            is_german = '.de/' in url or 'amazon.de' in url
            
            try:
                availability_elem = self.driver.find_element(By.ID, "availability")
                availability_text = availability_elem.text.lower()
                
                if is_german:
                    unavailable_phrases = [
                        'derzeit nicht verfÃ¼gbar', 'nicht auf lager', 'ausverkauft',
                        'currently unavailable', 'out of stock', 'temporarily out of stock'
                    ]
                    available_phrases = [
                        'auf lager', 'verfÃ¼gbar', 'in stock', 'only', 'left in stock'
                    ]
                else:
                    unavailable_phrases = [
                        'currently unavailable', 'out of stock', 'temporarily out of stock'
                    ]
                    available_phrases = [
                        'in stock', 'only', 'left in stock'
                    ]
                
                if any(phrase in availability_text for phrase in unavailable_phrases):
                    return False
                
                if any(phrase in availability_text for phrase in available_phrases):
                    return True
            except:
                pass
            
            buy_buttons = ["add-to-cart-button", "buy-now-button"]
            for button_id in buy_buttons:
                try:
                    button = self.driver.find_element(By.ID, button_id)
                    if button and button.is_enabled():
                        return True
                except:
                    continue
            
            return True
        except Exception:
            return True
    
    def extract_product_info(self, url, row_data, retry_count=0, max_retries=3):
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ"""
        try:
            logger.info("=" * 60)
            logger.info(f"ì œí’ˆ ì •ë³´ ì¶”ì¶œ ì‹œì‘: {url}")
            
            # URL í™•ì¸ (ë…ì¼ ì‚¬ì´íŠ¸ ì—¬ë¶€ íŒë³„ìš©)
            is_german_site = '.de/' in url or 'amazon.de' in url
            
            self.driver.get(url)
            time.sleep(random.uniform(2, 4))
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            try:
                self.wait.until(lambda driver: driver.execute_script("return document.readyState") == "complete")
            except:
                pass
            
            # ì°¨ë‹¨ í˜ì´ì§€ í™•ì¸
            page_source_lower = self.driver.page_source.lower()
            if any(text in page_source_lower for text in ['continue shopping', 'weiter einkaufen']):
                logger.info("ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€ - Continue ë²„íŠ¼ ì‹œë„")
                self.handle_captcha_or_block_page()
                time.sleep(3)
            
            # V2: íƒ€ì„ì¡´ ë¶„ë¦¬

            
            now_time = datetime.now(self.korea_tz)

            
            local_time = datetime.now(self.local_tz)

            # ISO 8601 í˜•ì‹
            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")
            tz_offset = local_time.strftime("%z")
            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"
            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"

            result = {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': 'gb',  # í•­ìƒ gbë¡œ ê³ ì •
                'ships_from': None,
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': None,
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),  # V2: í•œêµ­ì‹œê°„
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],  # V2: í•œêµ­ì‹œê°„ ë¬¸ìì—´
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'title': None,
                'vat': row_data.get('vat', 'x')
            }
            
            # ì œëª© ì¶”ì¶œ
            result['title'] = self.extract_element_text(self.selectors.get('title', []), "ì œëª©")
            
            # ì¬ê³  ìƒíƒœ í™•ì¸
            has_stock = self.check_stock_availability(url)
            
            # ê°€ê²© ì¶”ì¶œ
            result['retailprice'] = self.extract_price(url)
            
            # ê°€ê²© ê²€ì¦
            if result['retailprice']:
                try:
                    price_value = float(re.sub(r'[^\d.]', '', str(result['retailprice'])))
                    if price_value < 5 or price_value > 50000:
                        logger.warning(f"ë¹„ì •ìƒì ì¸ ê°€ê²© ë²”ìœ„: {result['retailprice']}")
                        result['retailprice'] = None
                except:
                    result['retailprice'] = None
            
            # ê°€ê²©ì´ ì—†ê±°ë‚˜ ì¬ê³ ê°€ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ì„¤ì •
            if result['retailprice'] is None:
                result['retailprice'] = None
            
            # íŒë§¤ì ì •ë³´ ì¶”ì¶œ (ìˆ˜ì •ëœ í•¨ìˆ˜ ì‚¬ìš©)
            result['ships_from'] = self.extract_ships_from(self.selectors.get('ships_from', []))
            result['sold_by'] = self.extract_element_text(self.selectors.get('sold_by', []), "Sold By")
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            for selector in self.selectors.get('imageurl', []):
                try:
                    if selector.startswith('//'):
                        element = self.driver.find_element(By.XPATH, selector)
                    else:
                        element = self.driver.find_element(By.CSS_SELECTOR, selector)
                    
                    result['imageurl'] = element.get_attribute('src')
                    if result['imageurl']:
                        break
                except:
                    continue
            
            # VAT í™•ì¸
            # page_source = self.driver.page_source.lower()
            # for vat_text in self.selectors.get('vat_text_list', []):
            #     if vat_text.lower() in page_source:
            #         result['vat'] = 'o'
            #         break
            
            site_type = "ë…ì¼" if is_german_site else "ì˜êµ­"
            logger.info(f"ì‚¬ì´íŠ¸: {site_type} (DB ì €ì¥: gb)")
            logger.info(f"ì œëª©: {result['title']}")
            logger.info(f"ê°€ê²©: {result['retailprice']}")
            logger.info(f"íŒë§¤ì: {result['sold_by']}")
            logger.info(f"ë°°ì†¡ì§€: {result['ships_from']}")
            logger.info(f"ì´ë¯¸ì§€: {result['imageurl']}")
            
            return result
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10
                logger.info(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„... ({retry_count + 1}/{max_retries})")
                time.sleep(wait_time)
                return self.extract_product_info(url, row_data, retry_count + 1, max_retries)
            
            # ì‹¤íŒ¨ ì‹œì—ë„ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜ (ê°€ê²©ì€ 0ìœ¼ë¡œ)
            # V2: íƒ€ì„ì¡´ ë¶„ë¦¬

            now_time = datetime.now(self.korea_tz)

            local_time = datetime.now(self.local_tz)
            return {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': 'gb',  # í•­ìƒ gbë¡œ ê³ ì •
                'ships_from': None,
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': "0",
                'sold_by': None,
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),  # V2: í•œêµ­ì‹œê°„
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],  # V2: í•œêµ­ì‹œê°„ ë¬¸ìì—´
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'title': None,
                'vat': row_data.get('vat', 'x')
            }
    
    def get_uk_crawl_targets(self, limit=None):
        """DBì—ì„œ UK í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ"""
        try:
            query = """
            SELECT *
            FROM samsung_price_tracking_list
            WHERE country = 'gb' 
              AND mall_name = 'amazon'
              AND is_active = TRUE
            """
            
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql(query, self.db_engine)
            logger.info(f"UK í¬ë¡¤ë§ ëŒ€ìƒ {len(df)}ê°œ ì¡°íšŒ")
            return df.to_dict('records')
        except Exception as e:
            logger.error(f"UK í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def save_to_db(self, df):
        """DBì— ê²°ê³¼ ì €ì¥"""
        if self.db_engine is None:
            logger.warning("DB ì—°ê²° ì—†ìŒ")
            return False
        
        try:
            table_name = 'amazon_price_crawl_tbl_uk_v2'
            df.to_sql(table_name, self.db_engine, if_exists='append', index=False)
            logger.info(f"DB ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ")
            return True
        except Exception as e:
            logger.error(f"DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def upload_to_file_server(self, local_file_path, date_folder):
        """íŒŒì¼ì„œë²„ì— ì—…ë¡œë“œ"""
        try:
            transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
            transport.connect(
                username=FILE_SERVER_CONFIG['username'],
                password=FILE_SERVER_CONFIG['password']
            )
            sftp = paramiko.SFTPClient.from_transport(transport)

            # êµ­ê°€ë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            country_dir = f"{FILE_SERVER_CONFIG['upload_path']}/{self.country_code}"

            # êµ­ê°€ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(country_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ êµ­ê°€ ë””ë ‰í† ë¦¬ ìƒì„±: {country_dir}")
                sftp.mkdir(country_dir)

            # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            date_dir = f"{country_dir}/{date_folder}"

            # ë‚ ì§œ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(date_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ ë‚ ì§œ ë””ë ‰í† ë¦¬ ìƒì„±: {date_dir}")
                sftp.mkdir(date_dir)

            # ì—…ë¡œë“œ ê²½ë¡œ
            remote_filename = os.path.basename(local_file_path)
            remote_path = f"{date_dir}/{remote_filename}"

            # íŒŒì¼ ì—…ë¡œë“œ
            sftp.put(local_file_path, remote_path)
            logger.info(f"âœ… íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì™„ë£Œ: {remote_path}")

            sftp.close()
            transport.close()

            return True
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    def save_results(self, df, save_db=True, upload_server=True):
        """ê²°ê³¼ ì €ì¥"""
        now = datetime.now(self.korea_tz)
        date_str = now.strftime('%Y%m%d')
        time_str = now.strftime('%H%M%S')
        base_filename = f"{date_str}_{time_str}_gb_amazon"

        results = {'db_saved': False, 'server_uploaded': False}

        if save_db:
            results['db_saved'] = self.save_to_db(df)

        if upload_server:
            try:
                # 1. CSV íŒŒì¼ ìƒì„±
                csv_filename = f'{base_filename}.csv'
                # Headerë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜
                df.columns = df.columns.str.upper()
                df.to_csv(csv_filename, index=False, encoding='utf-8', lineterminator='\r\n')

                # 2. CSVë¥¼ ZIPìœ¼ë¡œ ì••ì¶•
                zip_filename = f'{base_filename}.zip'
                with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    zipf.write(csv_filename, arcname=csv_filename)

                # 3. MD5 ê³„ì‚°
                def calculate_md5(filename):
                    md5 = hashlib.md5()
                    with open(filename, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b''):
                            md5.update(chunk)
                    return md5.hexdigest()

                csv_md5 = calculate_md5(csv_filename)
                zip_md5 = calculate_md5(zip_filename)

                # 4. MD5 íŒŒì¼ ìƒì„± (ì •í•©ì„± í™•ì¸)
                md5_filename = f'{base_filename}.md5'
                with open(md5_filename, 'w', encoding='utf-8') as f:
                    f.write(f"{os.path.basename(zip_filename)} {zip_md5}\n")
                    f.write(f"{os.path.basename(csv_filename)} {csv_md5}\n")

                # 5. ZIPê³¼ MD5ë¥¼ ë‚ ì§œ í´ë”ì— ì—…ë¡œë“œ
                if self.upload_to_file_server(zip_filename, date_str):
                    if self.upload_to_file_server(md5_filename, date_str):
                        results['server_uploaded'] = True

                # 6. ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                for temp_file in [csv_filename, zip_filename, md5_filename]:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)

                logger.info("ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        return results
    def scrape_urls(self, urls_data, max_items=None):
        """URL ìŠ¤í¬ë˜í•‘"""
        if max_items:
            urls_data = urls_data[:max_items]
        
        logger.info(f"UK í¬ë¡¤ë§ ì‹œì‘ - {len(urls_data)}ê°œ URL")
        
        if not self.setup_driver():
            logger.error("ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨")
            return None
        
        results = []
        failed_urls = []
        
        try:
            for idx, row in enumerate(urls_data):
                url = row.get('url')
                item_name = row.get('item', 'Unknown')
                
                logger.info(f"ì§„í–‰ë¥ : {idx + 1}/{len(urls_data)} - {item_name}")
                
                result = self.extract_product_info(url, row)
                
                if result['retailprice'] == '0':
                    failed_urls.append({
                        'url': url,
                        'item': row.get('item', ''),
                        'reason': 'ê°€ê²© ì—†ìŒ'
                    })
                
                results.append(result)
                
                # ì¤‘ê°„ ì €ì¥
                if (idx + 1) % 10 == 0:
                    interim_df = pd.DataFrame(results[-10:])
                    if self.db_engine:
                        try:
                            table_name = 'amazon_price_crawl_tbl_uk_v2'
                            interim_df.to_sql(table_name, self.db_engine, if_exists='append', index=False)
                            logger.info("ì¤‘ê°„ ì €ì¥: 10ê°œ ë ˆì½”ë“œ")
                        except Exception as e:
                            logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                if idx < len(urls_data) - 1:
                    wait_time = random.uniform(5, 10)
                    time.sleep(wait_time)
                    
                    if (idx + 1) % 20 == 0:
                        logger.info("20ê°œ ì²˜ë¦¬ ì™„ë£Œ, 30ì´ˆ íœ´ì‹")
                        time.sleep(30)
        
        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            if failed_urls:
                logger.warning(f"ì‹¤íŒ¨ URL {len(failed_urls)}ê°œ")
            
            if self.driver:
                try:
                    self.driver.quit()
                except:
                    pass
        
        return pd.DataFrame(results)
    
    def analyze_results(self, df):
        """ê²°ê³¼ ë¶„ì„"""
        logger.info("ê²°ê³¼ ë¶„ì„ ì‹œì‘")
        
        total = len(df)
        with_price = df[df['retailprice'] != '0'].shape[0]
        zero_price = df[df['retailprice'] == '0'].shape[0]
        success_rate = (with_price / total * 100) if total > 0 else 0
        
        logger.info(f"ì „ì²´: {total}ê°œ")
        logger.info(f"ê°€ê²© ì„±ê³µ: {with_price}ê°œ")
        logger.info(f"ê°€ê²© 0: {zero_price}ê°œ")
        logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if with_price > 0:
            try:
                price_df = df[df['retailprice'] != '0'].copy()
                price_df['price_numeric'] = price_df['retailprice'].astype(str).str.replace(',', '').astype(float)
                
                logger.info("ê°€ê²© í†µê³„:")
                logger.info(f"  í‰ê· : {price_df['price_numeric'].mean():.2f}")
                logger.info(f"  ìµœì €: {price_df['price_numeric'].min():.2f}")
                logger.info(f"  ìµœê³ : {price_df['price_numeric'].max():.2f}")
            except Exception as e:
                logger.warning(f"ê°€ê²© í†µê³„ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    max_items = int(os.getenv('MAX_ITEMS', '0')) or None
    
    print("=" * 60)
    print("Amazon UK Price Scraper (ë…ì¼ URL í˜¸í™˜)")
    print("=" * 60)
    if max_items:
        print(f"ìµœëŒ€: {max_items}ê°œ")
    print("=" * 60)
    
    scraper = AmazonUKScraper()
    
    logger.info("í¬ë¡¤ë§ ì‹œì‘")
    if scraper.db_engine is None:
        logger.error("DB ì—°ê²° ì‹¤íŒ¨")
        return
    
    urls_data = scraper.get_uk_crawl_targets(limit=max_items)
    if not urls_data:
        logger.warning("í¬ë¡¤ë§ ëŒ€ìƒ ì—†ìŒ")
        return
    
    results_df = scraper.scrape_urls(urls_data, max_items)
    if results_df is None or results_df.empty:
        logger.error("í¬ë¡¤ë§ ê²°ê³¼ ì—†ìŒ")
        return
    
    scraper.analyze_results(results_df)
    save_results = scraper.save_results(results_df, save_db=True, upload_server=True)
    
    logger.info("ì €ì¥ ê²°ê³¼:")
    logger.info(f"DB: {'ì„±ê³µ' if save_results['db_saved'] else 'ì‹¤íŒ¨'}")
    logger.info(f"íŒŒì¼: {'ì„±ê³µ' if save_results['server_uploaded'] else 'ì‹¤íŒ¨'}")
    logger.info("í¬ë¡¤ë§ ì™„ë£Œ!")

if __name__ == "__main__":
    main()