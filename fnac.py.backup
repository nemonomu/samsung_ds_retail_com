"""
Fnac Í∞ÄÍ≤© Ï∂îÏ∂ú ÏãúÏä§ÌÖú - Undetected Chromedriver Í∏∞Î∞ò Î≤ÑÏ†Ñ
DBÏóêÏÑú URL ÏùΩÏñ¥ÏôÄÏÑú ÌÅ¨Î°§ÎßÅ ÌõÑ Í≤∞Í≥º Ï†ÄÏû•
ÌååÏùºÎ™Ö ÌòïÏãù: {ÏàòÏßëÏùºÏûê}{ÏàòÏßëÏãúÍ∞Ñ}_{Íµ≠Í∞ÄÏΩîÎìú}_{ÏáºÌïëÎ™∞}.csv
"""
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import pandas as pd
import pymysql
from sqlalchemy import create_engine
import paramiko
import time
import random
import re
from datetime import datetime
import pytz
import logging
import os
import json
from io import StringIO
import zipfile
import hashlib

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Import database configuration V2
from config import DB_CONFIG_V2 as DB_CONFIG

from config import FILE_SERVER_CONFIG

class FnacScraper:
    def __init__(self):
        self.driver = None
        self.wait = None
        self.db_engine = None
        self.sftp_client = None
        self.country_code = 'fr'
        # V2: ÌÉÄÏûÑÏ°¥ Î∂ÑÎ¶¨ (ÌòÑÏßÄÏãúÍ∞Ñ + ÌïúÍµ≠ÏãúÍ∞Ñ)
        self.korea_tz = pytz.timezone('Asia/Seoul')
        self.local_tz = pytz.timezone('Europe/Paris')  # Fnac ÌîÑÎûëÏä§ ÌòÑÏßÄ ÏãúÍ∞Ñ

        # DB Ïó∞Í≤∞ ÏÑ§Ï†ï
        self.setup_db_connection()

        # DBÏóêÏÑú XPath Î°úÎìú
        self.load_xpaths_from_db()

    def setup_db_connection(self):
        """DB Ïó∞Í≤∞ ÏÑ§Ï†ï"""
        try:
            # SQLAlchemy ÏóîÏßÑ ÏÉùÏÑ±
            connection_string = (
                f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
                f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
            )
            self.db_engine = create_engine(connection_string)
            logger.info("‚úÖ DB Ïó∞Í≤∞ ÏÑ§Ï†ï ÏôÑÎ£å")

        except Exception as e:
            logger.error(f"‚ùå DB Ïó∞Í≤∞ Ïã§Ìå®: {e}")
            self.db_engine = None

    def load_xpaths_from_db(self):
        """DBÏóêÏÑú FnacÏö© ÏÑ†ÌÉùÏûê Î°úÎìú"""
        try:
            query = """
            SELECT element_type, selector_value, priority
            FROM mall_selectors
            WHERE mall_name = 'fnac'
              AND country_code = 'fr'
              AND is_active = TRUE
            ORDER BY element_type, priority DESC
            """

            df = pd.read_sql(query, self.db_engine)

            # element_typeÎ≥ÑÎ°ú Í∑∏Î£πÌôî
            self.XPATHS = {}
            for element_type in df['element_type'].unique():
                type_selectors = df[df['element_type'] == element_type]['selector_value'].tolist()
                self.XPATHS[element_type] = type_selectors

            logger.info(f"‚úÖ DBÏóêÏÑú ÏÑ†ÌÉùÏûê Î°úÎìú ÏôÑÎ£å: {len(df)}Í∞ú")

            # ÏÉàÎ°úÏö¥ XPathÎ•º Í∏∞Ï°¥ DB XPath ÏïûÏóê Ï∂îÍ∞Ä
            if 'price' in self.XPATHS:
                new_price_selectors = [
                    '.f-faPriceBox__price',  # CSS ÏÑ†ÌÉùÏûê
                    "//span[@class='f-faPriceBox__price userPrice checked']",
                    "//div[@class='f-faPriceBox__priceLine']//span[@class='f-faPriceBox__price']"
                ]
                self.XPATHS['price'] = new_price_selectors + self.XPATHS['price']
                logger.info(f"‚úÖ ÏÉàÎ°úÏö¥ price ÏÑ†ÌÉùÏûê Ï∂îÍ∞ÄÎê®. Ï¥ù price: {len(self.XPATHS['price'])}Í∞ú")
            else:
                self.XPATHS['price'] = [
                    '.f-faPriceBox__price',
                    "//span[@class='f-faPriceBox__price userPrice checked']",
                    "//div[@class='f-faPriceBox__priceLine']//span[@class='f-faPriceBox__price']"
                ]

            # title XPath Ï∂îÍ∞Ä
            if 'title' in self.XPATHS:
                new_title_selectors = [
                    '.f-productHeader__heading',
                    "//h1[@class='f-productHeader__heading']"
                ]
                self.XPATHS['title'] = new_title_selectors + self.XPATHS['title']
                logger.info(f"‚úÖ ÏÉàÎ°úÏö¥ title ÏÑ†ÌÉùÏûê Ï∂îÍ∞ÄÎê®. Ï¥ù title: {len(self.XPATHS['title'])}Í∞ú")
            else:
                self.XPATHS['title'] = [
                    '.f-productHeader__heading',
                    "//h1[@class='f-productHeader__heading']"
                ]

            # imageurl ÏÑ†ÌÉùÏûê Ï∂îÍ∞Ä
            if 'imageurl' in self.XPATHS:
                new_image_selectors = [
                    '.f-productMedias__viewItem--main',
                    "//img[@class='f-productMedias__viewItem--main']"
                ]
                self.XPATHS['imageurl'] = new_image_selectors + self.XPATHS['imageurl']
                logger.info(f"‚úÖ ÏÉàÎ°úÏö¥ imageurl ÏÑ†ÌÉùÏûê Ï∂îÍ∞ÄÎê®. Ï¥ù imageurl: {len(self.XPATHS['imageurl'])}Í∞ú")
            else:
                self.XPATHS['imageurl'] = [
                    '.f-productMedias__viewItem--main',
                    "//img[@class='f-productMedias__viewItem--main']"
                ]

            # Í∏∞Î≥∏Í∞í ÏÑ§Ï†ï (DBÏóê ÏóÜÎäî Í≤ΩÏö∞)
            if not self.XPATHS:
                logger.warning("‚ö†Ô∏è DBÏóê ÏÑ†ÌÉùÏûêÍ∞Ä ÏóÜÏñ¥ Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©")
                self.XPATHS = {
                    'price': [
                        '.f-faPriceBox__price',
                        "//span[@class='f-faPriceBox__price userPrice checked']"
                    ],
                    'title': [
                        '.f-productHeader__heading',
                        "//h1[@class='f-productHeader__heading']"
                    ],
                    'imageurl': [
                        '.f-productMedias__viewItem--main',
                        "//img[@class='f-productMedias__viewItem--main']"
                    ]
                }

        except Exception as e:
            logger.error(f"ÏÑ†ÌÉùÏûê Î°úÎìú Ïã§Ìå®: {e}")
            # Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
            self.XPATHS = {
                'price': [
                    '.f-faPriceBox__price',
                    "//span[@class='f-faPriceBox__price userPrice checked']"
                ],
                'title': [
                    '.f-productHeader__heading',
                    "//h1[@class='f-productHeader__heading']"
                ],
                'imageurl': [
                    '.f-productMedias__viewItem--main',
                    "//img[@class='f-productMedias__viewItem--main']"
                ]
            }

    def get_crawl_targets(self, limit=None, include_failed=False):
        """DBÏóêÏÑú ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅ URL Î™©Î°ù Ï°∞Ìöå"""
        try:
            if include_failed:
                # ÏµúÍ∑º Ïã§Ìå®Ìïú URLÎèÑ Ìè¨Ìï® (24ÏãúÍ∞Ñ Ïù¥ÎÇ¥ Ïã§Ìå® 3Ìöå ÎØ∏Îßå)
                query = """
                WITH failed_counts AS (
                    SELECT url, COUNT(*) as fail_count
                    FROM amazon_crawl_logs
                    WHERE retailprice IS NULL
                      AND crawl_datetime >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                      AND country_code = 'fr'
                    GROUP BY url
                )
                SELECT DISTINCT t.*
                FROM samsung_price_tracking_list t
                LEFT JOIN failed_counts f ON t.url = f.url
                WHERE t.country = 'fr'
                  AND t.mall_name = 'fnac'
                  AND t.is_active = TRUE
                  AND (f.fail_count IS NULL OR f.fail_count < 3)
                ORDER BY COALESCE(f.fail_count, 0) DESC  -- Ïã§Ìå®Ìïú Í≤É Ïö∞ÏÑ†
                """
            else:
                query = """
                SELECT *
                FROM samsung_price_tracking_list
                WHERE country = 'fr'
                  AND mall_name = 'fnac'
                  AND is_active = TRUE
                """

            if limit:
                query += f" LIMIT {limit}"

            df = pd.read_sql(query, self.db_engine)
            logger.info(f"‚úÖ ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅ {len(df)}Í∞ú Ï°∞Ìöå ÏôÑÎ£å")
            return df.to_dict('records')

        except Exception as e:
            logger.error(f"ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅ Ï°∞Ìöå Ïã§Ìå®: {e}")
            return []

    def setup_browser(self):
        """Chrome ÎìúÎùºÏù¥Î≤Ñ ÏÑ§Ï†ï"""
        logger.info("üîß Chrome ÎìúÎùºÏù¥Î≤Ñ ÏÑ§Ï†ï Ï§ë...")

        try:
            options = uc.ChromeOptions()

            options.add_argument('--disable-blink-features=AutomationControlled')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-setuid-sandbox')

            self.driver = uc.Chrome(options=options)
            self.driver.maximize_window()

            self.wait = WebDriverWait(self.driver, 20)

            logger.info("‚úÖ ÎìúÎùºÏù¥Î≤Ñ ÏÑ§Ï†ï ÏôÑÎ£å")
            return True

        except Exception as e:
            logger.error(f"‚ùå ÎìúÎùºÏù¥Î≤Ñ ÏÑ§Ï†ï Ïã§Ìå®: {e}")
            return False

    def wait_for_manual_captcha_solve(self, max_wait_seconds=300):
        """Ï∫°Ï∞®Î•º ÏàòÎèôÏúºÎ°ú Ìï¥Í≤∞Ìï† ÎïåÍπåÏßÄ ÎåÄÍ∏∞"""
        logger.info("üß© Ï∫°Ï∞® Í∞êÏßÄ - ÏàòÎèô Ìï¥Í≤∞ ÎåÄÍ∏∞ Ï§ë...")

        # Ï∫°Ï∞® Í¥ÄÎ†® ÏÑ†ÌÉùÏûêÎì§
        captcha_selectors = [
            "iframe[src*='captcha']",
            "iframe[title*='captcha' i]",
            "iframe[title*='verify' i]",
            "iframe[title*='puzzle' i]",
            "[class*='captcha' i]",
            "[id*='captcha' i]",
            "//div[contains(@class, 'captcha')]",
            "//div[contains(@id, 'captcha')]",
        ]

        # Ïä¨ÎùºÏù¥Îçî ÏÑ†ÌÉùÏûêÎì§ (Ï∫°Ï∞® Ï†ÑÏö©Îßå)
        slider_selectors = [
            ".slider",  # geo.captcha-delivery.com
            "div.slider",
            ".sliderContainer .slider",
            "//div[@class='slider']",
            "//div[@class='sliderContainer']//div[@class='slider']",
            "//div[contains(@class, 'slider') and contains(@class, 'button')]",
            "//div[contains(@class, 'slide-verify')]",
            "//span[contains(@class, 'slider') and contains(@class, 'btn')]",
            "//div[contains(@id, 'nc_') and contains(@class, 'btn')]",  # Alibaba Cloud
            ".captcha-slider-button",
            ".slide-verify-slider-mask-item",
            "#nc_1_n1z"
        ]

        try:
            # 1. Ï∫°Ï∞® Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏
            captcha_found = False
            for selector in captcha_selectors:
                try:
                    if selector.startswith('//'):
                        locator = self.page.locator(f'xpath={selector}')
                    else:
                        locator = self.page.locator(selector)

                    if locator.is_visible(timeout=2000):
                        logger.info(f"üîç Ï∫°Ï∞® ÏöîÏÜå Î∞úÍ≤¨: {selector}")
                        captcha_found = True
                        break
                except:
                    continue

            if not captcha_found:
                logger.info("‚úÖ Ï∫°Ï∞®Í∞Ä Í∞êÏßÄÎêòÏßÄ ÏïäÏùå")
                return True

            # 2. Ï∫°Ï∞®Í∞Ä ÏûàÏúºÎ©¥ ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏïåÎ¶¨Í≥† ÎåÄÍ∏∞
            logger.warning("‚ö†Ô∏è Ï∫°Ï∞®Í∞Ä Í∞êÏßÄÎêòÏóàÏäµÎãàÎã§!")
            logger.warning(f"üí° ÏàòÎèôÏúºÎ°ú Ï∫°Ï∞®Î•º Ìï¥Í≤∞Ìï¥Ï£ºÏÑ∏Ïöî. ÏµúÎåÄ {max_wait_seconds}Ï¥à ÎåÄÍ∏∞Ìï©ÎãàÎã§...")

            # 3. Ï∫°Ï∞®Í∞Ä ÏÇ¨ÎùºÏßà ÎïåÍπåÏßÄ Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÌôïÏù∏
            start_time = time.time()
            check_interval = 2  # 2Ï¥àÎßàÎã§ ÌôïÏù∏

            while (time.time() - start_time) < max_wait_seconds:
                time.sleep(check_interval)

                # Ï∫°Ï∞®Í∞Ä Ïó¨Ï†ÑÌûà ÏûàÎäîÏßÄ ÌôïÏù∏
                still_has_captcha = False
                for selector in captcha_selectors:
                    try:
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        if locator.is_visible(timeout=1000):
                            still_has_captcha = True
                            break
                    except:
                        continue

                if not still_has_captcha:
                    logger.info("‚úÖ Ï∫°Ï∞®Í∞Ä Ìï¥Í≤∞ÎêòÏóàÏäµÎãàÎã§!")
                    return True

                # ÏßÑÌñâ ÏÉÅÌô© ÌëúÏãú
                elapsed = int(time.time() - start_time)
                if elapsed % 10 == 0:  # 10Ï¥àÎßàÎã§
                    logger.info(f"‚è≥ ÎåÄÍ∏∞ Ï§ë... ({elapsed}/{max_wait_seconds}Ï¥à)")

            # ÏãúÍ∞Ñ Ï¥àÍ≥º
            logger.error(f"‚ùå {max_wait_seconds}Ï¥à ÎèôÏïà Ï∫°Ï∞®Í∞Ä Ìï¥Í≤∞ÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§")
            return False

        except Exception as e:
            logger.error(f"‚ùå Ï∫°Ï∞® ÎåÄÍ∏∞ Ï§ë Ïò§Î•ò: {e}")
            return False


    def _drag_slider(self, slider, page_or_frame):
        """Ïä¨ÎùºÏù¥ÎçîÎ•º ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎìúÎûòÍ∑∏"""
        try:
            # Ïä¨ÎùºÏù¥ÎçîÏùò ÏúÑÏπòÏôÄ ÌÅ¨Í∏∞ Í∞ÄÏ†∏Ïò§Í∏∞
            box = slider.bounding_box()
            if not box:
                logger.warning("Ïä¨ÎùºÏù¥Îçî ÏúÑÏπòÎ•º Í∞ÄÏ†∏Ïò¨ Ïàò ÏóÜÏùå")
                return False

            # ÏãúÏûë ÏúÑÏπò (Ïä¨ÎùºÏù¥Îçî Ï§ëÏïô)
            start_x = box['x'] + box['width'] / 2
            start_y = box['y'] + box['height'] / 2

            # ÎìúÎûòÍ∑∏ Í±∞Î¶¨ Í≥ÑÏÇ∞ - sliderTarget ÏúÑÏπòÎ•º Ï∞æÏïÑÏÑú Í∑∏Í≥≥ÏúºÎ°ú ÎìúÎûòÍ∑∏
            drag_distance = 300  # Í∏∞Î≥∏Í∞í

            # 1. canvas ÏöîÏÜåÏùò left style Í∞íÏúºÎ°ú Ï†ïÌôïÌïú ÌçºÏ¶ê Í∞≠ ÏúÑÏπò Ï∞æÍ∏∞
            target_found = False

            # canvas ÏöîÏÜå Ï∞æÍ∏∞
            try:
                canvas = page_or_frame.locator("canvas[style*='left']")
                if canvas.is_visible(timeout=2000):
                    # style ÏÜçÏÑ±ÏóêÏÑú left Í∞í Ï∂îÏ∂ú
                    style_attr = canvas.first.get_attribute('style')
                    if style_attr:
                        # "left: 94px;" Í∞ôÏùÄ ÌòïÏãùÏóêÏÑú Ïà´Ïûê Ï∂îÏ∂ú
                        import re
                        left_match = re.search(r'left:\s*(\d+)px', style_attr)
                        if left_match:
                            puzzle_gap_left = int(left_match.group(1))

                            # Ïä¨ÎùºÏù¥Îçî Ïª®ÌÖåÏù¥ÎÑàÏùò ÏôºÏ™Ω ÏúÑÏπò (Í∏∞Ï§ÄÏ†ê)
                            # canvasÏôÄ sliderÍ∞Ä Í∞ôÏùÄ Ïª®ÌÖåÏù¥ÎÑà ÏïàÏóê ÏûàÎã§Í≥† Í∞ÄÏ†ï
                            slider_container = page_or_frame.locator(".sliderContainer")
                            if slider_container.is_visible(timeout=1000):
                                container_box = slider_container.bounding_box()
                                if container_box:
                                    # ÌçºÏ¶ê Í∞≠Ïùò Ï†àÎåÄ ÏúÑÏπò
                                    target_absolute_left = container_box['x'] + puzzle_gap_left

                                    # Ïä¨ÎùºÏù¥Îçî ÌòÑÏû¨ ÏúÑÏπò
                                    slider_left = box['x']

                                    # ÎìúÎûòÍ∑∏ Í±∞Î¶¨ Í≥ÑÏÇ∞ (Ï†ïÌôïÌïòÍ≤å)
                                    drag_distance = target_absolute_left - slider_left

                                    logger.info(f"üéØ Canvas left Í∏∞Î∞ò ÎìúÎûòÍ∑∏ Í±∞Î¶¨: {drag_distance:.2f}px")
                                    logger.info(f"   Ïª®ÌÖåÏù¥ÎÑà: {container_box['x']:.2f}, ÌçºÏ¶ê Í∞≠: {puzzle_gap_left}px")
                                    logger.info(f"   ÌÉÄÍ≤ü Ï†àÎåÄÏúÑÏπò: {target_absolute_left:.2f}, Ïä¨ÎùºÏù¥Îçî: {slider_left:.2f}")
                                    target_found = True
            except Exception as e:
                logger.debug(f"Canvas ÏúÑÏπò Ï∂îÏ∂ú Ïã§Ìå®: {e}")

            # 2. Ïã§Ìå®ÌïòÎ©¥ Í∏∞Ï°¥ Î∞©ÏãùÏúºÎ°ú sliderTarget Ï∞æÍ∏∞
            if not target_found:
                target_selectors = [
                    ".sliderTarget",
                    "//div[@class='sliderTarget']",
                    ".slide-verify-target",
                    "//div[contains(@class, 'target')]"
                ]

                for target_sel in target_selectors:
                    try:
                        if target_sel.startswith('//'):
                            target = page_or_frame.locator(f'xpath={target_sel}')
                        else:
                            target = page_or_frame.locator(target_sel)

                        if target.is_visible(timeout=1000):
                            target_box = target.bounding_box()
                            if target_box:
                                slider_left = box['x']
                                target_left = target_box['x']
                                drag_distance = target_left - slider_left
                                drag_distance += random.uniform(-2, 2)

                                logger.info(f"üéØ ÌÉÄÍ≤ü ÏúÑÏπò Í∏∞Î∞ò ÎìúÎûòÍ∑∏ Í±∞Î¶¨: {drag_distance:.1f}px")
                                logger.info(f"   Ïä¨ÎùºÏù¥Îçî ÏôºÏ™Ω: {slider_left:.0f}, ÌÉÄÍ≤ü ÏôºÏ™Ω: {target_left:.0f}")
                                target_found = True
                                break
                    except:
                        continue

            # 2. ÌÉÄÍ≤üÏùÑ Î™ª Ï∞æÏúºÎ©¥ Ìä∏Îûô ÎÑàÎπÑ Í∏∞Î∞òÏúºÎ°ú Í≥ÑÏÇ∞ (ÌïòÏßÄÎßå ÎÅùÍπåÏßÄÎäî Ïïà Í∞ê)
            if not target_found:
                track_selectors = [
                    ".sliderContainer",
                    ".sliderbg",
                    "//div[@class='sliderContainer']",
                    "//div[@class='sliderbg']",
                    "//div[contains(@class, 'slider-track')]",
                    "//div[contains(@class, 'slide-track')]",
                    ".slider-track",
                    ".slide-verify-slider-track"
                ]

                for track_sel in track_selectors:
                    try:
                        if track_sel.startswith('//'):
                            track = page_or_frame.locator(f'xpath={track_sel}')
                        else:
                            track = page_or_frame.locator(track_sel)

                        if track.is_visible(timeout=1000):
                            track_box = track.bounding_box()
                            if track_box:
                                # Ìä∏Îûô ÎÑàÎπÑÏùò 70-90% Ï†ïÎèÑÎßå ÎìúÎûòÍ∑∏ (ÎÅùÍπåÏßÄ Í∞ÄÏßÄ ÏïäÏùå)
                                max_distance = track_box['width'] - box['width']
                                drag_distance = max_distance * random.uniform(0.7, 0.9)
                                logger.info(f"üìè Ìä∏Îûô Í∏∞Î∞ò ÎìúÎûòÍ∑∏ Í±∞Î¶¨: {drag_distance:.0f}px (ÏµúÎåÄ: {max_distance:.0f}px)")
                                break
                    except:
                        continue

            # Î™©Ìëú ÏúÑÏπò
            end_x = start_x + drag_distance
            end_y = start_y

            logger.info(f"üñ±Ô∏è Ïä¨ÎùºÏù¥Îçî ÎìúÎûòÍ∑∏: ({start_x:.0f}, {start_y:.0f}) ‚Üí ({end_x:.0f}, {end_y:.0f})")

            # page Í∞ùÏ≤¥ Í∞ÄÏ†∏Ïò§Í∏∞ (FrameÏóêÎäî mouseÍ∞Ä ÏóÜÏúºÎØÄÎ°ú)
            # FrameÏù¥Î©¥ pageÎ•º Í∞ÄÏ†∏Ïò§Í≥†, PageÎ©¥ Í∑∏ÎåÄÎ°ú ÏÇ¨Ïö©
            if hasattr(page_or_frame, 'page'):
                # Frame Í∞ùÏ≤¥
                mouse_obj = page_or_frame.page.mouse
            else:
                # Page Í∞ùÏ≤¥
                mouse_obj = page_or_frame.mouse

            # Îß§Ïö∞ ÏÇ¨ÎûåÎã§Ïö¥ ÎßàÏö∞Ïä§ ÏõÄÏßÅÏûÑ ÏãúÎÆ¨Î†àÏù¥ÏÖò
            # 1. ÎßàÏö∞Ïä§Î•º Ïä¨ÎùºÏù¥ÎçîÎ°ú Ï≤úÏ≤úÌûà Ïù¥Îèô (ÏÇ¨ÎûåÏùÄ Î∞îÎ°ú Ïïà ÏõÄÏßÅÏûÑ)
            mouse_obj.move(start_x, start_y)
            time.sleep(random.uniform(0.3, 0.6))  # ÎßùÏÑ§ÏûÑ

            # 2. ÎßàÏö∞Ïä§ Î≤ÑÌäº ÎàÑÎ•¥Í∏∞ Ï†Ñ ÏßßÏùÄ ÎåÄÍ∏∞
            time.sleep(random.uniform(0.1, 0.2))
            mouse_obj.down()
            time.sleep(random.uniform(0.15, 0.25))  # ÎàÑÎ•∏ ÌõÑ ÏïΩÍ∞Ñ ÎåÄÍ∏∞

            # 3. ÏÇ¨ÎûåÏ≤òÎüº Í∞ÄÎ≥Ä ÏÜçÎèÑÎ°ú ÎìúÎûòÍ∑∏ (ÎäêÎ¶º‚ÜíÎπ†Î¶Ñ‚ÜíÎäêÎ¶º)
            steps = random.randint(25, 40)  # Îçî ÎßéÏùÄ Îã®Í≥Ñ

            for i in range(steps):
                progress = (i + 1) / steps

                # Í∞ÄÏÜçÎèÑ Í≥°ÏÑ† (ease-in-out): Ï≤òÏùåÍ≥º ÎÅùÏùÄ ÎäêÎ¶¨Í≥† Ï§ëÍ∞ÑÏùÄ Îπ†Î¶Ñ
                # ÏÇ¨Ïù∏ Ìï®Ïàò ÏÇ¨Ïö©ÏúºÎ°ú ÏûêÏó∞Ïä§Îü¨Ïö¥ Í∞ÄÏÜç/Í∞êÏÜç
                import math
                eased_progress = (math.sin((progress - 0.5) * math.pi) + 1) / 2

                # ÌòÑÏû¨ x ÏúÑÏπò
                current_x = start_x + (drag_distance * eased_progress)

                # yÏ∂ï ÌùîÎì§Î¶º (ÏÇ¨ÎûåÏùÄ ÏôÑÎ≤ΩÌïòÍ≤å ÏßÅÏÑ†ÏúºÎ°ú Î™ª Í∑∏Ïùå)
                wobble = random.uniform(-3, 3)
                current_y = start_y + wobble

                # ÎßàÏö∞Ïä§ Ïù¥Îèô
                mouse_obj.move(current_x, current_y)

                # Í∞ÄÎ≥Ä ÎîúÎ†àÏù¥ (ÏÇ¨ÎûåÏùÄ ÏùºÏ†ïÌïú ÏÜçÎèÑÎ°ú Ïïà ÏõÄÏßÅÏûÑ)
                base_delay = 0.01
                # Ï≤òÏùåÍ≥º ÎÅùÏùÄ ÎäêÎ¶¨Í≤å, Ï§ëÍ∞ÑÏùÄ Îπ†Î•¥Í≤å
                if progress < 0.2 or progress > 0.8:
                    delay = random.uniform(0.02, 0.04)  # ÎäêÎ¶º
                else:
                    delay = random.uniform(0.005, 0.015)  # Îπ†Î¶Ñ

                time.sleep(delay)

                # Í∞ÄÎÅî Ï§ëÍ∞ÑÏóê ÏïÑÏ£º ÏßßÍ≤å Î©àÏ∂§ (ÏÇ¨ÎûåÏùò ÎØ∏ÏÑ∏Ìïú Ï°∞Ï†ï)
                if random.random() < 0.15:  # 15% ÌôïÎ•†
                    time.sleep(random.uniform(0.05, 0.1))

            # 4. Î™©Ìëú ÏßÄÏ†êÏóê Ï†ïÌôïÌûà ÎèÑÎã¨ (ÎßàÏßÄÎßâÏùÄ Ï†ïÌôïÌïòÍ≤å)
            mouse_obj.move(end_x, end_y)
            time.sleep(random.uniform(0.15, 0.25))

            # 5. ÎßàÏö∞Ïä§ Î≤ÑÌäº ÎÜìÍ∏∞
            mouse_obj.up()

            logger.info("‚úÖ Ïä¨ÎùºÏù¥Îçî ÎìúÎûòÍ∑∏ ÏôÑÎ£å")
            time.sleep(1)

            return True

        except Exception as e:
            logger.error(f"‚ùå Ïä¨ÎùºÏù¥Îçî ÎìúÎûòÍ∑∏ Ïã§Ìå®: {e}")
            return False

    def initialize_session(self):
        """Fnac ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî"""
        logger.info("Fnac ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî...")

        try:
            # Fnac Î©îÏù∏ ÌéòÏù¥ÏßÄ Ï†ëÏÜç (domcontentloadedÎ°ú Î≥ÄÍ≤Ω)
            self.page.goto("https://www.fnac.com", wait_until='domcontentloaded', timeout=30000)
            logger.info("‚úÖ ÌéòÏù¥ÏßÄ Î°úÎìú ÏôÑÎ£å")
            time.sleep(2)

            # Ïø†ÌÇ§ ÌåùÏóÖ Ï≤òÎ¶¨
            try:
                logger.info("üç™ Ïø†ÌÇ§ ÌåùÏóÖ ÌôïÏù∏ Ï§ë...")
                time.sleep(1)  # ÌåùÏóÖÏù¥ ÎÇòÌÉÄÎÇ† ÏãúÍ∞Ñ ÎåÄÍ∏∞

                # "J'accepte" Î≤ÑÌäº ÌÅ¥Î¶≠ (Ïó¨Îü¨ ÏÑ†ÌÉùÏûê ÏãúÎèÑ)
                cookie_selectors = [
                    "text=J'accepte",
                    "button:has-text(\"J'accepte\")",
                    "//button[contains(text(), \"J'accepte\")]",
                    "//button[contains(text(), 'accepte')]",
                    "[class*='accept' i]",
                    "[id*='accept' i]",
                    "button[class*='cookie']",
                    ".didomi-button"
                ]

                cookie_found = False
                for selector in cookie_selectors:
                    try:
                        logger.info(f"üîç Ïø†ÌÇ§ ÏÑ†ÌÉùÏûê ÏãúÎèÑ: {selector}")

                        if selector.startswith('text=') or selector.startswith('button:'):
                            button = self.page.locator(selector).first
                        elif selector.startswith('//'):
                            button = self.page.locator(f'xpath={selector}').first
                        else:
                            button = self.page.locator(selector).first

                        # Î≤ÑÌäºÏù¥ Î≥¥Ïù¥ÎäîÏßÄ ÌôïÏù∏
                        if button.is_visible(timeout=2000):
                            button.click(timeout=3000)
                            logger.info(f"üç™ Ïø†ÌÇ§ ÎèôÏùò ÌåùÏóÖ Ï≤òÎ¶¨ ÏôÑÎ£å (ÏÑ†ÌÉùÏûê: {selector})")
                            time.sleep(2)  # Ïø†ÌÇ§ Ï≤òÎ¶¨ ÌõÑ ÎåÄÍ∏∞ ÏãúÍ∞Ñ Ï¶ùÍ∞Ä
                            cookie_found = True
                            break
                    except Exception as e:
                        logger.debug(f"ÏÑ†ÌÉùÏûê {selector} Ïã§Ìå®: {e}")
                        continue

                if not cookie_found:
                    logger.info("Ïø†ÌÇ§ ÌåùÏóÖÏù¥ ÏóÜÍ±∞ÎÇò Ïù¥ÎØ∏ Ï≤òÎ¶¨Îê®")

            except Exception as e:
                logger.debug(f"Ïø†ÌÇ§ ÌåùÏóÖ Ï≤òÎ¶¨ Ï§ë Ïò§Î•ò (Î¨¥Ïãú): {e}")

            # Ï∫°Ï∞® Í∞êÏßÄ Î∞è ÏàòÎèô Ìï¥Í≤∞ ÎåÄÍ∏∞
            time.sleep(2)  # Ï∫°Ï∞®Í∞Ä ÎÇòÌÉÄÎÇ† ÏãúÍ∞Ñ ÎåÄÍ∏∞
            self.wait_for_manual_captcha_solve(max_wait_seconds=300)  # ÏµúÎåÄ 5Î∂Ñ ÎåÄÍ∏∞

            # ÏÑ∏ÏÖòÏù¥ Ï†úÎåÄÎ°ú ÏÑ§Ï†ïÎêòÏóàÎäîÏßÄ ÌôïÏù∏
            title = self.page.title()
            if "fnac" in title.lower():
                logger.info("‚úÖ Fnac ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
                return True
            else:
                logger.warning("‚ö†Ô∏è ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî Î∂ÄÎ∂Ñ ÏÑ±Í≥µ")
                return True

        except Exception as e:
            logger.error(f"‚ùå ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî Ïã§Ìå®: {e}")
            return False

    def extract_product_info(self, url, row_data, retry_count=0, max_retries=3):
        """Ï†úÌíà Ï†ïÎ≥¥ Ï∂îÏ∂ú (Ï∞®Îã® ÌéòÏù¥ÏßÄ Í∞êÏßÄ Î∞è Ïû¨ÏãúÎèÑ Î°úÏßÅ)"""
        try:
            logger.info(f"üîç ÌéòÏù¥ÏßÄ Ï†ëÏÜç: {url} (ÏãúÎèÑ: {retry_count + 1}/{max_retries + 1})")
            response = self.page.goto(url, wait_until='domcontentloaded', timeout=30000)

            # ÌéòÏù¥ÏßÄ Î°úÎìú ÎåÄÍ∏∞
            time.sleep(random.uniform(3, 5))

            # Ïä¨ÎùºÏù¥Îçî Ï∫°Ï∞®Í∞Ä ÎÇòÌÉÄÎÇ¨ÎäîÏßÄ ÌôïÏù∏ Î∞è ÏàòÎèô Ìï¥Í≤∞ ÎåÄÍ∏∞
            self.wait_for_manual_captcha_solve(max_wait_seconds=120)  # Ï†úÌíà ÌéòÏù¥ÏßÄÎäî 2Î∂ÑÎßå ÎåÄÍ∏∞

            # 404 ÏóêÎü¨ Ï≤¥ÌÅ¨ (Î¥á Í∞êÏßÄÎ°ú Ïù∏Ìïú 404 ÏúÑÏû• Í∞ÄÎä•ÏÑ±)
            if response and response.status == 404:
                logger.warning("‚ö†Ô∏è 404 ÏóêÎü¨ Í∞êÏßÄ - Î¥á Í∞êÏßÄ Í∞ÄÎä•ÏÑ±, Ïû¨Ï†ëÏÜç ÏãúÎèÑ")

                # Ïû†Ïãú ÎåÄÍ∏∞
                time.sleep(random.uniform(3, 5))

                # Î∞îÎ°ú ÏõêÎûò URL Ïû¨Ï†ëÏÜç (Î©îÏù∏ ÌéòÏù¥ÏßÄ Í±∞ÏπòÏßÄ ÏïäÏùå)
                logger.info(f"üîÑ URL ÏßÅÏ†ë Ïû¨Ï†ëÏÜç: {url}")
                response = self.page.goto(url, wait_until='domcontentloaded', timeout=30000)
                time.sleep(random.uniform(3, 5))

                # Ïó¨Ï†ÑÌûà 404Ïù¥Î©¥ ÏóêÎü¨ Î∞úÏÉù
                if response and response.status == 404:
                    logger.error("‚ùå Ïû¨Ï†ëÏÜç ÌõÑÏóêÎèÑ 404 ÏóêÎü¨ - URLÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÍ±∞ÎÇò Ï∞®Îã®Îê®")
                    raise Exception("404 error after retry - possible blocked or invalid URL")
                else:
                    logger.info("‚úÖ Ïû¨Ï†ëÏÜç ÏÑ±Í≥µ")

            # ÌòÑÏû¨ ÏãúÍ∞Ñ
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)

            # ISO 8601 ÌòïÏãù
            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")
            tz_offset = local_time.strftime("%z")
            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"
            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"

            # Í∏∞Î≥∏ Í≤∞Í≥º Íµ¨Ï°∞
            result = {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'fr'),
                'ships_from': 'FR',
                'channel_name': 'fnac',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'Fnac',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': row_data.get('vat', 'o')
            }

            # Ï†úÎ™© Ï∂îÏ∂ú (Ï∞®Îã® ÌéòÏù¥ÏßÄ Í∞êÏßÄ)
            title_extracted = False
            try:
                for selector in self.XPATHS.get('title', []):
                    try:
                        # XPathÏù∏ÏßÄ CSSÏù∏ÏßÄ ÌåêÎã®
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        # ÏöîÏÜåÍ∞Ä ÎÇòÌÉÄÎÇ† ÎïåÍπåÏßÄ ÎåÄÍ∏∞ (ÏµúÎåÄ 5Ï¥à)
                        locator.wait_for(state='visible', timeout=5000)
                        title_text = locator.inner_text()

                        if title_text and title_text.strip():
                            result['title'] = title_text.strip()
                            logger.info(f"Ï†úÎ™©: {result['title']}")
                            title_extracted = True
                            break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"Ï†úÎ™© Ï∂îÏ∂ú Ïã§Ìå®: {e}")

            # Ï∞®Îã® ÌéòÏù¥ÏßÄ Í∞êÏßÄ Î∞è Ïû¨ÏãúÎèÑ Î°úÏßÅ
            if not title_extracted:
                logger.warning("‚ö†Ô∏è Ï†úÎ™© Ï∂îÏ∂ú Ïã§Ìå® - Ï∞®Îã® ÌéòÏù¥ÏßÄ Í∞ÄÎä•ÏÑ±")

                # 1Îã®Í≥Ñ: ÏÉàÎ°úÍ≥†Ïπ® ÏãúÎèÑ
                logger.info("üîÑ ÏÉàÎ°úÍ≥†Ïπ® ÏãúÎèÑ...")
                self.page.reload(wait_until='networkidle', timeout=30000)
                time.sleep(random.uniform(3, 5))

                # Ï†úÎ™© Ïû¨Ï∂îÏ∂ú ÏãúÎèÑ
                for selector in self.XPATHS.get('title', []):
                    try:
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        locator.wait_for(state='visible', timeout=5000)
                        title_text = locator.inner_text()

                        if title_text and title_text.strip():
                            result['title'] = title_text.strip()
                            logger.info(f"‚úÖ ÏÉàÎ°úÍ≥†Ïπ® ÌõÑ Ï†úÎ™© Ï∂îÏ∂ú ÏÑ±Í≥µ: {result['title']}")
                            title_extracted = True
                            break
                    except:
                        continue

                # 2Îã®Í≥Ñ: Ïó¨Ï†ÑÌûà Ïã§Ìå®ÌïòÎ©¥ fnac.com Ï†ëÏÜç ÌõÑ Ïû¨ÏãúÎèÑ
                if not title_extracted:
                    logger.warning("‚ö†Ô∏è ÏÉàÎ°úÍ≥†Ïπ® ÌõÑÏóêÎèÑ Ïã§Ìå® - fnac.com Ï†ëÏÜç ÌõÑ Ïû¨ÏãúÎèÑ")

                    # Fnac Î©îÏù∏ ÌéòÏù¥ÏßÄ Ï†ëÏÜç
                    self.page.goto("https://www.fnac.com", wait_until='networkidle', timeout=30000)
                    time.sleep(random.uniform(2, 4))

                    # ÏõêÎûò URL Ïû¨Ï†ëÏÜç
                    logger.info(f"üîÑ ÏõêÎûò URL Ïû¨Ï†ëÏÜç: {url}")
                    self.page.goto(url, wait_until='networkidle', timeout=30000)
                    time.sleep(random.uniform(3, 5))

                    # Ï†úÎ™© Ïû¨Ï∂îÏ∂ú ÏãúÎèÑ
                    for selector in self.XPATHS.get('title', []):
                        try:
                            if selector.startswith('//'):
                                locator = self.page.locator(f'xpath={selector}')
                            else:
                                locator = self.page.locator(selector)

                            locator.wait_for(state='visible', timeout=5000)
                            title_text = locator.inner_text()

                            if title_text and title_text.strip():
                                result['title'] = title_text.strip()
                                logger.info(f"‚úÖ Ïû¨Ï†ëÏÜç ÌõÑ Ï†úÎ™© Ï∂îÏ∂ú ÏÑ±Í≥µ: {result['title']}")
                                title_extracted = True
                                break
                        except:
                            continue

                    # Ïó¨Ï†ÑÌûà Ïã§Ìå®ÌïòÎ©¥ ÏóêÎü¨Î°ú Ï≤òÎ¶¨
                    if not title_extracted:
                        logger.error("‚ùå Î™®Îì† Ïû¨ÏãúÎèÑ ÌõÑÏóêÎèÑ Ï†úÎ™© Ï∂îÏ∂ú Ïã§Ìå® - Ï∞®Îã®Îêú Í≤ÉÏúºÎ°ú ÌåêÎã®")
                        raise Exception("Title extraction failed - possible blocked page")

            # Í∞ÄÍ≤© Ï∂îÏ∂ú
            try:
                price_found = False

                logger.info("üîç Í∞ÄÍ≤© Ï∂îÏ∂ú ÏãúÎèÑ...")

                for selector in self.XPATHS.get('price', []):
                    try:
                        logger.info(f"üîç ÏÑ†ÌÉùÏûê ÏãúÎèÑ: {selector}")

                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        locator.wait_for(state='visible', timeout=5000)
                        price_text = locator.inner_text()
                        logger.info(f"üîç Ï∂îÏ∂úÌïú ÌÖçÏä§Ìä∏: '{price_text}'")

                        if price_text and price_text.strip():
                            # Fnac ÌîÑÎûëÏä§ Í∞ÄÍ≤© ÌòïÏãù: "419,99 ‚Ç¨" ÎòêÎäî "419,99‚Ç¨"
                            # ÏâºÌëúÎ•º Ï†êÏúºÎ°ú Î≥ÄÌôò, ‚Ç¨ Í∏∞Ìò∏ Ï†úÍ±∞
                            price_text_clean = price_text.replace(',', '.').replace('‚Ç¨', '').replace('\xa0', '').strip()
                            price_match = re.search(r'(\d+\.?\d*)', price_text_clean)
                            if price_match:
                                price_number = price_match.group(1)
                                result['retailprice'] = float(price_number)
                                logger.info(f"‚úÖ Í∞ÄÍ≤© Ï∂îÏ∂ú ÏÑ±Í≥µ: ‚Ç¨{result['retailprice']}")
                                price_found = True
                                break

                    except Exception as e:
                        logger.warning(f"‚ùå ÏÑ†ÌÉùÏûê {selector} Ïã§Ìå®: {e}")
                        continue

                # JavaScriptÎ°ú Í∞ÄÍ≤© Ï∞æÍ∏∞ (ÏµúÌõÑ ÏàòÎã®)
                if not price_found:
                    try:
                        js_result = self.page.evaluate("""
                            () => {
                                var priceSelectors = [
                                    '.f-faPriceBox__price',
                                    '[class*="price"]',
                                    'span[class*="Price"]'
                                ];

                                for (var i = 0; i < priceSelectors.length; i++) {
                                    var elements = document.querySelectorAll(priceSelectors[i]);
                                    for (var j = 0; j < elements.length; j++) {
                                        var text = elements[j].textContent || elements[j].innerText;
                                        if (text && /\\d/.test(text) && text.includes('‚Ç¨')) {
                                            return text.trim();
                                        }
                                    }
                                }
                                return null;
                            }
                        """)

                        if js_result:
                            logger.info(f"üîç JavaScriptÏóêÏÑú Ï∂îÏ∂úÌïú ÌÖçÏä§Ìä∏: '{js_result}'")
                            price_text_clean = js_result.replace(',', '.').replace('‚Ç¨', '').replace('\xa0', '').strip()
                            price_match = re.search(r'(\d+\.?\d*)', price_text_clean)
                            if price_match:
                                price_number = price_match.group(1)
                                result['retailprice'] = float(price_number)
                                logger.info(f"‚úÖ Í∞ÄÍ≤© Ï∂îÏ∂ú ÏÑ±Í≥µ (JS): ‚Ç¨{result['retailprice']}")
                                price_found = True
                    except Exception as e:
                        logger.debug(f"JavaScript Í∞ÄÍ≤© Ï∂îÏ∂ú Ïã§Ìå®: {e}")

                if not price_found:
                    logger.warning("Î™®Îì† Í∞ÄÍ≤© Ï∂îÏ∂ú Î∞©Î≤ï Ïã§Ìå®")

            except Exception as e:
                logger.warning(f"Í∞ÄÍ≤© Ï∂îÏ∂ú Ïã§Ìå®: {e}")

            # Ïù¥ÎØ∏ÏßÄ URL Ï∂îÏ∂ú
            try:
                image_found = False

                # 1. ÏÑ†ÌÉùÏûêÎì§ ÏãúÎèÑ
                for selector in self.XPATHS.get('imageurl', []):
                    try:
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        locator.wait_for(state='visible', timeout=5000)
                        src = locator.get_attribute('src')

                        if src and 'fnac-static.com' in src:
                            result['imageurl'] = src
                            logger.info(f"Ïù¥ÎØ∏ÏßÄ URL: {result['imageurl']}")
                            image_found = True
                            break
                    except:
                        continue

                # 2. JavaScriptÎ°ú Ïù¥ÎØ∏ÏßÄ Ï∞æÍ∏∞
                if not image_found:
                    try:
                        js_result = self.page.evaluate("""
                            () => {
                                var imgs = document.querySelectorAll('img');
                                for (var i = 0; i < imgs.length; i++) {
                                    var src = imgs[i].src || imgs[i].getAttribute('data-src');
                                    if (src && src.includes('fnac-static.com')) {
                                        return src;
                                    }
                                }
                                return null;
                            }
                        """)

                        if js_result:
                            result['imageurl'] = js_result
                            logger.info(f"Ïù¥ÎØ∏ÏßÄ URL (JS): {result['imageurl']}")
                            image_found = True
                    except:
                        pass

                if not image_found:
                    logger.warning("Ïù¥ÎØ∏ÏßÄ URL Ï∂îÏ∂ú Ïã§Ìå®")

            except Exception as e:
                logger.warning(f"Ïù¥ÎØ∏ÏßÄ URL Ï∂îÏ∂ú Ïã§Ìå®: {e}")

            return result

        except Exception as e:
            logger.error(f"‚ùå ÌéòÏù¥ÏßÄ Ï≤òÎ¶¨ Ïò§Î•ò: {e}")

            # Ïû¨ÏãúÎèÑ Î°úÏßÅ
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10
                logger.info(f"üîÑ {wait_time}Ï¥à ÌõÑ Ïû¨ÏãúÎèÑÌï©ÎãàÎã§... (Ïû¨ÏãúÎèÑ {retry_count + 1}/{max_retries})")
                time.sleep(wait_time)

                # Ïû¨Í∑Ä Ìò∏Ï∂úÎ°ú Ïû¨ÏãúÎèÑ
                return self.extract_product_info(url, row_data, retry_count + 1, max_retries)

            # ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò Ï¥àÍ≥º Ïãú Í∏∞Î≥∏Í∞í Î∞òÌôò
            logger.error(f"‚ùå ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò Ï¥àÍ≥º: {url}")
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)

            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")
            tz_offset = local_time.strftime("%z")
            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"
            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"

            return {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'fr'),
                'ships_from': 'FR',
                'channel_name': 'fnac',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'Fnac',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': row_data.get('vat', 'o')
            }

    def save_to_db(self, df):
        """DBÏóê Í≤∞Í≥º Ï†ÄÏû•"""
        if self.db_engine is None:
            logger.warning("‚ö†Ô∏è DB Ïó∞Í≤∞Ïù¥ ÏóÜÏñ¥ DB Ï†ÄÏû•ÏùÑ Í±¥ÎÑàÎúÅÎãàÎã§")
            return False

        try:
            # fnac_price_crawl_tbl_fr ÌÖåÏù¥Î∏îÏóê Ï†ÄÏû•
            df.to_sql('fnac_price_crawl_tbl_fr', self.db_engine, if_exists='append', index=False)
            logger.info(f"‚úÖ DB Ï†ÄÏû• ÏôÑÎ£å: {len(df)}Í∞ú Î†àÏΩîÎìú")

            # ÌÅ¨Î°§ÎßÅ Î°úÍ∑∏Î•º pandas DataFrameÏúºÎ°ú ÎßåÎì§Ïñ¥ÏÑú ÌïúÎ≤àÏóê Ï†ÄÏû•
            log_records = []
            for _, row in df.iterrows():
                log_records.append({
                    'country_code': 'fr',
                    'url': row['producturl'],
                    'error_message': None if row['retailprice'] is not None else 'Price not found',
                    'execution_time': random.uniform(3, 10),
                    'retailprice': row['retailprice'],
                    'crawl_datetime': row['crawl_datetime']
                })

            if log_records:
                log_df = pd.DataFrame(log_records)
                log_df.to_sql('amazon_crawl_logs', self.db_engine, if_exists='append', index=False)
                logger.info(f"‚úÖ ÌÅ¨Î°§ÎßÅ Î°úÍ∑∏ Ï†ÄÏû• ÏôÑÎ£å: {len(log_records)}Í∞ú")

            return True

        except Exception as e:
            logger.error(f"‚ùå DB Ï†ÄÏû• Ïã§Ìå®: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def upload_to_file_server(self, local_file_path, date_folder):
        """ÌååÏùºÏÑúÎ≤ÑÏóê ÏóÖÎ°úÎìú"""
        try:
            transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
            transport.connect(
                username=FILE_SERVER_CONFIG['username'],
                password=FILE_SERVER_CONFIG['password']
            )
            sftp = paramiko.SFTPClient.from_transport(transport)

            # Íµ≠Í∞ÄÎ≥Ñ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
            country_dir = f"{FILE_SERVER_CONFIG['upload_path']}/{self.country_code}"

            # Íµ≠Í∞Ä ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
            try:
                sftp.stat(country_dir)
            except FileNotFoundError:
                logger.info(f"üìÅ Íµ≠Í∞Ä ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±: {country_dir}")
                sftp.mkdir(country_dir)

            # ÎÇ†ÏßúÎ≥Ñ ÎîîÎ†âÌÜ†Î¶¨ Í≤ΩÎ°ú
            date_dir = f"{country_dir}/{date_folder}"

            # ÎÇ†Ïßú ÎîîÎ†âÌÜ†Î¶¨Í∞Ä ÏóÜÏúºÎ©¥ ÏÉùÏÑ±
            try:
                sftp.stat(date_dir)
            except FileNotFoundError:
                logger.info(f"üìÅ ÎÇ†Ïßú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±: {date_dir}")
                sftp.mkdir(date_dir)

            # ÏóÖÎ°úÎìú Í≤ΩÎ°ú
            remote_filename = os.path.basename(local_file_path)
            remote_path = f"{date_dir}/{remote_filename}"

            # ÌååÏùº ÏóÖÎ°úÎìú
            sftp.put(local_file_path, remote_path)
            logger.info(f"‚úÖ ÌååÏùºÏÑúÎ≤Ñ ÏóÖÎ°úÎìú ÏôÑÎ£å: {remote_path}")

            sftp.close()
            transport.close()

            return True
        except Exception as e:
            logger.error(f"‚ùå ÌååÏùºÏÑúÎ≤Ñ ÏóÖÎ°úÎìú Ïã§Ìå®: {e}")
            return False

    def save_results(self, df, save_db=True, upload_server=True):
        """Í≤∞Í≥º Ï†ÄÏû•"""
        now = datetime.now(self.korea_tz)
        date_str = now.strftime('%Y%m%d')
        time_str = now.strftime('%H%M%S')
        base_filename = f"{date_str}_{time_str}_fr_fnac"

        results = {'db_saved': False, 'server_uploaded': False}

        if save_db:
            results['db_saved'] = self.save_to_db(df)

        if upload_server:
            try:
                # 1. CSV ÌååÏùº ÏÉùÏÑ±
                csv_filename = f'{base_filename}.csv'
                # HeaderÎ•º ÎåÄÎ¨∏ÏûêÎ°ú Î≥ÄÌôò
                df.columns = df.columns.str.upper()
                df.to_csv(csv_filename, index=False, encoding='utf-8', lineterminator='\r\n')

                # 2. CSVÎ•º ZIPÏúºÎ°ú ÏïïÏ∂ï
                zip_filename = f'{base_filename}.zip'
                with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    zipf.write(csv_filename, arcname=csv_filename)

                # 3. MD5 Í≥ÑÏÇ∞
                def calculate_md5(filename):
                    md5 = hashlib.md5()
                    with open(filename, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b''):
                            md5.update(chunk)
                    return md5.hexdigest()

                csv_md5 = calculate_md5(csv_filename)
                zip_md5 = calculate_md5(zip_filename)

                # 4. MD5 ÌååÏùº ÏÉùÏÑ±
                md5_filename = f'{base_filename}.md5'
                with open(md5_filename, 'w', encoding='utf-8') as f:
                    f.write(f"{os.path.basename(zip_filename)} {zip_md5}\n")
                    f.write(f"{os.path.basename(csv_filename)} {csv_md5}\n")

                # 5. ZIPÍ≥º MD5Î•º ÎÇ†Ïßú Ìè¥ÎçîÏóê ÏóÖÎ°úÎìú
                if self.upload_to_file_server(zip_filename, date_str):
                    if self.upload_to_file_server(md5_filename, date_str):
                        results['server_uploaded'] = True

                # 6. Î°úÏª¨ ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú
                for temp_file in [csv_filename, zip_filename, md5_filename]:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)

                logger.info("ÏûÑÏãú ÌååÏùº ÏÇ≠Ï†ú ÏôÑÎ£å")
            except Exception as e:
                logger.error(f"ÌååÏùº Ï†ÄÏû• Ïã§Ìå®: {e}")

        return results

    def test_connection(self):
        """Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ Î∞è ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî"""
        logger.info("=== Fnac ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî Î∞è ÌÖåÏä§Ìä∏ ===")

        if not self.setup_browser():
            return False

        try:
            # 1Îã®Í≥Ñ: Google Ïó∞Í≤∞ ÌÖåÏä§Ìä∏
            logger.info("1Îã®Í≥Ñ: Google Ïó∞Í≤∞ ÌÖåÏä§Ìä∏...")
            self.page.goto("https://www.google.com", wait_until='networkidle', timeout=30000)
            time.sleep(2)
            google_title = self.page.title()

            if "Google" in google_title:
                logger.info("‚úÖ Google Ï†ëÏÜç ÏÑ±Í≥µ")
            else:
                logger.warning("‚ö†Ô∏è Google Ï†ëÏÜç Ïù¥ÏÉÅ")

            # 2Îã®Í≥Ñ: Fnac ÏÑ∏ÏÖò Ï¥àÍ∏∞Ìôî
            if not self.initialize_session():
                return False

            # 3Îã®Í≥Ñ: ÌÖåÏä§Ìä∏ ÏÉÅÌíà ÌéòÏù¥ÏßÄ Ï†ëÏÜç (DBÏóêÏÑú Ïã§Ï†ú URL Í∞ÄÏ†∏Ïò§Í∏∞)
            logger.info("3Îã®Í≥Ñ: ÌÖåÏä§Ìä∏ ÏÉÅÌíà ÌéòÏù¥ÏßÄ Ï†ëÏÜç...")

            # DBÏóêÏÑú Ï≤´ Î≤àÏß∏ Ï†úÌíà Í∞ÄÏ†∏Ïò§Í∏∞
            test_products = self.get_crawl_targets(limit=1)
            if not test_products:
                logger.warning("‚ö†Ô∏è ÌÖåÏä§Ìä∏Ïö© Ï†úÌíàÏù¥ ÏóÜÏñ¥ Í±¥ÎÑàÎúÅÎãàÎã§")
                return True

            test_row = test_products[0]
            test_url = test_row.get('url')

            test_result = self.extract_product_info(test_url, test_row)

            logger.info("Ï∂îÏ∂úÎêú Ï†ïÎ≥¥:")
            logger.info(f"  - ÏÉÅÌíàÎ™Ö: {test_result['title']}")
            logger.info(f"  - Í∞ÄÍ≤©: ‚Ç¨{test_result['retailprice']}")
            logger.info(f"  - Ïù¥ÎØ∏ÏßÄ: {'Ï∂îÏ∂úÎê®' if test_result['imageurl'] else 'ÏóÜÏùå'}")

            # 4Îã®Í≥Ñ: ÌååÏùºÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏
            logger.info("4Îã®Í≥Ñ: ÌååÏùºÏÑúÎ≤Ñ Ïó∞Í≤∞ ÌÖåÏä§Ìä∏...")
            try:
                transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
                transport.connect(
                    username=FILE_SERVER_CONFIG['username'],
                    password=FILE_SERVER_CONFIG['password']
                )
                transport.close()
                logger.info("‚úÖ ÌååÏùºÏÑúÎ≤Ñ Ïó∞Í≤∞ ÏÑ±Í≥µ")
            except:
                logger.warning("‚ö†Ô∏è ÌååÏùºÏÑúÎ≤Ñ Ïó∞Í≤∞ Ïã§Ìå® - ÌÅ¨Î°§ÎßÅÏùÄ Í≥ÑÏÜç ÏßÑÌñâ")

            if test_result['retailprice'] or test_result['title']:
                logger.info("‚úÖ Ï†ïÎ≥¥ Ï∂îÏ∂ú ÏÑ±Í≥µ - ÌÅ¨Î°§ÎßÅ Ï§ÄÎπÑ ÏôÑÎ£å!")
                return True
            else:
                logger.warning("‚ö†Ô∏è Ï†ïÎ≥¥ Ï∂îÏ∂ú Î∂ÄÎ∂Ñ Ïã§Ìå® - Í∑∏ÎûòÎèÑ Í≥ÑÏÜç ÏßÑÌñâ")
                return True

        except Exception as e:
            logger.error(f"‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®: {e}")
            return False

    def scrape_urls(self, urls_data, max_items=None):
        """Ïó¨Îü¨ URL Ïä§ÌÅ¨ÎûòÌïë"""
        if max_items:
            urls_data = urls_data[:max_items]

        logger.info(f"üìä Ï¥ù {len(urls_data)}Í∞ú Ï†úÌíà Ï≤òÎ¶¨ ÏãúÏûë")

        results = []
        failed_urls = []

        try:
            for idx, row in enumerate(urls_data):
                logger.info(f"\n{'='*50}")
                logger.info(f"ÏßÑÌñâÎ•†: {idx + 1}/{len(urls_data)} ({(idx + 1)/len(urls_data)*100:.1f}%)")

                url = row.get('url')
                result = self.extract_product_info(url, row)

                if result['retailprice'] is None:
                    failed_urls.append({
                        'url': url,
                        'item': row.get('item', ''),
                        'brand': row.get('brand', '')
                    })

                results.append(result)

                # 10Í∞úÎßàÎã§ DBÏóê Ï§ëÍ∞Ñ Ï†ÄÏû•
                if (idx + 1) % 10 == 0:
                    interim_df = pd.DataFrame(results[-10:])
                    if self.db_engine:
                        try:
                            interim_df.to_sql('fnac_price_crawl_tbl_fr', self.db_engine,
                                            if_exists='append', index=False)
                            logger.info(f"üíæ Ï§ëÍ∞Ñ Ï†ÄÏû•: 10Í∞ú Î†àÏΩîÎìú DB Ï†ÄÏû•")
                        except Exception as e:
                            logger.error(f"Ï§ëÍ∞Ñ Ï†ÄÏû• Ïã§Ìå®: {e}")

                # Îã§Ïùå ÏöîÏ≤≠ Ï†Ñ ÎåÄÍ∏∞
                if idx < len(urls_data) - 1:
                    wait_time = random.uniform(2, 5)
                    logger.info(f"‚è≥ {wait_time:.1f}Ï¥à ÎåÄÍ∏∞ Ï§ë...")
                    time.sleep(wait_time)

                    if (idx + 1) % 10 == 0:
                        logger.info("‚òï 10Í∞ú Ï≤òÎ¶¨ ÏôÑÎ£å, 30Ï¥à Ìú¥Ïãù...")
                        time.sleep(30)

        except Exception as e:
            logger.error(f"‚ùå Ïä§ÌÅ¨ÎûòÌïë Ï§ë Ïò§Î•ò: {e}")

        finally:
            if failed_urls:
                logger.warning(f"\n‚ö†Ô∏è Í∞ÄÍ≤© Ï∂îÏ∂ú Ïã§Ìå®Ìïú URL {len(failed_urls)}Í∞ú:")
                for fail in failed_urls[:5]:
                    logger.warning(f"  - {fail['brand']} {fail['item']}: {fail['url']}")
                if len(failed_urls) > 5:
                    logger.warning(f"  ... Ïô∏ {len(failed_urls) - 5}Í∞ú")

            if self.browser:
                self.browser.close()
                logger.info("üîß Î∏åÎùºÏö∞Ï†Ä Ï¢ÖÎ£å")

            if self.playwright:
                self.playwright.stop()
                logger.info("üîß Playwright Ï¢ÖÎ£å")

        return pd.DataFrame(results)

    def analyze_results(self, df):
        """Í≤∞Í≥º Î∂ÑÏÑù"""
        logger.info("\nüìä === Í≤∞Í≥º Î∂ÑÏÑù ===")

        total = len(df)
        with_price = df['retailprice'].notna().sum()
        without_price = df['retailprice'].isna().sum()
        success_rate = (with_price / total * 100) if total > 0 else 0

        logger.info(f"Ï†ÑÏ≤¥ Ï†úÌíà: {total}Í∞ú")
        logger.info(f"Í∞ÄÍ≤© Ï∂îÏ∂ú ÏÑ±Í≥µ: {with_price}Í∞ú")
        logger.info(f"Í∞ÄÍ≤© Ï∂îÏ∂ú Ïã§Ìå®: {without_price}Í∞ú")
        logger.info(f"ÏÑ±Í≥µÎ•†: {success_rate:.1f}%")

        if with_price > 0:
            price_df = df[df['retailprice'].notna()].copy()
            price_df['numeric_price'] = price_df['retailprice']

            logger.info(f"\nüí∞ Í∞ÄÍ≤© ÌÜµÍ≥Ñ:")
            logger.info(f"ÌèâÍ∑†Í∞Ä: ‚Ç¨{price_df['numeric_price'].mean():.2f}")
            logger.info(f"ÏµúÏ†ÄÍ∞Ä: ‚Ç¨{price_df['numeric_price'].min():.2f}")
            logger.info(f"ÏµúÍ≥†Í∞Ä: ‚Ç¨{price_df['numeric_price'].max():.2f}")
            logger.info(f"Ï§ëÍ∞ÑÍ∞í: ‚Ç¨{price_df['numeric_price'].median():.2f}")

def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    print("\nüöÄ Fnac Í∞ÄÍ≤© Ï∂îÏ∂ú ÏãúÏä§ÌÖú - Playwright Í∏∞Î∞ò Î≤ÑÏ†Ñ")
    print("="*60)

    scraper = FnacScraper()

    if scraper.db_engine is None:
        logger.error("DB Ïó∞Í≤∞ Ïã§Ìå®Î°ú Ï¢ÖÎ£åÌï©ÎãàÎã§.")
        return

    # ÌÖåÏä§Ìä∏ Î™®Îìú
    test_mode = os.getenv("TEST_MODE", "false").lower()

    if test_mode in ["true", "1", "yes"]:
        logger.info("üß™ ÌÖåÏä§Ìä∏ Î™®Îìú Ïã§Ìñâ")

        if scraper.test_connection():
            logger.info("‚úÖ ÌÖåÏä§Ìä∏ ÏôÑÎ£å")
        else:
            logger.error("‚ùå ÌÖåÏä§Ìä∏ Ïã§Ìå®")

        if scraper.browser:
            scraper.browser.close()
        if scraper.playwright:
            scraper.playwright.stop()
        return

    # Ïã§Ï†ú ÌÅ¨Î°§ÎßÅ
    logger.info("\nüìä Ïã§Ï†ú ÌÅ¨Î°§ÎßÅ ÏãúÏûë")

    if not scraper.test_connection():
        logger.error("Ïó∞Í≤∞ ÌÖåÏä§Ìä∏ Ïã§Ìå®Î°ú Ï¢ÖÎ£åÌï©ÎãàÎã§.")
        return

    urls_data = scraper.get_crawl_targets()

    if not urls_data:
        logger.warning("ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅÏù¥ ÏóÜÏäµÎãàÎã§.")
        return

    logger.info(f"‚úÖ ÌÅ¨Î°§ÎßÅ ÎåÄÏÉÅ: {len(urls_data)}Í∞ú")

    start_time = datetime.now(scraper.korea_tz)
    results_df = scraper.scrape_urls(urls_data)

    if results_df is None or results_df.empty:
        logger.error("ÌÅ¨Î°§ÎßÅ Í≤∞Í≥ºÍ∞Ä ÏóÜÏäµÎãàÎã§.")
        return

    end_time = datetime.now(scraper.korea_tz)

    logger.info("\nüíæ ÏµúÏ¢Ö Í≤∞Í≥º Ï†ÄÏû•")

    success_count = results_df['retailprice'].notna().sum()
    failed_count = results_df['retailprice'].isna().sum()
    success_rate = (success_count / len(results_df) * 100) if len(results_df) > 0 else 0

    logger.info(f"\nüìä === ÏµúÏ¢Ö Í≤∞Í≥º ===")
    logger.info(f"Ï†ÑÏ≤¥: {len(results_df)}Í∞ú")
    logger.info(f"ÏÑ±Í≥µ: {success_count}Í∞ú")
    logger.info(f"Ïã§Ìå®: {failed_count}Í∞ú")
    logger.info(f"ÏÑ±Í≥µÎ•†: {success_rate:.1f}%")
    logger.info(f"ÏÜåÏöî ÏãúÍ∞Ñ: {round((end_time - start_time).total_seconds() / 60, 2)} Î∂Ñ")

    save_results = scraper.save_results(
        results_df,
        save_db=True,
        upload_server=True
    )

    scraper.analyze_results(results_df)

    logger.info("\nüìä Ï†ÄÏû• Í≤∞Í≥º:")
    logger.info(f"DB Ï†ÄÏû•: {'‚úÖ ÏÑ±Í≥µ' if save_results['db_saved'] else '‚ùå Ïã§Ìå®'}")
    logger.info(f"ÌååÏùºÏÑúÎ≤Ñ ÏóÖÎ°úÎìú: {'‚úÖ ÏÑ±Í≥µ' if save_results['server_uploaded'] else '‚ùå Ïã§Ìå®'}")

    logger.info("\n‚úÖ ÌÅ¨Î°§ÎßÅ ÌîÑÎ°úÏÑ∏Ïä§ ÏôÑÎ£å!")

if __name__ == "__main__":
    print("üì¶ ÌïÑÏöîÌïú Ìå®ÌÇ§ÏßÄ:")
    print("pip install playwright pandas pymysql sqlalchemy paramiko")
    print("playwright install chromium")
    print()

    main()
