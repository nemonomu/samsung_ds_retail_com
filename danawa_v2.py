#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Amazon ë‹¤ë‚˜ì™€ í•œêµ­ ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ V2 (íƒ€ì„ì¡´ ë¶„ë¦¬ ë²„ì „)
ì›ë³¸ danawa.py ê¸°ë°˜ - DB/íƒ€ì„ì¡´/íŒŒì¼ì„œë²„ ì„¤ì •ë§Œ V2ë¡œ ë³€ê²½
- í˜„ì§€ì‹œê°„(ë‹¤ë‚˜ì™€ í•œêµ­)ê³¼ í•œêµ­ì‹œê°„ ë¶„ë¦¬ ì €ì¥
- ìƒˆ ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš© (DB_CONFIG_V2)
- í•µì‹¬ ë¡œì§ì€ ì›ë³¸ê³¼ ë™ì¼
"""
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import pymysql
from sqlalchemy import create_engine
import paramiko
import time
import random
import re
from datetime import datetime
import logging
import os
from io import StringIO
import pytz
import zipfile
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Import database configuration V2
from config import DB_CONFIG_V2 as DB_CONFIG

from config import FILE_SERVER_CONFIG

class DanawaScraper:
    def __init__(self):
        self.driver = None
        self.db_engine = None
        self.sftp_client = None
        self.country_code = 'kr'

        # V2: íƒ€ì„ì¡´ ì„¤ì • (ë‹¤ë‚˜ì™€ëŠ” í•œêµ­ ì‚¬ì´íŠ¸ì´ë¯€ë¡œ ë‘˜ ë‹¤ Asia/Seoul)
        self.korea_tz = pytz.timezone('Asia/Seoul')
        self.local_tz = pytz.timezone('Asia/Seoul')

        # DB ì—°ê²° ì„¤ì •
        self.setup_db_connection()

        # DBì—ì„œ XPath ë¡œë“œ
        self.load_xpaths_from_db()
        
    def setup_db_connection(self):
        """DB ì—°ê²° ì„¤ì •"""
        try:
            # SQLAlchemy ì—”ì§„ ìƒì„±
            connection_string = (
                f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
                f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
            )
            self.db_engine = create_engine(connection_string)
            logger.info("âœ… DB ì—°ê²° ì„¤ì • ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.db_engine = None
    
    def load_xpaths_from_db(self):
        """DBì—ì„œ ë‹¤ë‚˜ì™€ìš© ì„ íƒì ë¡œë“œ"""
        try:
            query = """
            SELECT element_type, selector_value, priority
            FROM mall_selectors
            WHERE mall_name = 'danawa' 
              AND country_code = 'kr'
              AND is_active = TRUE
            ORDER BY element_type, priority DESC
            """
            
            df = pd.read_sql(query, self.db_engine)
            
            # element_typeë³„ë¡œ ê·¸ë£¹í™”
            self.XPATHS = {}
            for element_type in df['element_type'].unique():
                type_selectors = df[df['element_type'] == element_type]['selector_value'].tolist()
                self.XPATHS[element_type] = type_selectors
            
            logger.info(f"âœ… DBì—ì„œ ì„ íƒì ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")
            
            # ê¸°ë³¸ê°’ ì„¤ì • (DBì— ì—†ëŠ” ê²½ìš°)
            if not self.XPATHS:
                logger.warning("âš ï¸ DBì— ì„ íƒìê°€ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.XPATHS = {
                    'price': [
                        '/html/body/div[2]/div[5]/div[2]/div[2]/div[2]/div[1]/div[2]/div[1]/div[2]/a/div/span[1]'
                    ],
                    'title': [
                        '/html/body/div[2]/div[5]/div[2]/div[2]/div[1]/h3/span'
                    ],
                    'imageurl': [
                        '/html/body/div[2]/div[5]/div[2]/div[2]/div[2]/div[1]/div[1]/div[1]/a/img'
                    ],
                    'ships_from': [
                        '/html/body/div[2]/div/div/div[5]/div[1]/div[4]/div/div[1]/div/div/div/form/div/div/div/div/div[4]/div/div[19]/div/div/div[1]/div/div[2]/div[2]/div[1]/span'
                    ],
                    'sold_by': [
                        '/html/body/div[2]/div/div/div[5]/div[1]/div[4]/div/div[1]/div/div/div/form/div/div/div/div/div[4]/div/div[19]/div/div/div[1]/div/div[3]/div[2]/div[1]/span'
                    ]
                }
                
        except Exception as e:
            logger.error(f"ì„ íƒì ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            self.XPATHS = {
                'price': [],
                'title': [],
                'imageurl': [],
                'ships_from': [],
                'sold_by': []
            }
    
    def get_crawl_targets(self, limit=None, include_failed=False):
        """DBì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ URL ëª©ë¡ ì¡°íšŒ"""
        try:
            if include_failed:
                # ìµœê·¼ ì‹¤íŒ¨í•œ URLë„ í¬í•¨ (24ì‹œê°„ ì´ë‚´ ì‹¤íŒ¨ 3íšŒ ë¯¸ë§Œ)
                query = """
                WITH failed_counts AS (
                    SELECT url, COUNT(*) as fail_count
                    FROM danawa_crawl_logs
                    WHERE status = 'failed'
                      AND crawl_datetime >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                      AND country_code = 'kr'
                    GROUP BY url
                )
                SELECT DISTINCT t.*
                FROM samsung_price_tracking_list t
                LEFT JOIN failed_counts f ON t.url = f.url
                WHERE t.country = 'kr' 
                  AND t.mall_name = 'danawa'
                  AND t.is_active = TRUE
                  AND (f.fail_count IS NULL OR f.fail_count < 3)
                ORDER BY COALESCE(f.fail_count, 0) DESC  -- ì‹¤íŒ¨í•œ ê²ƒ ìš°ì„ 
                """
            else:
                query = """
                SELECT *
                FROM samsung_price_tracking_list
                WHERE country = 'kr' 
                  AND mall_name = 'danawa'
                  AND is_active = TRUE
                """
                
            if limit:
                query += f" LIMIT {limit}"
            
            df = pd.read_sql(query, self.db_engine)
            logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ {len(df)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return df.to_dict('records')
            
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
        logger.info("ğŸ”§ Chrome ë“œë¼ì´ë²„ ì„¤ì • ì¤‘...")
        
        try:
            options = uc.ChromeOptions()
            options.add_argument('--accept-lang=ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7')
            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36')
            
            self.driver = uc.Chrome(options=options)
            self.driver.maximize_window()
            logger.info("âœ… ë“œë¼ì´ë²„ ì„¤ì • ì™„ë£Œ")
            return True
        except Exception as e:
            logger.error(f"âŒ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def is_page_normal(self):
        """í˜ì´ì§€ ì •ìƒ ì—¬ë¶€ í™•ì¸"""
        try:
            page_title = self.driver.title
            page_source = self.driver.page_source
            
            # ì°¨ë‹¨ ë˜ëŠ” ì˜¤ë¥˜ í˜ì´ì§€ ì œëª© íŒ¨í„´
            blocked_patterns_title = [
                r'(?i)sorry', r'(?i)robot check', 
                r'^Amazon\.com$', r'^Amazon\.in$',
                r'^503 - Service Unavailable Error'
            ]
            
            for pattern in blocked_patterns_title:
                if re.search(pattern, page_title):
                    logger.warning("í˜ì´ì§€ ë¹„ì •ìƒ: ì œëª© íŒ¨í„´ ê°ì§€")
                    return False
            
            # ë³¸ë¬¸ ì°¨ë‹¨ íŒ¨í„´
            blocked_patterns_body = [
                "We're sorry, an error has occurred. Please reload this page and try again."
            ]
            
            for pattern in blocked_patterns_body:
                if pattern in page_source:
                    logger.warning("í˜ì´ì§€ ë¹„ì •ìƒ: ë³¸ë¬¸ íŒ¨í„´ ê°ì§€")
                    return False
            
            logger.debug("í˜ì´ì§€ ì •ìƒ")
            return True
            
        except Exception as e:
            logger.error(f"í˜ì´ì§€ ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def retry_page(self, url, max_retry=5, initial_wait=1, backoff=1.5):
        """ì¬ì‹œë„ ë¡œì§: ì§€ìˆ˜ ë°±ì˜¤í”„ ë° ì°¨ë‹¨ ê°ì§€"""
        wait = initial_wait
        
        for i in range(max_retry):
            try:
                logger.info(f"í˜ì´ì§€ ì ‘ì† ì‹œë„ {i + 1}/{max_retry}: {url}")
                self.driver.get(url)
                time.sleep(wait)
                
                if self.is_page_normal():
                    return True
                
                # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°: ì§€ìˆ˜ ë°±ì˜¤í”„ + ëœë¤
                wait = wait * backoff + random.uniform(0, 1)
                logger.warning(f"ì¬ì‹œë„ ëŒ€ê¸°: {wait:.1f}ì´ˆ")
                
            except Exception as e:
                logger.error(f"í˜ì´ì§€ ì ‘ì† ì˜¤ë¥˜ (ì‹œë„ {i + 1}): {e}")
                wait = wait * backoff
        
        logger.error(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
        return False
    
    def check_stock_status(self):
        """ì¬ê³  ìƒíƒœ í™•ì¸"""
        try:
            # ì¬ê³  ì—†ìŒì„ ë‚˜íƒ€ë‚´ëŠ” í…ìŠ¤íŠ¸ íŒ¨í„´
            stock_flag_patterns = [
                'No featured offers available',
                '^Currently unavailable',
                'ì¼ì‹œí’ˆì ˆ',
                'í’ˆì ˆ',
                'ì¬ê³ ì—†ìŒ'
            ]
            
            page_source = self.driver.page_source
            
            for pattern in stock_flag_patterns:
                if re.search(pattern, page_source, re.IGNORECASE):
                    logger.info("ì¬ê³  ì—†ìŒ ê°ì§€")
                    return False
            
            return True
            
        except Exception as e:
            logger.error(f"ì¬ê³  í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return True  # ê¸°ë³¸ì ìœ¼ë¡œ ì¬ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ê°„ì£¼
    
    def parse_price_by_country(self, price_str, country_code='kr'):
        """êµ­ê°€ë³„ ê°€ê²© í˜•ì‹ ì²˜ë¦¬"""
        if not price_str or pd.isna(price_str):
            return 0
        
        try:
            # í•œêµ­: ì‰¼í‘œë¥¼ ì²œë‹¨ìœ„ êµ¬ë¶„ìë¡œ ì‚¬ìš©
            if country_code == 'kr':
                # ìˆ«ìì™€ ì‰¼í‘œë§Œ ì¶”ì¶œ
                price_match = re.search(r'[\d,]+', str(price_str))
                if price_match:
                    price_clean = price_match.group().replace(',', '')
                    return float(price_clean)
            
            # ê¸°íƒ€ êµ­ê°€ ì²˜ë¦¬
            elif country_code in ['fr', 'it', 'es']:
                # ìœ ëŸ½: ì‰¼í‘œë¥¼ ì†Œìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©
                price_match = re.search(r'[\d,]+', str(price_str))
                if price_match:
                    price_clean = price_match.group().replace(',', '.')
                    return float(price_clean)
            else:
                # ì˜ë¯¸ê¶Œ: ì ì„ ì†Œìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©
                price_match = re.search(r'[\d,.]+', str(price_str))
                if price_match:
                    price_clean = price_match.group().replace(',', '')
                    return float(price_clean)
            
            return 0
            
        except Exception as e:
            logger.error(f"ê°€ê²© íŒŒì‹± ì˜¤ë¥˜: {price_str} -> {e}")
            return 0
    
    def extract_product_info(self, url, row_data, retry_count=0, max_retries=3):
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)"""
        try:
            logger.info(f"ğŸ” í˜ì´ì§€ ì ‘ì†: {url} (ì‹œë„: {retry_count + 1}/{max_retries + 1})")
            
            # í˜ì´ì§€ ë¡œë“œ ë° ìœ íš¨ì„± ì²´í¬
            if not self.retry_page(url):
                raise Exception("í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨")
            
            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            time.sleep(random.uniform(2, 4))
            
            # í˜„ì¬ ì‹œê°„
            # V2: íƒ€ì„ì¡´ ë¶„ë¦¬
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)


            # ISO 8601 í˜•ì‹

            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")

            tz_offset = local_time.strftime("%z")

            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"

            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"


            # ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°
            result = {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': 'kr',
                'ships_from': 'KR',
                'channel_name': 'danawa',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'Danawa',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': 'o'  # í•œêµ­ì€ VAT í¬í•¨
            }
            
            # ì¬ê³  ìƒíƒœ í™•ì¸
            stock_available = self.check_stock_status()
            
            if stock_available:
                # ê°€ê²© ì¶”ì¶œ
                try:
                    price_found = False
                    for xpath in self.XPATHS.get('price', []):
                        try:
                            price_element = self.driver.find_element(By.XPATH, xpath)
                            price_text = price_element.text.strip()
                            
                            if price_text:
                                parsed_price = self.parse_price_by_country(price_text, 'kr')
                                if parsed_price > 0:
                                    result['retailprice'] = parsed_price
                                    logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {result['retailprice']}")
                                    price_found = True
                                    break
                        except Exception as e:
                            logger.debug(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨ (XPath: {xpath}): {e}")
                            continue
                    
                    if not price_found:
                        logger.warning("ëª¨ë“  ê°€ê²© XPathì—ì„œ ì¶”ì¶œ ì‹¤íŒ¨")
                        
                except Exception as e:
                    logger.warning(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            else:
                # ì¬ê³  ì—†ìŒ
                result['retailprice'] = 0
                logger.info("ì¬ê³  ì—†ìŒìœ¼ë¡œ ê°€ê²© 0 ì„¤ì •")
            
            # ì œëª© ì¶”ì¶œ
            try:
                for xpath in self.XPATHS.get('title', []):
                    try:
                        title_element = self.driver.find_element(By.XPATH, xpath)
                        result['title'] = title_element.text.strip()
                        logger.info(f"ì œëª©: {result['title']}")
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            try:
                for xpath in self.XPATHS.get('imageurl', []):
                    try:
                        image_element = self.driver.find_element(By.XPATH, xpath)
                        result['imageurl'] = image_element.get_attribute('src')
                        logger.info(f"ì´ë¯¸ì§€ URL: {result['imageurl']}")
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ë°°ì†¡ì§€ ì •ë³´ ì¶”ì¶œ
            try:
                for xpath in self.XPATHS.get('ships_from', []):
                    try:
                        ships_element = self.driver.find_element(By.XPATH, xpath)
                        ships_text = ships_element.text.strip()
                        if ships_text:
                            result['ships_from'] = ships_text
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ë°°ì†¡ì§€ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # íŒë§¤ì ì •ë³´ ì¶”ì¶œ
            try:
                for xpath in self.XPATHS.get('sold_by', []):
                    try:
                        seller_element = self.driver.find_element(By.XPATH, xpath)
                        seller_text = seller_element.text.strip()
                        if seller_text:
                            result['sold_by'] = seller_text
                        break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"íŒë§¤ì ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            
            # ì¬ì‹œë„ ë¡œì§
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10  # ì¬ì‹œë„ë§ˆë‹¤ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                logger.info(f"ğŸ”„ {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì¬ì‹œë„ {retry_count + 1}/{max_retries})")
                time.sleep(wait_time)
                
                # ë“œë¼ì´ë²„ ìƒˆë¡œê³ ì¹¨
                try:
                    self.driver.refresh()
                except:
                    # ë“œë¼ì´ë²„ê°€ ì£½ì—ˆìœ¼ë©´ ì¬ì‹œì‘
                    logger.info("ğŸ”§ ë“œë¼ì´ë²„ ì¬ì‹œì‘ ì¤‘...")
                    self.driver.quit()
                    self.setup_driver()
                
                # ì¬ê·€ í˜¸ì¶œë¡œ ì¬ì‹œë„
                return self.extract_product_info(url, row_data, retry_count + 1, max_retries)
            
            # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
            # V2: íƒ€ì„ì¡´ ë¶„ë¦¬
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)

            return {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': 'kr',
                'ships_from': 'KR',
                'channel_name': 'danawa',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'Danawa',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': 'o'
            }
    
    def save_to_db(self, df):
        """DBì— ê²°ê³¼ ì €ì¥"""
        if self.db_engine is None:
            logger.warning("âš ï¸ DB ì—°ê²°ì´ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return False
        
        try:
            # danawa_price_crawl_tbl_kr_v2 í…Œì´ë¸”ì— ì €ì¥
            df.to_sql('danawa_price_crawl_tbl_kr_v2', self.db_engine, if_exists='append', index=False)
            logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")
            
            # í¬ë¡¤ë§ ë¡œê·¸ë¥¼ pandas DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ì„œ í•œë²ˆì— ì €ì¥
            log_records = []
            for _, row in df.iterrows():
                log_records.append({
                    'country_code': 'kr',
                    'url': row['producturl'],
                    'status': 'success' if row['retailprice'] is not None else 'failed',
                    'error_message': None if row['retailprice'] is not None else 'Price not found',
                    'execution_time': random.uniform(3, 10),
                    'retailprice': row['retailprice'],
                    'crawl_datetime': row['crawl_datetime']
                })
            
            if log_records:
                log_df = pd.DataFrame(log_records)
                log_df.to_sql('danawa_crawl_logs', self.db_engine, if_exists='append', index=False)
                logger.info(f"âœ… í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {len(log_records)}ê°œ")
            
            # ì €ì¥ëœ ë°ì´í„° í™•ì¸
            with self.db_engine.connect() as conn:
                count_query = "SELECT COUNT(*) FROM danawa_price_crawl_tbl_kr_v2 WHERE DATE(crawl_datetime) = CURDATE()"
                result = conn.execute(count_query)
                today_count = result.scalar()
                logger.info(f"ğŸ“Š ì˜¤ëŠ˜ ì €ì¥ëœ ì´ ë ˆì½”ë“œ: {today_count}ê°œ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
    
    def upload_to_file_server(self, local_file_path, date_folder):
        """íŒŒì¼ì„œë²„ì— ì—…ë¡œë“œ"""
        try:
            transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
            transport.connect(
                username=FILE_SERVER_CONFIG['username'],
                password=FILE_SERVER_CONFIG['password']
            )
            sftp = paramiko.SFTPClient.from_transport(transport)

            # êµ­ê°€ë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            country_dir = f"{FILE_SERVER_CONFIG['upload_path']}/{self.country_code}"

            # êµ­ê°€ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(country_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ êµ­ê°€ ë””ë ‰í† ë¦¬ ìƒì„±: {country_dir}")
                sftp.mkdir(country_dir)

            # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            date_dir = f"{country_dir}/{date_folder}"

            # ë‚ ì§œ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(date_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ ë‚ ì§œ ë””ë ‰í† ë¦¬ ìƒì„±: {date_dir}")
                sftp.mkdir(date_dir)

            # ì—…ë¡œë“œ ê²½ë¡œ
            remote_filename = os.path.basename(local_file_path)
            remote_path = f"{date_dir}/{remote_filename}"

            # íŒŒì¼ ì—…ë¡œë“œ
            sftp.put(local_file_path, remote_path)
            logger.info(f"âœ… íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì™„ë£Œ: {remote_path}")

            sftp.close()
            transport.close()

            return True
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    def save_results(self, df, save_db=True, upload_server=True):
        """ê²°ê³¼ ì €ì¥"""
        now = datetime.now(self.korea_tz)
        date_str = now.strftime('%Y%m%d')
        time_str = now.strftime('%H%M%S')
        base_filename = f"{date_str}_{time_str}_kr_danawa"

        results = {'db_saved': False, 'server_uploaded': False}

        if save_db:
            results['db_saved'] = self.save_to_db(df)

        if upload_server:
            try:
                # DataFrame ë³µì‚¬ë³¸ ìƒì„± (ì›ë³¸ ë³´í˜¸)
                df_copy = df.copy()

                # 1. CSV íŒŒì¼ ìƒì„±
                csv_filename = f'{base_filename}.csv'
                # Headerë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜
                df_copy.columns = df_copy.columns.str.upper()
                df_copy.to_csv(csv_filename, index=False, encoding='utf-8', lineterminator='\r\n')

                # 2. CSVë¥¼ ZIPìœ¼ë¡œ ì••ì¶•
                zip_filename = f'{base_filename}.zip'
                with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    zipf.write(csv_filename, arcname=csv_filename)

                # 3. MD5 ê³„ì‚°
                def calculate_md5(filename):
                    md5 = hashlib.md5()
                    with open(filename, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b''):
                            md5.update(chunk)
                    return md5.hexdigest()

                csv_md5 = calculate_md5(csv_filename)
                zip_md5 = calculate_md5(zip_filename)

                # 4. MD5 íŒŒì¼ ìƒì„± (ì •í•©ì„± í™•ì¸)
                md5_filename = f'{base_filename}.md5'
                with open(md5_filename, 'w', encoding='utf-8') as f:
                    f.write(f"{os.path.basename(zip_filename)} {zip_md5}\n")
                    f.write(f"{os.path.basename(csv_filename)} {csv_md5}\n")

                # 5. ZIPê³¼ MD5ë¥¼ ë‚ ì§œ í´ë”ì— ì—…ë¡œë“œ
                if self.upload_to_file_server(zip_filename, date_str):
                    if self.upload_to_file_server(md5_filename, date_str):
                        results['server_uploaded'] = True

                # 6. ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                for temp_file in [csv_filename, zip_filename, md5_filename]:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)

                logger.info("ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        return results
    def scrape_urls(self, urls_data, max_items=None):
        """ì—¬ëŸ¬ URL ìŠ¤í¬ë˜í•‘"""
        if max_items:
            urls_data = urls_data[:max_items]
        
        logger.info(f"ğŸ“Š ì´ {len(urls_data)}ê°œ ì œí’ˆ ì²˜ë¦¬ ì‹œì‘")
        
        if not self.setup_driver():
            return None
        
        results = []
        failed_urls = []  # ì‹¤íŒ¨í•œ URL ì¶”ì 
        
        try:
            for idx, row in enumerate(urls_data):
                logger.info(f"\n{'='*50}")
                logger.info(f"ì§„í–‰ë¥ : {idx + 1}/{len(urls_data)} ({(idx + 1)/len(urls_data)*100:.1f}%)")
                
                # URL ì¶”ì¶œ
                url = row.get('url')
                
                # ì œí’ˆ ì •ë³´ ì¶”ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
                result = self.extract_product_info(url, row)
                
                # ì‹¤íŒ¨ ì—¬ë¶€ í™•ì¸
                if result['retailprice'] is None:
                    failed_urls.append({
                        'url': url,
                        'item': row.get('item', ''),
                        'brand': row.get('brand', '')
                    })
                
                results.append(result)
                
                # 10ê°œë§ˆë‹¤ DBì— ì¤‘ê°„ ì €ì¥
                if (idx + 1) % 10 == 0:
                    interim_df = pd.DataFrame(results[-10:])
                    if self.db_engine:
                        try:
                            interim_df.to_sql('danawa_price_crawl_tbl_kr_v2', self.db_engine, 
                                            if_exists='append', index=False)
                            logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: 10ê°œ ë ˆì½”ë“œ DB ì €ì¥")
                        except Exception as e:
                            logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
                
                # ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°
                if idx < len(urls_data) - 1:
                    wait_time = random.uniform(1, 3)  # ë‹¤ë‚˜ì™€ëŠ” ì¢€ ë” ì§§ì€ ëŒ€ê¸°ì‹œê°„
                    logger.info(f"â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)
                    
                    # 20ê°œë§ˆë‹¤ ê¸´ íœ´ì‹
                    if (idx + 1) % 20 == 0:
                        logger.info("â˜• 20ê°œ ì²˜ë¦¬ ì™„ë£Œ, 30ì´ˆ íœ´ì‹...")
                        time.sleep(30)

            # ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì €ì¥ (10ê°œ ë‹¨ìœ„ë¡œ ë–¨ì–´ì§€ì§€ ì•ŠëŠ” ê²½ìš°)
            remaining_count = len(results) % 10
            if remaining_count > 0 and self.db_engine:
                try:
                    remaining_df = pd.DataFrame(results[-remaining_count:])
                    remaining_df.to_sql('danawa_price_crawl_tbl_kr_v2', self.db_engine,
                                      if_exists='append', index=False)
                    logger.info(f"ğŸ’¾ ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥: {remaining_count}ê°œ ë ˆì½”ë“œ DB ì €ì¥")
                except Exception as e:
                    logger.error(f"ë§ˆì§€ë§‰ ë°°ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")

        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")
        
        finally:
            # ì‹¤íŒ¨ URL ë¡œê·¸
            if failed_urls:
                logger.warning(f"\nâš ï¸ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨í•œ URL {len(failed_urls)}ê°œ:")
                for fail in failed_urls[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                    logger.warning(f"  - {fail['brand']} {fail['item']}: {fail['url']}")
                if len(failed_urls) > 5:
                    logger.warning(f"  ... ì™¸ {len(failed_urls) - 5}ê°œ")
            
            if self.driver:
                self.driver.quit()
                logger.info("ğŸ”§ ë“œë¼ì´ë²„ ì¢…ë£Œ")
        
        return pd.DataFrame(results)
    
    def analyze_results(self, df):
        """ê²°ê³¼ ë¶„ì„"""
        logger.info("\nğŸ“Š === ê²°ê³¼ ë¶„ì„ ===")
        
        total = len(df)
        with_price = df[df['retailprice'] > 0].shape[0] if 'retailprice' in df.columns else 0
        out_of_stock = df[df['retailprice'] == 0].shape[0] if 'retailprice' in df.columns else 0
        failed = df[(df['retailprice'].isna()) | (df['retailprice'] < 0)].shape[0] if 'retailprice' in df.columns else 0
        success_rate = (with_price / total * 100) if total > 0 else 0
        
        logger.info(f"ì „ì²´ ì œí’ˆ: {total}ê°œ")
        logger.info(f"ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {with_price}ê°œ")
        logger.info(f"ì¬ê³  ì—†ìŒ: {out_of_stock}ê°œ")
        logger.info(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {failed}ê°œ")
        logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        
        if with_price > 0:
            price_df = df[df['retailprice'] > 0].copy()
            
            logger.info(f"\nğŸ’° ê°€ê²© í†µê³„:")
            logger.info(f"í‰ê· ê°€: â‚©{price_df['retailprice'].mean():.0f}")
            logger.info(f"ìµœì €ê°€: â‚©{price_df['retailprice'].min():.0f}")
            logger.info(f"ìµœê³ ê°€: â‚©{price_df['retailprice'].max():.0f}")
            logger.info(f"ì¤‘ê°„ê°’: â‚©{price_df['retailprice'].median():.0f}")
            
            # ë¸Œëœë“œë³„ í†µê³„
            if 'brand' in df.columns:
                brand_stats = price_df['brand'].value_counts()
                logger.info(f"\nğŸ“ˆ ë¸Œëœë“œë³„ ì„±ê³µ:")
                for brand, count in brand_stats.items():
                    logger.info(f"  {brand}: {count}ê°œ")
            
            # ìš©ëŸ‰ë³„ í‰ê·  ê°€ê²©
            if 'capacity' in df.columns:
                capacity_stats = price_df.groupby('capacity')['retailprice'].agg(['mean', 'count'])
                logger.info(f"\nğŸ’¾ ìš©ëŸ‰ë³„ í‰ê·  ê°€ê²©:")
                for capacity, stats in capacity_stats.iterrows():
                    logger.info(f"  {capacity}: â‚©{stats['mean']:.0f} ({int(stats['count'])}ê°œ)")

def get_db_history(engine, days=7):
    """DBì—ì„œ ìµœê·¼ ê¸°ë¡ ì¡°íšŒ"""
    try:
        query = f"""
        SELECT DATE(crawl_datetime) as date, 
               COUNT(*) as total_count,
               SUM(CASE WHEN retailprice IS NOT NULL AND retailprice > 0 THEN 1 ELSE 0 END) as with_price,
               SUM(CASE WHEN retailprice = 0 THEN 1 ELSE 0 END) as out_of_stock,
               COUNT(DISTINCT brand) as brands,
               COUNT(DISTINCT item) as items
        FROM danawa_price_crawl_tbl_kr_v2
        WHERE crawl_datetime >= DATE_SUB(NOW(), INTERVAL {days} DAY)
        GROUP BY DATE(crawl_datetime)
        ORDER BY date DESC
        """
        
        df = pd.read_sql(query, engine)
        logger.info(f"\nğŸ“… ìµœê·¼ {days}ì¼ í¬ë¡¤ë§ ê¸°ë¡:")
        if not df.empty:
            print(df.to_string(index=False))
        else:
            logger.info("ìµœê·¼ í¬ë¡¤ë§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        logger.error(f"DB ì¡°íšŒ ì˜¤ë¥˜: {e}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\nğŸš€ ë‹¤ë‚˜ì™€ ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ - DB ê¸°ë°˜ ë²„ì „")
    print("="*60)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = DanawaScraper()
    
    if scraper.db_engine is None:
        logger.error("DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return
    
    # ìµœê·¼ í¬ë¡¤ë§ ê¸°ë¡ í™•ì¸
    get_db_history(scraper.db_engine, 7)
    
    # 1ë‹¨ê³„: ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
    logger.info("\nğŸ“Š 1ë‹¨ê³„: ì „ì²´ í¬ë¡¤ë§ ì‹œì‘")
    urls_data = scraper.get_crawl_targets()
    
    if not urls_data:
        logger.warning("í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ: {len(urls_data)}ê°œ")
    
    # ì²« ë²ˆì§¸ í¬ë¡¤ë§ ì‹¤í–‰
    first_results_df = scraper.scrape_urls(urls_data)
    
    if first_results_df is None or first_results_df.empty:
        logger.error("í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì²« ë²ˆì§¸ ê²°ê³¼ ë¶„ì„
    logger.info("\nğŸ“Š 1ë‹¨ê³„ ê²°ê³¼:")
    first_failed = first_results_df['retailprice'].isna().sum()
    first_success = first_results_df[first_results_df['retailprice'] > 0].shape[0]
    logger.info(f"ì„±ê³µ: {first_success}ê°œ, ì‹¤íŒ¨: {first_failed}ê°œ")
    
    # 2ë‹¨ê³„: ì‹¤íŒ¨í•œ URL ì¬ì‹œë„ (ì‹¤íŒ¨ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
    final_results_df = first_results_df.copy()
    
    if first_failed > 0:
        logger.info(f"\nğŸ”„ 2ë‹¨ê³„: ì‹¤íŒ¨í•œ {first_failed}ê°œ URL ì¬ì‹œë„")
        logger.info("60ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤...")
        time.sleep(60)
        
        # ì‹¤íŒ¨í•œ URLë§Œ ë‹¤ì‹œ ì¡°íšŒ
        failed_urls = scraper.get_crawl_targets(include_failed=True)
        
        if failed_urls:
            logger.info(f"ì¬ì‹œë„ ëŒ€ìƒ: {len(failed_urls)}ê°œ")
            
            # ìƒˆ ë“œë¼ì´ë²„ë¡œ ì¬ì‹œë„
            scraper.driver = None
            retry_results_df = scraper.scrape_urls(failed_urls)
            
            if retry_results_df is not None and not retry_results_df.empty:
                # ì¬ì‹œë„ ê²°ê³¼ ë¶„ì„
                retry_success = retry_results_df[retry_results_df['retailprice'] > 0].shape[0]
                retry_failed = retry_results_df['retailprice'].isna().sum()
                logger.info(f"\nğŸ“Š ì¬ì‹œë„ ê²°ê³¼: ì„±ê³µ {retry_success}ê°œ, ì‹¤íŒ¨ {retry_failed}ê°œ")
                
                # ê¸°ì¡´ ì‹¤íŒ¨í•œ ê²°ê³¼ë¥¼ ì¬ì‹œë„ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸
                for _, retry_row in retry_results_df.iterrows():
                    if retry_row['retailprice'] is not None:
                        # ì„±ê³µí•œ ê²½ìš° ê¸°ì¡´ ë°ì´í„° ì—…ë°ì´íŠ¸
                        mask = final_results_df['producturl'] == retry_row['producturl']
                        if mask.any():
                            final_results_df.loc[mask, 'retailprice'] = retry_row['retailprice']
                            final_results_df.loc[mask, 'title'] = retry_row['title']
                            final_results_df.loc[mask, 'imageurl'] = retry_row['imageurl']
                            final_results_df.loc[mask, 'crawl_datetime'] = retry_row['crawl_datetime']
                            final_results_df.loc[mask, 'crawl_strdatetime'] = retry_row['crawl_strdatetime']
    
    # 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì €ì¥
    logger.info("\nğŸ’¾ 3ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì €ì¥")
    
    # ìµœì¢… í†µê³„
    final_success = final_results_df[final_results_df['retailprice'] > 0].shape[0]
    final_out_of_stock = final_results_df[final_results_df['retailprice'] == 0].shape[0]
    final_failed = final_results_df['retailprice'].isna().sum()
    success_rate = (final_success / len(final_results_df) * 100) if len(final_results_df) > 0 else 0
    
    logger.info(f"\nğŸ“Š === ìµœì¢… ê²°ê³¼ ===")
    logger.info(f"ì „ì²´: {len(final_results_df)}ê°œ")
    logger.info(f"ì„±ê³µ: {final_success}ê°œ")
    logger.info(f"ì¬ê³ ì—†ìŒ: {final_out_of_stock}ê°œ")
    logger.info(f"ì‹¤íŒ¨: {final_failed}ê°œ")
    logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
    
    # ê°œì„ ìœ¨ í‘œì‹œ
    if first_failed > 0 and first_failed > final_failed:
        improvement = first_failed - final_failed
        logger.info(f"âœ¨ ì¬ì‹œë„ë¡œ {improvement}ê°œ ì¶”ê°€ ì„±ê³µ!")
    
    # DBì™€ íŒŒì¼ì„œë²„ì— ìµœì¢… ê²°ê³¼ ì €ì¥
    save_results = scraper.save_results(
        final_results_df,
        save_db=False,
        upload_server=True
    )
    
    # ìƒì„¸ ë¶„ì„
    scraper.analyze_results(final_results_df)
    
    # ì €ì¥ ê²°ê³¼ ì¶œë ¥
    logger.info("\nğŸ“Š ì €ì¥ ê²°ê³¼:")
    logger.info(f"DB ì €ì¥: {'âœ… ì„±ê³µ' if save_results['db_saved'] else 'âŒ ì‹¤íŒ¨'}")
    logger.info(f"íŒŒì¼ì„œë²„ ì—…ë¡œë“œ: {'âœ… ì„±ê³µ' if save_results['server_uploaded'] else 'âŒ ì‹¤íŒ¨'}")
    
    # ì—¬ì „íˆ ì‹¤íŒ¨í•œ URL ë¡œê·¸
    if final_failed > 0:
        logger.warning(f"\nâš ï¸ ìµœì¢…ì ìœ¼ë¡œ {final_failed}ê°œ URLì—ì„œ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨")
        if 'retailprice' in final_results_df.columns:
            failed_items = final_results_df[final_results_df['retailprice'].isna()]
            logger.warning("ì‹¤íŒ¨ ëª©ë¡ (ìƒìœ„ 5ê°œ):")
            for idx, row in failed_items.head().iterrows():
                logger.warning(f"  - {row.get('brand', 'N/A')} {row.get('item', 'N/A')}: {str(row.get('producturl', ''))[:50]}...")
    
    logger.info("\nâœ… í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")

if __name__ == "__main__":
    # í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
    required_packages = [
        'undetected-chromedriver',
        'selenium',
        'pandas',
        'pymysql',
        'sqlalchemy',
        'paramiko',
        'openpyxl'
    ]
    
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("pip install " + " ".join(required_packages))
    print("\nâš ï¸ DB ì„¤ì •ì„ ë¨¼ì € í™•ì¸í•˜ì„¸ìš”:")
    print("DB_CONFIG ë”•ì…”ë„ˆë¦¬ì˜ user, password, host ì •ë³´ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.")
    print()
    
    main()