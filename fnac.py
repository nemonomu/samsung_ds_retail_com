"""
Fnac ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ - Playwright ê¸°ë°˜ ë²„ì „
DBì—ì„œ URL ì½ì–´ì™€ì„œ í¬ë¡¤ë§ í›„ ê²°ê³¼ ì €ì¥
íŒŒì¼ëª… í˜•ì‹: {ìˆ˜ì§‘ì¼ì}{ìˆ˜ì§‘ì‹œê°„}_{êµ­ê°€ì½”ë“œ}_{ì‡¼í•‘ëª°}.csv
"""
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeout
import pandas as pd
import pymysql
from sqlalchemy import create_engine
import paramiko
import time
import random
import re
from datetime import datetime
import pytz
import logging
import os
import json
from io import StringIO
import zipfile
import hashlib

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# Import database configuration V2
from config import DB_CONFIG_V2 as DB_CONFIG

from config import FILE_SERVER_CONFIG

class FnacScraper:
    def __init__(self):
        self.playwright = None
        self.browser = None
        self.context = None
        self.page = None
        self.db_engine = None
        self.sftp_client = None
        self.country_code = 'fr'
        # V2: íƒ€ì„ì¡´ ë¶„ë¦¬ (í˜„ì§€ì‹œê°„ + í•œêµ­ì‹œê°„)
        self.korea_tz = pytz.timezone('Asia/Seoul')
        self.local_tz = pytz.timezone('Europe/Paris')  # Fnac í”„ë‘ìŠ¤ í˜„ì§€ ì‹œê°„

        # DB ì—°ê²° ì„¤ì •
        self.setup_db_connection()

        # DBì—ì„œ XPath ë¡œë“œ
        self.load_xpaths_from_db()

    def setup_db_connection(self):
        """DB ì—°ê²° ì„¤ì •"""
        try:
            # SQLAlchemy ì—”ì§„ ìƒì„±
            connection_string = (
                f"mysql+pymysql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
                f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}"
            )
            self.db_engine = create_engine(connection_string)
            logger.info("âœ… DB ì—°ê²° ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            self.db_engine = None

    def load_xpaths_from_db(self):
        """DBì—ì„œ Fnacìš© ì„ íƒì ë¡œë“œ"""
        try:
            query = """
            SELECT element_type, selector_value, priority
            FROM mall_selectors
            WHERE mall_name = 'fnac'
              AND country_code = 'fr'
              AND is_active = TRUE
            ORDER BY element_type, priority DESC
            """

            df = pd.read_sql(query, self.db_engine)

            # element_typeë³„ë¡œ ê·¸ë£¹í™”
            self.XPATHS = {}
            for element_type in df['element_type'].unique():
                type_selectors = df[df['element_type'] == element_type]['selector_value'].tolist()
                self.XPATHS[element_type] = type_selectors

            logger.info(f"âœ… DBì—ì„œ ì„ íƒì ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ")

            # ìƒˆë¡œìš´ XPathë¥¼ ê¸°ì¡´ DB XPath ì•ì— ì¶”ê°€
            if 'price' in self.XPATHS:
                new_price_selectors = [
                    '.f-faPriceBox__price',  # CSS ì„ íƒì
                    "//span[@class='f-faPriceBox__price userPrice checked']",
                    "//div[@class='f-faPriceBox__priceLine']//span[@class='f-faPriceBox__price']"
                ]
                self.XPATHS['price'] = new_price_selectors + self.XPATHS['price']
                logger.info(f"âœ… ìƒˆë¡œìš´ price ì„ íƒì ì¶”ê°€ë¨. ì´ price: {len(self.XPATHS['price'])}ê°œ")
            else:
                self.XPATHS['price'] = [
                    '.f-faPriceBox__price',
                    "//span[@class='f-faPriceBox__price userPrice checked']",
                    "//div[@class='f-faPriceBox__priceLine']//span[@class='f-faPriceBox__price']"
                ]

            # title XPath ì¶”ê°€
            if 'title' in self.XPATHS:
                new_title_selectors = [
                    '.f-productHeader__heading',
                    "//h1[@class='f-productHeader__heading']"
                ]
                self.XPATHS['title'] = new_title_selectors + self.XPATHS['title']
                logger.info(f"âœ… ìƒˆë¡œìš´ title ì„ íƒì ì¶”ê°€ë¨. ì´ title: {len(self.XPATHS['title'])}ê°œ")
            else:
                self.XPATHS['title'] = [
                    '.f-productHeader__heading',
                    "//h1[@class='f-productHeader__heading']"
                ]

            # imageurl ì„ íƒì ì¶”ê°€
            if 'imageurl' in self.XPATHS:
                new_image_selectors = [
                    '.f-productMedias__viewItem--main',
                    "//img[@class='f-productMedias__viewItem--main']"
                ]
                self.XPATHS['imageurl'] = new_image_selectors + self.XPATHS['imageurl']
                logger.info(f"âœ… ìƒˆë¡œìš´ imageurl ì„ íƒì ì¶”ê°€ë¨. ì´ imageurl: {len(self.XPATHS['imageurl'])}ê°œ")
            else:
                self.XPATHS['imageurl'] = [
                    '.f-productMedias__viewItem--main',
                    "//img[@class='f-productMedias__viewItem--main']"
                ]

            # ê¸°ë³¸ê°’ ì„¤ì • (DBì— ì—†ëŠ” ê²½ìš°)
            if not self.XPATHS:
                logger.warning("âš ï¸ DBì— ì„ íƒìê°€ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©")
                self.XPATHS = {
                    'price': [
                        '.f-faPriceBox__price',
                        "//span[@class='f-faPriceBox__price userPrice checked']"
                    ],
                    'title': [
                        '.f-productHeader__heading',
                        "//h1[@class='f-productHeader__heading']"
                    ],
                    'imageurl': [
                        '.f-productMedias__viewItem--main',
                        "//img[@class='f-productMedias__viewItem--main']"
                    ]
                }

        except Exception as e:
            logger.error(f"ì„ íƒì ë¡œë“œ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ê°’ ì‚¬ìš©
            self.XPATHS = {
                'price': [
                    '.f-faPriceBox__price',
                    "//span[@class='f-faPriceBox__price userPrice checked']"
                ],
                'title': [
                    '.f-productHeader__heading',
                    "//h1[@class='f-productHeader__heading']"
                ],
                'imageurl': [
                    '.f-productMedias__viewItem--main',
                    "//img[@class='f-productMedias__viewItem--main']"
                ]
            }

    def get_crawl_targets(self, limit=None, include_failed=False):
        """DBì—ì„œ í¬ë¡¤ë§ ëŒ€ìƒ URL ëª©ë¡ ì¡°íšŒ"""
        try:
            if include_failed:
                # ìµœê·¼ ì‹¤íŒ¨í•œ URLë„ í¬í•¨ (24ì‹œê°„ ì´ë‚´ ì‹¤íŒ¨ 3íšŒ ë¯¸ë§Œ)
                query = """
                WITH failed_counts AS (
                    SELECT url, COUNT(*) as fail_count
                    FROM amazon_crawl_logs
                    WHERE retailprice IS NULL
                      AND crawl_datetime >= DATE_SUB(NOW(), INTERVAL 24 HOUR)
                      AND country_code = 'fr'
                    GROUP BY url
                )
                SELECT DISTINCT t.*
                FROM samsung_price_tracking_list t
                LEFT JOIN failed_counts f ON t.url = f.url
                WHERE t.country = 'fr'
                  AND t.mall_name = 'fnac'
                  AND t.is_active = TRUE
                  AND (f.fail_count IS NULL OR f.fail_count < 3)
                ORDER BY COALESCE(f.fail_count, 0) DESC  -- ì‹¤íŒ¨í•œ ê²ƒ ìš°ì„ 
                """
            else:
                query = """
                SELECT *
                FROM samsung_price_tracking_list
                WHERE country = 'fr'
                  AND mall_name = 'fnac'
                  AND is_active = TRUE
                """

            if limit:
                query += f" LIMIT {limit}"

            df = pd.read_sql(query, self.db_engine)
            logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ {len(df)}ê°œ ì¡°íšŒ ì™„ë£Œ")
            return df.to_dict('records')

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ëŒ€ìƒ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def setup_browser(self):
        """Playwright ë¸Œë¼ìš°ì € ì„¤ì •"""
        logger.info("ğŸ”§ Playwright ë¸Œë¼ìš°ì € ì„¤ì • ì¤‘...")

        try:
            self.playwright = sync_playwright().start()

            # Chromium ë¸Œë¼ìš°ì € ì‹œì‘ (headless=Falseë¡œ ë” ìì—°ìŠ¤ëŸ½ê²Œ)
            self.browser = self.playwright.chromium.launch(
                headless=False,  # GUI ëª¨ë“œ
                args=[
                    '--disable-blink-features=AutomationControlled',
                    '--disable-dev-shm-usage',
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-web-security',
                    '--disable-features=IsolateOrigins,site-per-process'
                ]
            )

            # ì»¨í…ìŠ¤íŠ¸ ìƒì„± (í”„ë‘ìŠ¤ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜)
            self.context = self.browser.new_context(
                viewport={'width': 1920, 'height': 1080},
                user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                locale='fr-FR',
                timezone_id='Europe/Paris',
                geolocation={'latitude': 48.8566, 'longitude': 2.3522},  # Paris
                permissions=['geolocation']
            )

            # í˜ì´ì§€ ìƒì„±
            self.page = self.context.new_page()

            # ì¶”ê°€ ìŠ¤í…”ìŠ¤ ì„¤ì •
            self.page.add_init_script("""
                Object.defineProperty(navigator, 'webdriver', {
                    get: () => undefined
                });

                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });

                Object.defineProperty(navigator, 'languages', {
                    get: () => ['fr-FR', 'fr', 'en-US', 'en']
                });

                window.chrome = {
                    runtime: {}
                };
            """)

            logger.info("âœ… Playwright ë¸Œë¼ìš°ì € ì„¤ì • ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ë¸Œë¼ìš°ì € ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def solve_slider_captcha(self, max_attempts=3):
        """ìŠ¬ë¼ì´ë” ìº¡ì°¨ ìë™ í•´ê²°"""
        logger.info("ğŸ§© ìŠ¬ë¼ì´ë” ìº¡ì°¨ ê°ì§€ ë° í•´ê²° ì‹œë„...")

        # ìº¡ì°¨ ê´€ë ¨ ì„ íƒìë“¤ (ë” êµ¬ì²´ì ìœ¼ë¡œ)
        captcha_selectors = [
            "//div[contains(@class, 'captcha')]",
            "//div[contains(@id, 'captcha')]",
            "//div[contains(@class, 'verify')]",
            "//div[contains(@class, 'verification')]",
            "[class*='captcha' i]",
            "[id*='captcha' i]",
            "iframe[src*='captcha']",
            "iframe[title*='captcha' i]",
            "iframe[title*='verify' i]",
            "iframe[title*='puzzle' i]",
            "//div[contains(text(), 'robot')]",
            "//div[contains(text(), 'verify')]",
            "//div[contains(text(), 'slide')]"
        ]

        # ìŠ¬ë¼ì´ë” ì„ íƒìë“¤ (ìº¡ì°¨ ì „ìš©ë§Œ)
        slider_selectors = [
            ".slider",  # geo.captcha-delivery.com
            "div.slider",
            ".sliderContainer .slider",
            "//div[@class='slider']",
            "//div[@class='sliderContainer']//div[@class='slider']",
            "//div[contains(@class, 'slider') and contains(@class, 'button')]",
            "//div[contains(@class, 'slide-verify')]",
            "//span[contains(@class, 'slider') and contains(@class, 'btn')]",
            "//div[contains(@id, 'nc_') and contains(@class, 'btn')]",  # Alibaba Cloud
            ".captcha-slider-button",
            ".slide-verify-slider-mask-item",
            "#nc_1_n1z"
        ]

        try:
            # 1. ìº¡ì°¨ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            captcha_found = False
            captcha_element = None
            for selector in captcha_selectors:
                try:
                    if selector.startswith('//'):
                        locator = self.page.locator(f'xpath={selector}')
                    else:
                        locator = self.page.locator(selector)

                    if locator.is_visible(timeout=2000):
                        logger.info(f"ğŸ” ìº¡ì°¨ ìš”ì†Œ ë°œê²¬: {selector}")
                        # ìš”ì†Œì˜ í…ìŠ¤íŠ¸ë‚˜ ì†ì„± í™•ì¸
                        try:
                            text_content = locator.first.text_content()
                            if text_content:
                                logger.info(f"   ìº¡ì°¨ í…ìŠ¤íŠ¸: {text_content[:100]}")
                        except:
                            pass
                        captcha_found = True
                        captcha_element = locator
                        break
                except:
                    continue

            if not captcha_found:
                logger.info("âœ… ìº¡ì°¨ê°€ ê°ì§€ë˜ì§€ ì•ŠìŒ")
                return True

            logger.info("âš ï¸ ìº¡ì°¨ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤!")

            # 2. iframe ë‚´ë¶€ ìº¡ì°¨ í™•ì¸ ë° ì²˜ë¦¬
            try:
                frames = self.page.frames
                for frame in frames:
                    if 'captcha' in frame.url.lower() or 'verify' in frame.url.lower():
                        logger.info(f"ğŸ” ìº¡ì°¨ iframe ë°œê²¬: {frame.url}")
                        # iframe ë‚´ë¶€ì—ì„œ ìŠ¬ë¼ì´ë” ì°¾ê¸° ì‹œë„
                        for slider_sel in slider_selectors:
                            try:
                                if slider_sel.startswith('//'):
                                    slider = frame.locator(f'xpath={slider_sel}')
                                else:
                                    slider = frame.locator(slider_sel)

                                if slider.is_visible(timeout=2000):
                                    logger.info(f"âœ… iframe ë‚´ ìŠ¬ë¼ì´ë” ë°œê²¬: {slider_sel}")
                                    return self._drag_slider(slider, frame)
                            except:
                                continue
            except Exception as e:
                logger.debug(f"iframe ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

            # 3. ë©”ì¸ í˜ì´ì§€ì—ì„œ ìŠ¬ë¼ì´ë” ì°¾ê¸°
            for attempt in range(max_attempts):
                logger.info(f"ğŸ”„ ìŠ¬ë¼ì´ë” í•´ê²° ì‹œë„ {attempt + 1}/{max_attempts}")

                for slider_sel in slider_selectors:
                    try:
                        if slider_sel.startswith('//'):
                            slider = self.page.locator(f'xpath={slider_sel}')
                        else:
                            slider = self.page.locator(slider_sel)

                        if slider.is_visible(timeout=2000):
                            # ìŠ¬ë¼ì´ë” ìš”ì†Œ ì •ë³´ ì¶œë ¥
                            try:
                                slider_class = slider.first.get_attribute('class')
                                slider_id = slider.first.get_attribute('id')
                                logger.info(f"âœ… ìŠ¬ë¼ì´ë” ë°œê²¬: {slider_sel}")
                                logger.info(f"   class: {slider_class}")
                                logger.info(f"   id: {slider_id}")
                            except:
                                logger.info(f"âœ… ìŠ¬ë¼ì´ë” ë°œê²¬: {slider_sel}")

                            # ë“œë˜ê·¸ ì „ ìŠ¤í¬ë¦°ìƒ·
                            try:
                                screenshot_before = f"captcha_before_{attempt}.png"
                                self.page.screenshot(path=screenshot_before)
                                logger.info(f"ğŸ“¸ ë“œë˜ê·¸ ì „ ìŠ¤í¬ë¦°ìƒ·: {screenshot_before}")
                            except:
                                pass

                            if self._drag_slider(slider, self.page):
                                # ë“œë˜ê·¸ í›„ ëŒ€ê¸°
                                time.sleep(2)

                                # ë“œë˜ê·¸ í›„ ìŠ¤í¬ë¦°ìƒ·
                                try:
                                    screenshot_after = f"captcha_after_{attempt}.png"
                                    self.page.screenshot(path=screenshot_after)
                                    logger.info(f"ğŸ“¸ ë“œë˜ê·¸ í›„ ìŠ¤í¬ë¦°ìƒ·: {screenshot_after}")
                                except:
                                    pass

                                # ìº¡ì°¨ê°€ ì‚¬ë¼ì¡ŒëŠ”ì§€ í™•ì¸
                                captcha_still_visible = False
                                for cap_sel in captcha_selectors[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì²´í¬
                                    try:
                                        if cap_sel.startswith('//'):
                                            cap_loc = self.page.locator(f'xpath={cap_sel}')
                                        else:
                                            cap_loc = self.page.locator(cap_sel)

                                        if cap_loc.is_visible(timeout=1000):
                                            captcha_still_visible = True
                                            logger.warning(f"âš ï¸ ìº¡ì°¨ê°€ ì—¬ì „íˆ ë³´ì„: {cap_sel}")
                                            break
                                    except:
                                        continue

                                if not captcha_still_visible:
                                    logger.info("âœ… ìŠ¬ë¼ì´ë” ìº¡ì°¨ í•´ê²° ì„±ê³µ! (ìº¡ì°¨ê°€ ì‚¬ë¼ì§)")
                                    return True
                                else:
                                    logger.warning("âš ï¸ ìŠ¬ë¼ì´ë”ë¥¼ ë“œë˜ê·¸í–ˆì§€ë§Œ ìº¡ì°¨ê°€ ì—¬ì „íˆ ë³´ì…ë‹ˆë‹¤")
                                    # ë‹¤ìŒ ìŠ¬ë¼ì´ë” ì‹œë„
                                    continue

                    except Exception as e:
                        logger.debug(f"ìŠ¬ë¼ì´ë” {slider_sel} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue

                # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                if attempt < max_attempts - 1:
                    time.sleep(2)

            logger.warning("âš ï¸ ìŠ¬ë¼ì´ë” ìº¡ì°¨ë¥¼ ìë™ìœ¼ë¡œ í•´ê²°í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤")
            logger.warning("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ ìº¡ì°¨ë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”. 30ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
            time.sleep(30)  # ìˆ˜ë™ í•´ê²° ì‹œê°„
            return False

        except Exception as e:
            logger.error(f"âŒ ìº¡ì°¨ í•´ê²° ì¤‘ ì˜¤ë¥˜: {e}")
            return False

    def _drag_slider(self, slider, page_or_frame):
        """ìŠ¬ë¼ì´ë”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë“œë˜ê·¸"""
        try:
            # ìŠ¬ë¼ì´ë”ì˜ ìœ„ì¹˜ì™€ í¬ê¸° ê°€ì ¸ì˜¤ê¸°
            box = slider.bounding_box()
            if not box:
                logger.warning("ìŠ¬ë¼ì´ë” ìœ„ì¹˜ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ")
                return False

            # ì‹œì‘ ìœ„ì¹˜ (ìŠ¬ë¼ì´ë” ì¤‘ì•™)
            start_x = box['x'] + box['width'] / 2
            start_y = box['y'] + box['height'] / 2

            # ë“œë˜ê·¸ ê±°ë¦¬ ê³„ì‚° (ì¼ë°˜ì ìœ¼ë¡œ ìŠ¬ë¼ì´ë” íŠ¸ë™ ë„ˆë¹„ë§Œí¼)
            # íŠ¸ë™ ìš”ì†Œë¥¼ ì°¾ì•„ì„œ ë„ˆë¹„ë¥¼ ê°€ì ¸ì˜¤ê±°ë‚˜, ê¸°ë³¸ê°’ ì‚¬ìš©
            drag_distance = 300  # ê¸°ë³¸ê°’

            # íŠ¸ë™ ìš”ì†Œ ì°¾ê¸° ì‹œë„
            track_selectors = [
                ".sliderContainer",  # geo.captcha-delivery.com
                ".sliderbg",
                "//div[@class='sliderContainer']",
                "//div[@class='sliderbg']",
                "//div[contains(@class, 'slider-track')]",
                "//div[contains(@class, 'slide-track')]",
                ".slider-track",
                ".slide-verify-slider-track"
            ]

            for track_sel in track_selectors:
                try:
                    if track_sel.startswith('//'):
                        track = page_or_frame.locator(f'xpath={track_sel}')
                    else:
                        track = page_or_frame.locator(track_sel)

                    if track.is_visible(timeout=1000):
                        track_box = track.bounding_box()
                        if track_box:
                            drag_distance = track_box['width'] - box['width']
                            logger.info(f"íŠ¸ë™ ë„ˆë¹„ ê¸°ë°˜ ë“œë˜ê·¸ ê±°ë¦¬: {drag_distance}px")
                            break
                except:
                    continue

            # ëª©í‘œ ìœ„ì¹˜
            end_x = start_x + drag_distance
            end_y = start_y

            logger.info(f"ğŸ–±ï¸ ìŠ¬ë¼ì´ë” ë“œë˜ê·¸: ({start_x:.0f}, {start_y:.0f}) â†’ ({end_x:.0f}, {end_y:.0f})")

            # page ê°ì²´ ê°€ì ¸ì˜¤ê¸° (Frameì—ëŠ” mouseê°€ ì—†ìœ¼ë¯€ë¡œ)
            # Frameì´ë©´ pageë¥¼ ê°€ì ¸ì˜¤ê³ , Pageë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if hasattr(page_or_frame, 'page'):
                # Frame ê°ì²´
                mouse_obj = page_or_frame.page.mouse
            else:
                # Page ê°ì²´
                mouse_obj = page_or_frame.mouse

            # ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆìš°ìŠ¤ ì›€ì§ì„ ì‹œë®¬ë ˆì´ì…˜
            # 1. ë§ˆìš°ìŠ¤ë¥¼ ìŠ¬ë¼ì´ë”ë¡œ ì´ë™
            mouse_obj.move(start_x, start_y)
            time.sleep(random.uniform(0.1, 0.3))

            # 2. ë§ˆìš°ìŠ¤ ë²„íŠ¼ ëˆ„ë¥´ê¸°
            mouse_obj.down()
            time.sleep(random.uniform(0.1, 0.2))

            # 3. ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë“œë˜ê·¸
            steps = random.randint(20, 30)
            for i in range(steps):
                # ì§„í–‰ë¥ 
                progress = (i + 1) / steps

                # í˜„ì¬ x ìœ„ì¹˜ (ì•½ê°„ì˜ ëœë¤ ë³€í™” ì¶”ê°€)
                current_x = start_x + (drag_distance * progress)

                # yì¶•ì— ì•½ê°„ì˜ í”ë“¤ë¦¼ ì¶”ê°€ (ì‚¬ëŒì²˜ëŸ¼)
                wobble = random.uniform(-2, 2)
                current_y = start_y + wobble

                # ë§ˆìš°ìŠ¤ ì´ë™
                mouse_obj.move(current_x, current_y)

                # ê° ìŠ¤í…ë§ˆë‹¤ ì•½ê°„ì˜ ëœë¤ ë”œë ˆì´
                time.sleep(random.uniform(0.01, 0.03))

            # 4. ëª©í‘œ ì§€ì ì— ì •í™•íˆ ë„ë‹¬
            mouse_obj.move(end_x, end_y)
            time.sleep(random.uniform(0.1, 0.2))

            # 5. ë§ˆìš°ìŠ¤ ë²„íŠ¼ ë†“ê¸°
            mouse_obj.up()

            logger.info("âœ… ìŠ¬ë¼ì´ë” ë“œë˜ê·¸ ì™„ë£Œ")
            time.sleep(1)

            return True

        except Exception as e:
            logger.error(f"âŒ ìŠ¬ë¼ì´ë” ë“œë˜ê·¸ ì‹¤íŒ¨: {e}")
            return False

    def initialize_session(self):
        """Fnac ì„¸ì…˜ ì´ˆê¸°í™”"""
        logger.info("Fnac ì„¸ì…˜ ì´ˆê¸°í™”...")

        try:
            # Fnac ë©”ì¸ í˜ì´ì§€ ì ‘ì† (domcontentloadedë¡œ ë³€ê²½)
            self.page.goto("https://www.fnac.com", wait_until='domcontentloaded', timeout=30000)
            logger.info("âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
            time.sleep(2)

            # ì¿ í‚¤ íŒì—… ì²˜ë¦¬
            try:
                logger.info("ğŸª ì¿ í‚¤ íŒì—… í™•ì¸ ì¤‘...")
                time.sleep(1)  # íŒì—…ì´ ë‚˜íƒ€ë‚  ì‹œê°„ ëŒ€ê¸°

                # "J'accepte" ë²„íŠ¼ í´ë¦­ (ì—¬ëŸ¬ ì„ íƒì ì‹œë„)
                cookie_selectors = [
                    "text=J'accepte",
                    "button:has-text(\"J'accepte\")",
                    "//button[contains(text(), \"J'accepte\")]",
                    "//button[contains(text(), 'accepte')]",
                    "[class*='accept' i]",
                    "[id*='accept' i]",
                    "button[class*='cookie']",
                    ".didomi-button"
                ]

                cookie_found = False
                for selector in cookie_selectors:
                    try:
                        logger.info(f"ğŸ” ì¿ í‚¤ ì„ íƒì ì‹œë„: {selector}")

                        if selector.startswith('text=') or selector.startswith('button:'):
                            button = self.page.locator(selector).first
                        elif selector.startswith('//'):
                            button = self.page.locator(f'xpath={selector}').first
                        else:
                            button = self.page.locator(selector).first

                        # ë²„íŠ¼ì´ ë³´ì´ëŠ”ì§€ í™•ì¸
                        if button.is_visible(timeout=2000):
                            button.click(timeout=3000)
                            logger.info(f"ğŸª ì¿ í‚¤ ë™ì˜ íŒì—… ì²˜ë¦¬ ì™„ë£Œ (ì„ íƒì: {selector})")
                            time.sleep(2)  # ì¿ í‚¤ ì²˜ë¦¬ í›„ ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                            cookie_found = True
                            break
                    except Exception as e:
                        logger.debug(f"ì„ íƒì {selector} ì‹¤íŒ¨: {e}")
                        continue

                if not cookie_found:
                    logger.info("ì¿ í‚¤ íŒì—…ì´ ì—†ê±°ë‚˜ ì´ë¯¸ ì²˜ë¦¬ë¨")

            except Exception as e:
                logger.debug(f"ì¿ í‚¤ íŒì—… ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")

            # ìŠ¬ë¼ì´ë” ìº¡ì°¨ í•´ê²° ì‹œë„
            time.sleep(2)  # ìº¡ì°¨ê°€ ë‚˜íƒ€ë‚  ì‹œê°„ ëŒ€ê¸°
            self.solve_slider_captcha()

            # ì„¸ì…˜ì´ ì œëŒ€ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
            title = self.page.title()
            if "fnac" in title.lower():
                logger.info("âœ… Fnac ì„¸ì…˜ ì´ˆê¸°í™” ì™„ë£Œ")
                return True
            else:
                logger.warning("âš ï¸ ì„¸ì…˜ ì´ˆê¸°í™” ë¶€ë¶„ ì„±ê³µ")
                return True

        except Exception as e:
            logger.error(f"âŒ ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    def extract_product_info(self, url, row_data, retry_count=0, max_retries=3):
        """ì œí’ˆ ì •ë³´ ì¶”ì¶œ (ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€ ë° ì¬ì‹œë„ ë¡œì§)"""
        try:
            logger.info(f"ğŸ” í˜ì´ì§€ ì ‘ì†: {url} (ì‹œë„: {retry_count + 1}/{max_retries + 1})")
            response = self.page.goto(url, wait_until='domcontentloaded', timeout=30000)

            # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
            time.sleep(random.uniform(3, 5))

            # ìŠ¬ë¼ì´ë” ìº¡ì°¨ê°€ ë‚˜íƒ€ë‚¬ëŠ”ì§€ í™•ì¸ ë° í•´ê²°
            self.solve_slider_captcha()

            # 404 ì—ëŸ¬ ì²´í¬ (ë´‡ ê°ì§€ë¡œ ì¸í•œ 404 ìœ„ì¥ ê°€ëŠ¥ì„±)
            if response and response.status == 404:
                logger.warning("âš ï¸ 404 ì—ëŸ¬ ê°ì§€ - ë´‡ ê°ì§€ ê°€ëŠ¥ì„±, ì¬ì ‘ì† ì‹œë„")

                # ì ì‹œ ëŒ€ê¸°
                time.sleep(random.uniform(3, 5))

                # ë°”ë¡œ ì›ë˜ URL ì¬ì ‘ì† (ë©”ì¸ í˜ì´ì§€ ê±°ì¹˜ì§€ ì•ŠìŒ)
                logger.info(f"ğŸ”„ URL ì§ì ‘ ì¬ì ‘ì†: {url}")
                response = self.page.goto(url, wait_until='domcontentloaded', timeout=30000)
                time.sleep(random.uniform(3, 5))

                # ì—¬ì „íˆ 404ì´ë©´ ì—ëŸ¬ ë°œìƒ
                if response and response.status == 404:
                    logger.error("âŒ ì¬ì ‘ì† í›„ì—ë„ 404 ì—ëŸ¬ - URLì´ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì°¨ë‹¨ë¨")
                    raise Exception("404 error after retry - possible blocked or invalid URL")
                else:
                    logger.info("âœ… ì¬ì ‘ì† ì„±ê³µ")

            # í˜„ì¬ ì‹œê°„
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)

            # ISO 8601 í˜•ì‹
            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")
            tz_offset = local_time.strftime("%z")
            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"
            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"

            # ê¸°ë³¸ ê²°ê³¼ êµ¬ì¡°
            result = {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'fr'),
                'ships_from': 'FR',
                'channel_name': 'fnac',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'Fnac',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': row_data.get('vat', 'o')
            }

            # ì œëª© ì¶”ì¶œ (ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€)
            title_extracted = False
            try:
                for selector in self.XPATHS.get('title', []):
                    try:
                        # XPathì¸ì§€ CSSì¸ì§€ íŒë‹¨
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        # ìš”ì†Œê°€ ë‚˜íƒ€ë‚  ë•Œê¹Œì§€ ëŒ€ê¸° (ìµœëŒ€ 5ì´ˆ)
                        locator.wait_for(state='visible', timeout=5000)
                        title_text = locator.inner_text()

                        if title_text and title_text.strip():
                            result['title'] = title_text.strip()
                            logger.info(f"ì œëª©: {result['title']}")
                            title_extracted = True
                            break
                    except:
                        continue
            except Exception as e:
                logger.warning(f"ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            # ì°¨ë‹¨ í˜ì´ì§€ ê°ì§€ ë° ì¬ì‹œë„ ë¡œì§
            if not title_extracted:
                logger.warning("âš ï¸ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨ - ì°¨ë‹¨ í˜ì´ì§€ ê°€ëŠ¥ì„±")

                # 1ë‹¨ê³„: ìƒˆë¡œê³ ì¹¨ ì‹œë„
                logger.info("ğŸ”„ ìƒˆë¡œê³ ì¹¨ ì‹œë„...")
                self.page.reload(wait_until='networkidle', timeout=30000)
                time.sleep(random.uniform(3, 5))

                # ì œëª© ì¬ì¶”ì¶œ ì‹œë„
                for selector in self.XPATHS.get('title', []):
                    try:
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        locator.wait_for(state='visible', timeout=5000)
                        title_text = locator.inner_text()

                        if title_text and title_text.strip():
                            result['title'] = title_text.strip()
                            logger.info(f"âœ… ìƒˆë¡œê³ ì¹¨ í›„ ì œëª© ì¶”ì¶œ ì„±ê³µ: {result['title']}")
                            title_extracted = True
                            break
                    except:
                        continue

                # 2ë‹¨ê³„: ì—¬ì „íˆ ì‹¤íŒ¨í•˜ë©´ fnac.com ì ‘ì† í›„ ì¬ì‹œë„
                if not title_extracted:
                    logger.warning("âš ï¸ ìƒˆë¡œê³ ì¹¨ í›„ì—ë„ ì‹¤íŒ¨ - fnac.com ì ‘ì† í›„ ì¬ì‹œë„")

                    # Fnac ë©”ì¸ í˜ì´ì§€ ì ‘ì†
                    self.page.goto("https://www.fnac.com", wait_until='networkidle', timeout=30000)
                    time.sleep(random.uniform(2, 4))

                    # ì›ë˜ URL ì¬ì ‘ì†
                    logger.info(f"ğŸ”„ ì›ë˜ URL ì¬ì ‘ì†: {url}")
                    self.page.goto(url, wait_until='networkidle', timeout=30000)
                    time.sleep(random.uniform(3, 5))

                    # ì œëª© ì¬ì¶”ì¶œ ì‹œë„
                    for selector in self.XPATHS.get('title', []):
                        try:
                            if selector.startswith('//'):
                                locator = self.page.locator(f'xpath={selector}')
                            else:
                                locator = self.page.locator(selector)

                            locator.wait_for(state='visible', timeout=5000)
                            title_text = locator.inner_text()

                            if title_text and title_text.strip():
                                result['title'] = title_text.strip()
                                logger.info(f"âœ… ì¬ì ‘ì† í›„ ì œëª© ì¶”ì¶œ ì„±ê³µ: {result['title']}")
                                title_extracted = True
                                break
                        except:
                            continue

                    # ì—¬ì „íˆ ì‹¤íŒ¨í•˜ë©´ ì—ëŸ¬ë¡œ ì²˜ë¦¬
                    if not title_extracted:
                        logger.error("âŒ ëª¨ë“  ì¬ì‹œë„ í›„ì—ë„ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨ - ì°¨ë‹¨ëœ ê²ƒìœ¼ë¡œ íŒë‹¨")
                        raise Exception("Title extraction failed - possible blocked page")

            # ê°€ê²© ì¶”ì¶œ
            try:
                price_found = False

                logger.info("ğŸ” ê°€ê²© ì¶”ì¶œ ì‹œë„...")

                for selector in self.XPATHS.get('price', []):
                    try:
                        logger.info(f"ğŸ” ì„ íƒì ì‹œë„: {selector}")

                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        locator.wait_for(state='visible', timeout=5000)
                        price_text = locator.inner_text()
                        logger.info(f"ğŸ” ì¶”ì¶œí•œ í…ìŠ¤íŠ¸: '{price_text}'")

                        if price_text and price_text.strip():
                            # Fnac í”„ë‘ìŠ¤ ê°€ê²© í˜•ì‹: "419,99 â‚¬" ë˜ëŠ” "419,99â‚¬"
                            # ì‰¼í‘œë¥¼ ì ìœ¼ë¡œ ë³€í™˜, â‚¬ ê¸°í˜¸ ì œê±°
                            price_text_clean = price_text.replace(',', '.').replace('â‚¬', '').replace('\xa0', '').strip()
                            price_match = re.search(r'(\d+\.?\d*)', price_text_clean)
                            if price_match:
                                price_number = price_match.group(1)
                                result['retailprice'] = float(price_number)
                                logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ: â‚¬{result['retailprice']}")
                                price_found = True
                                break

                    except Exception as e:
                        logger.warning(f"âŒ ì„ íƒì {selector} ì‹¤íŒ¨: {e}")
                        continue

                # JavaScriptë¡œ ê°€ê²© ì°¾ê¸° (ìµœí›„ ìˆ˜ë‹¨)
                if not price_found:
                    try:
                        js_result = self.page.evaluate("""
                            () => {
                                var priceSelectors = [
                                    '.f-faPriceBox__price',
                                    '[class*="price"]',
                                    'span[class*="Price"]'
                                ];

                                for (var i = 0; i < priceSelectors.length; i++) {
                                    var elements = document.querySelectorAll(priceSelectors[i]);
                                    for (var j = 0; j < elements.length; j++) {
                                        var text = elements[j].textContent || elements[j].innerText;
                                        if (text && /\\d/.test(text) && text.includes('â‚¬')) {
                                            return text.trim();
                                        }
                                    }
                                }
                                return null;
                            }
                        """)

                        if js_result:
                            logger.info(f"ğŸ” JavaScriptì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸: '{js_result}'")
                            price_text_clean = js_result.replace(',', '.').replace('â‚¬', '').replace('\xa0', '').strip()
                            price_match = re.search(r'(\d+\.?\d*)', price_text_clean)
                            if price_match:
                                price_number = price_match.group(1)
                                result['retailprice'] = float(price_number)
                                logger.info(f"âœ… ê°€ê²© ì¶”ì¶œ ì„±ê³µ (JS): â‚¬{result['retailprice']}")
                                price_found = True
                    except Exception as e:
                        logger.debug(f"JavaScript ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")

                if not price_found:
                    logger.warning("ëª¨ë“  ê°€ê²© ì¶”ì¶œ ë°©ë²• ì‹¤íŒ¨")

            except Exception as e:
                logger.warning(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            # ì´ë¯¸ì§€ URL ì¶”ì¶œ
            try:
                image_found = False

                # 1. ì„ íƒìë“¤ ì‹œë„
                for selector in self.XPATHS.get('imageurl', []):
                    try:
                        if selector.startswith('//'):
                            locator = self.page.locator(f'xpath={selector}')
                        else:
                            locator = self.page.locator(selector)

                        locator.wait_for(state='visible', timeout=5000)
                        src = locator.get_attribute('src')

                        if src and 'fnac-static.com' in src:
                            result['imageurl'] = src
                            logger.info(f"ì´ë¯¸ì§€ URL: {result['imageurl']}")
                            image_found = True
                            break
                    except:
                        continue

                # 2. JavaScriptë¡œ ì´ë¯¸ì§€ ì°¾ê¸°
                if not image_found:
                    try:
                        js_result = self.page.evaluate("""
                            () => {
                                var imgs = document.querySelectorAll('img');
                                for (var i = 0; i < imgs.length; i++) {
                                    var src = imgs[i].src || imgs[i].getAttribute('data-src');
                                    if (src && src.includes('fnac-static.com')) {
                                        return src;
                                    }
                                }
                                return null;
                            }
                        """)

                        if js_result:
                            result['imageurl'] = js_result
                            logger.info(f"ì´ë¯¸ì§€ URL (JS): {result['imageurl']}")
                            image_found = True
                    except:
                        pass

                if not image_found:
                    logger.warning("ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨")

            except Exception as e:
                logger.warning(f"ì´ë¯¸ì§€ URL ì¶”ì¶œ ì‹¤íŒ¨: {e}")

            return result

        except Exception as e:
            logger.error(f"âŒ í˜ì´ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

            # ì¬ì‹œë„ ë¡œì§
            if retry_count < max_retries:
                wait_time = (retry_count + 1) * 10
                logger.info(f"ğŸ”„ {wait_time}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤... (ì¬ì‹œë„ {retry_count + 1}/{max_retries})")
                time.sleep(wait_time)

                # ì¬ê·€ í˜¸ì¶œë¡œ ì¬ì‹œë„
                return self.extract_product_info(url, row_data, retry_count + 1, max_retries)

            # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            logger.error(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
            now_time = datetime.now(self.korea_tz)
            local_time = datetime.now(self.local_tz)

            crawl_dt = local_time.strftime("%Y-%m-%dT%H:%M:%S")
            tz_offset = local_time.strftime("%z")
            tz_formatted = f"{tz_offset[:3]}:{tz_offset[3:]}" if tz_offset else "+00:00"
            crawl_datetime_iso = f"{crawl_dt}{tz_formatted}"

            return {
                'retailerid': row_data.get('retailerid', ''),
                'country_code': row_data.get('country', 'fr'),
                'ships_from': 'FR',
                'channel_name': 'fnac',
                'channel': row_data.get('channel', 'Online'),
                'retailersku': row_data.get('retailersku', ''),
                'brand': row_data.get('brand', ''),
                'brand_eng': row_data.get('brand_eng', row_data.get('brand', '')),
                'form_factor': row_data.get('form_factor', ''),
                'segment_lv1': row_data.get('seg_lv1', ''),
                'segment_lv2': row_data.get('seg_lv2', ''),
                'segment_lv3': row_data.get('seg_lv3', ''),
                'capacity': row_data.get('capacity', ''),
                'item': row_data.get('item', ''),
                'retailprice': None,
                'sold_by': 'Fnac',
                'imageurl': None,
                'producturl': url,
                'crawl_datetime': crawl_datetime_iso,
                'crawl_strdatetime': local_time.strftime('%Y%m%d%H%M%S') + f"{local_time.microsecond:06d}"[:4],
                'kr_crawl_datetime': now_time.strftime('%Y-%m-%d %H:%M:%S'),
                'kr_crawl_strdatetime': now_time.strftime('%Y%m%d%H%M%S') + f"{now_time.microsecond:06d}"[:4],
                'title': None,
                'vat': row_data.get('vat', 'o')
            }

    def save_to_db(self, df):
        """DBì— ê²°ê³¼ ì €ì¥"""
        if self.db_engine is None:
            logger.warning("âš ï¸ DB ì—°ê²°ì´ ì—†ì–´ DB ì €ì¥ì„ ê±´ë„ˆëœë‹ˆë‹¤")
            return False

        try:
            # fnac_price_crawl_tbl_fr í…Œì´ë¸”ì— ì €ì¥
            df.to_sql('fnac_price_crawl_tbl_fr', self.db_engine, if_exists='append', index=False)
            logger.info(f"âœ… DB ì €ì¥ ì™„ë£Œ: {len(df)}ê°œ ë ˆì½”ë“œ")

            # í¬ë¡¤ë§ ë¡œê·¸ë¥¼ pandas DataFrameìœ¼ë¡œ ë§Œë“¤ì–´ì„œ í•œë²ˆì— ì €ì¥
            log_records = []
            for _, row in df.iterrows():
                log_records.append({
                    'country_code': 'fr',
                    'url': row['producturl'],
                    'error_message': None if row['retailprice'] is not None else 'Price not found',
                    'execution_time': random.uniform(3, 10),
                    'retailprice': row['retailprice'],
                    'crawl_datetime': row['crawl_datetime']
                })

            if log_records:
                log_df = pd.DataFrame(log_records)
                log_df.to_sql('amazon_crawl_logs', self.db_engine, if_exists='append', index=False)
                logger.info(f"âœ… í¬ë¡¤ë§ ë¡œê·¸ ì €ì¥ ì™„ë£Œ: {len(log_records)}ê°œ")

            return True

        except Exception as e:
            logger.error(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False

    def upload_to_file_server(self, local_file_path, date_folder):
        """íŒŒì¼ì„œë²„ì— ì—…ë¡œë“œ"""
        try:
            transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
            transport.connect(
                username=FILE_SERVER_CONFIG['username'],
                password=FILE_SERVER_CONFIG['password']
            )
            sftp = paramiko.SFTPClient.from_transport(transport)

            # êµ­ê°€ë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            country_dir = f"{FILE_SERVER_CONFIG['upload_path']}/{self.country_code}"

            # êµ­ê°€ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(country_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ êµ­ê°€ ë””ë ‰í† ë¦¬ ìƒì„±: {country_dir}")
                sftp.mkdir(country_dir)

            # ë‚ ì§œë³„ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            date_dir = f"{country_dir}/{date_folder}"

            # ë‚ ì§œ ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
            try:
                sftp.stat(date_dir)
            except FileNotFoundError:
                logger.info(f"ğŸ“ ë‚ ì§œ ë””ë ‰í† ë¦¬ ìƒì„±: {date_dir}")
                sftp.mkdir(date_dir)

            # ì—…ë¡œë“œ ê²½ë¡œ
            remote_filename = os.path.basename(local_file_path)
            remote_path = f"{date_dir}/{remote_filename}"

            # íŒŒì¼ ì—…ë¡œë“œ
            sftp.put(local_file_path, remote_path)
            logger.info(f"âœ… íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì™„ë£Œ: {remote_path}")

            sftp.close()
            transport.close()

            return True
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ì„œë²„ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False

    def save_results(self, df, save_db=True, upload_server=True):
        """ê²°ê³¼ ì €ì¥"""
        now = datetime.now(self.korea_tz)
        date_str = now.strftime('%Y%m%d')
        time_str = now.strftime('%H%M%S')
        base_filename = f"{date_str}_{time_str}_fr_fnac"

        results = {'db_saved': False, 'server_uploaded': False}

        if save_db:
            results['db_saved'] = self.save_to_db(df)

        if upload_server:
            try:
                # 1. CSV íŒŒì¼ ìƒì„±
                csv_filename = f'{base_filename}.csv'
                # Headerë¥¼ ëŒ€ë¬¸ìë¡œ ë³€í™˜
                df.columns = df.columns.str.upper()
                df.to_csv(csv_filename, index=False, encoding='utf-8', lineterminator='\r\n')

                # 2. CSVë¥¼ ZIPìœ¼ë¡œ ì••ì¶•
                zip_filename = f'{base_filename}.zip'
                with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    zipf.write(csv_filename, arcname=csv_filename)

                # 3. MD5 ê³„ì‚°
                def calculate_md5(filename):
                    md5 = hashlib.md5()
                    with open(filename, 'rb') as f:
                        for chunk in iter(lambda: f.read(4096), b''):
                            md5.update(chunk)
                    return md5.hexdigest()

                csv_md5 = calculate_md5(csv_filename)
                zip_md5 = calculate_md5(zip_filename)

                # 4. MD5 íŒŒì¼ ìƒì„±
                md5_filename = f'{base_filename}.md5'
                with open(md5_filename, 'w', encoding='utf-8') as f:
                    f.write(f"{os.path.basename(zip_filename)} {zip_md5}\n")
                    f.write(f"{os.path.basename(csv_filename)} {csv_md5}\n")

                # 5. ZIPê³¼ MD5ë¥¼ ë‚ ì§œ í´ë”ì— ì—…ë¡œë“œ
                if self.upload_to_file_server(zip_filename, date_str):
                    if self.upload_to_file_server(md5_filename, date_str):
                        results['server_uploaded'] = True

                # 6. ë¡œì»¬ ì„ì‹œ íŒŒì¼ ì‚­ì œ
                for temp_file in [csv_filename, zip_filename, md5_filename]:
                    if os.path.exists(temp_file):
                        os.remove(temp_file)

                logger.info("ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
            except Exception as e:
                logger.error(f"íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

        return results

    def test_connection(self):
        """ì—°ê²° í…ŒìŠ¤íŠ¸ ë° ì„¸ì…˜ ì´ˆê¸°í™”"""
        logger.info("=== Fnac ì„¸ì…˜ ì´ˆê¸°í™” ë° í…ŒìŠ¤íŠ¸ ===")

        if not self.setup_browser():
            return False

        try:
            # 1ë‹¨ê³„: Google ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("1ë‹¨ê³„: Google ì—°ê²° í…ŒìŠ¤íŠ¸...")
            self.page.goto("https://www.google.com", wait_until='networkidle', timeout=30000)
            time.sleep(2)
            google_title = self.page.title()

            if "Google" in google_title:
                logger.info("âœ… Google ì ‘ì† ì„±ê³µ")
            else:
                logger.warning("âš ï¸ Google ì ‘ì† ì´ìƒ")

            # 2ë‹¨ê³„: Fnac ì„¸ì…˜ ì´ˆê¸°í™”
            if not self.initialize_session():
                return False

            # 3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ìƒí’ˆ í˜ì´ì§€ ì ‘ì† (DBì—ì„œ ì‹¤ì œ URL ê°€ì ¸ì˜¤ê¸°)
            logger.info("3ë‹¨ê³„: í…ŒìŠ¤íŠ¸ ìƒí’ˆ í˜ì´ì§€ ì ‘ì†...")

            # DBì—ì„œ ì²« ë²ˆì§¸ ì œí’ˆ ê°€ì ¸ì˜¤ê¸°
            test_products = self.get_crawl_targets(limit=1)
            if not test_products:
                logger.warning("âš ï¸ í…ŒìŠ¤íŠ¸ìš© ì œí’ˆì´ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤")
                return True

            test_row = test_products[0]
            test_url = test_row.get('url')

            test_result = self.extract_product_info(test_url, test_row)

            logger.info("ì¶”ì¶œëœ ì •ë³´:")
            logger.info(f"  - ìƒí’ˆëª…: {test_result['title']}")
            logger.info(f"  - ê°€ê²©: â‚¬{test_result['retailprice']}")
            logger.info(f"  - ì´ë¯¸ì§€: {'ì¶”ì¶œë¨' if test_result['imageurl'] else 'ì—†ìŒ'}")

            # 4ë‹¨ê³„: íŒŒì¼ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸
            logger.info("4ë‹¨ê³„: íŒŒì¼ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸...")
            try:
                transport = paramiko.Transport((FILE_SERVER_CONFIG['host'], FILE_SERVER_CONFIG['port']))
                transport.connect(
                    username=FILE_SERVER_CONFIG['username'],
                    password=FILE_SERVER_CONFIG['password']
                )
                transport.close()
                logger.info("âœ… íŒŒì¼ì„œë²„ ì—°ê²° ì„±ê³µ")
            except:
                logger.warning("âš ï¸ íŒŒì¼ì„œë²„ ì—°ê²° ì‹¤íŒ¨ - í¬ë¡¤ë§ì€ ê³„ì† ì§„í–‰")

            if test_result['retailprice'] or test_result['title']:
                logger.info("âœ… ì •ë³´ ì¶”ì¶œ ì„±ê³µ - í¬ë¡¤ë§ ì¤€ë¹„ ì™„ë£Œ!")
                return True
            else:
                logger.warning("âš ï¸ ì •ë³´ ì¶”ì¶œ ë¶€ë¶„ ì‹¤íŒ¨ - ê·¸ë˜ë„ ê³„ì† ì§„í–‰")
                return True

        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

    def scrape_urls(self, urls_data, max_items=None):
        """ì—¬ëŸ¬ URL ìŠ¤í¬ë˜í•‘"""
        if max_items:
            urls_data = urls_data[:max_items]

        logger.info(f"ğŸ“Š ì´ {len(urls_data)}ê°œ ì œí’ˆ ì²˜ë¦¬ ì‹œì‘")

        results = []
        failed_urls = []

        try:
            for idx, row in enumerate(urls_data):
                logger.info(f"\n{'='*50}")
                logger.info(f"ì§„í–‰ë¥ : {idx + 1}/{len(urls_data)} ({(idx + 1)/len(urls_data)*100:.1f}%)")

                url = row.get('url')
                result = self.extract_product_info(url, row)

                if result['retailprice'] is None:
                    failed_urls.append({
                        'url': url,
                        'item': row.get('item', ''),
                        'brand': row.get('brand', '')
                    })

                results.append(result)

                # 10ê°œë§ˆë‹¤ DBì— ì¤‘ê°„ ì €ì¥
                if (idx + 1) % 10 == 0:
                    interim_df = pd.DataFrame(results[-10:])
                    if self.db_engine:
                        try:
                            interim_df.to_sql('fnac_price_crawl_tbl_fr', self.db_engine,
                                            if_exists='append', index=False)
                            logger.info(f"ğŸ’¾ ì¤‘ê°„ ì €ì¥: 10ê°œ ë ˆì½”ë“œ DB ì €ì¥")
                        except Exception as e:
                            logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")

                # ë‹¤ìŒ ìš”ì²­ ì „ ëŒ€ê¸°
                if idx < len(urls_data) - 1:
                    wait_time = random.uniform(2, 5)
                    logger.info(f"â³ {wait_time:.1f}ì´ˆ ëŒ€ê¸° ì¤‘...")
                    time.sleep(wait_time)

                    if (idx + 1) % 10 == 0:
                        logger.info("â˜• 10ê°œ ì²˜ë¦¬ ì™„ë£Œ, 30ì´ˆ íœ´ì‹...")
                        time.sleep(30)

        except Exception as e:
            logger.error(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜¤ë¥˜: {e}")

        finally:
            if failed_urls:
                logger.warning(f"\nâš ï¸ ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨í•œ URL {len(failed_urls)}ê°œ:")
                for fail in failed_urls[:5]:
                    logger.warning(f"  - {fail['brand']} {fail['item']}: {fail['url']}")
                if len(failed_urls) > 5:
                    logger.warning(f"  ... ì™¸ {len(failed_urls) - 5}ê°œ")

            if self.browser:
                self.browser.close()
                logger.info("ğŸ”§ ë¸Œë¼ìš°ì € ì¢…ë£Œ")

            if self.playwright:
                self.playwright.stop()
                logger.info("ğŸ”§ Playwright ì¢…ë£Œ")

        return pd.DataFrame(results)

    def analyze_results(self, df):
        """ê²°ê³¼ ë¶„ì„"""
        logger.info("\nğŸ“Š === ê²°ê³¼ ë¶„ì„ ===")

        total = len(df)
        with_price = df['retailprice'].notna().sum()
        without_price = df['retailprice'].isna().sum()
        success_rate = (with_price / total * 100) if total > 0 else 0

        logger.info(f"ì „ì²´ ì œí’ˆ: {total}ê°œ")
        logger.info(f"ê°€ê²© ì¶”ì¶œ ì„±ê³µ: {with_price}ê°œ")
        logger.info(f"ê°€ê²© ì¶”ì¶œ ì‹¤íŒ¨: {without_price}ê°œ")
        logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")

        if with_price > 0:
            price_df = df[df['retailprice'].notna()].copy()
            price_df['numeric_price'] = price_df['retailprice']

            logger.info(f"\nğŸ’° ê°€ê²© í†µê³„:")
            logger.info(f"í‰ê· ê°€: â‚¬{price_df['numeric_price'].mean():.2f}")
            logger.info(f"ìµœì €ê°€: â‚¬{price_df['numeric_price'].min():.2f}")
            logger.info(f"ìµœê³ ê°€: â‚¬{price_df['numeric_price'].max():.2f}")
            logger.info(f"ì¤‘ê°„ê°’: â‚¬{price_df['numeric_price'].median():.2f}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\nğŸš€ Fnac ê°€ê²© ì¶”ì¶œ ì‹œìŠ¤í…œ - Playwright ê¸°ë°˜ ë²„ì „")
    print("="*60)

    scraper = FnacScraper()

    if scraper.db_engine is None:
        logger.error("DB ì—°ê²° ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
    test_mode = os.getenv("TEST_MODE", "false").lower()

    if test_mode in ["true", "1", "yes"]:
        logger.info("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰")

        if scraper.test_connection():
            logger.info("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        else:
            logger.error("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")

        if scraper.browser:
            scraper.browser.close()
        if scraper.playwright:
            scraper.playwright.stop()
        return

    # ì‹¤ì œ í¬ë¡¤ë§
    logger.info("\nğŸ“Š ì‹¤ì œ í¬ë¡¤ë§ ì‹œì‘")

    if not scraper.test_connection():
        logger.error("ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return

    urls_data = scraper.get_crawl_targets()

    if not urls_data:
        logger.warning("í¬ë¡¤ë§ ëŒ€ìƒì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info(f"âœ… í¬ë¡¤ë§ ëŒ€ìƒ: {len(urls_data)}ê°œ")

    start_time = datetime.now(scraper.korea_tz)
    results_df = scraper.scrape_urls(urls_data)

    if results_df is None or results_df.empty:
        logger.error("í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    end_time = datetime.now(scraper.korea_tz)

    logger.info("\nğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥")

    success_count = results_df['retailprice'].notna().sum()
    failed_count = results_df['retailprice'].isna().sum()
    success_rate = (success_count / len(results_df) * 100) if len(results_df) > 0 else 0

    logger.info(f"\nğŸ“Š === ìµœì¢… ê²°ê³¼ ===")
    logger.info(f"ì „ì²´: {len(results_df)}ê°œ")
    logger.info(f"ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"ì„±ê³µë¥ : {success_rate:.1f}%")
    logger.info(f"ì†Œìš” ì‹œê°„: {round((end_time - start_time).total_seconds() / 60, 2)} ë¶„")

    save_results = scraper.save_results(
        results_df,
        save_db=True,
        upload_server=True
    )

    scraper.analyze_results(results_df)

    logger.info("\nğŸ“Š ì €ì¥ ê²°ê³¼:")
    logger.info(f"DB ì €ì¥: {'âœ… ì„±ê³µ' if save_results['db_saved'] else 'âŒ ì‹¤íŒ¨'}")
    logger.info(f"íŒŒì¼ì„œë²„ ì—…ë¡œë“œ: {'âœ… ì„±ê³µ' if save_results['server_uploaded'] else 'âŒ ì‹¤íŒ¨'}")

    logger.info("\nâœ… í¬ë¡¤ë§ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")

if __name__ == "__main__":
    print("ğŸ“¦ í•„ìš”í•œ íŒ¨í‚¤ì§€:")
    print("pip install playwright pandas pymysql sqlalchemy paramiko")
    print("playwright install chromium")
    print()

    main()
